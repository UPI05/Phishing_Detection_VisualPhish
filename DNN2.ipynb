{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Reshape, Flatten, Conv2D, Conv2DTranspose, LeakyReLU, BatchNormalization\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import Accuracy\n",
    "from tensorflow.keras.metrics import BinaryAccuracy, Precision, Recall, AUC\n",
    "from imblearn.over_sampling import SMOTE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_data(folder):\n",
    "    data = []\n",
    "    labels = []\n",
    "    for subfolder in os.listdir(folder):\n",
    "        subfolder_path = os.path.join(folder, subfolder)\n",
    "        if os.path.isdir(subfolder_path):\n",
    "            for file in os.listdir(subfolder_path):\n",
    "                file_path = os.path.join(subfolder_path, file)\n",
    "                if file.endswith('.npy'):\n",
    "                    # Load numpy array\n",
    "                    array = np.load(file_path).flatten()\n",
    "                    data.append(array)\n",
    "                    # Label phishing as 1, benign as 0\n",
    "                    label = 1 if (folder.find('phishing') != -1) else 0\n",
    "                    labels.append(label)\n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "# Đường dẫn đến thư mục chứa dữ liệu\n",
    "phishing_path = 'VisualPhish/phishing_features'\n",
    "benign_path = 'VisualPhish/trusted_list_features'\n",
    "\n",
    "# Load dữ liệu\n",
    "phishing_data, phishing_labels = load_data(phishing_path)\n",
    "benign_data, benign_labels = load_data(benign_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1195, 512)\n",
      "(1195,)\n",
      "(9363, 512)\n",
      "(9363,)\n"
     ]
    }
   ],
   "source": [
    "print(phishing_data.shape)\n",
    "print(phishing_labels.shape)\n",
    "print(benign_data.shape)\n",
    "print(benign_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_and_remove(data, labels):\n",
    "    # Tách 20% dữ liệu cho test set và lưu chỉ mục\n",
    "    train_idx, test_idx = train_test_split(\n",
    "        np.arange(len(labels)), test_size=0.2, random_state=42, stratify=labels)\n",
    "\n",
    "    # Phân chia dữ liệu dựa trên chỉ mục\n",
    "    test_set = data[test_idx]\n",
    "    test_labels = labels[test_idx]\n",
    "    train_set = np.delete(data, test_idx, axis=0)\n",
    "    train_labels = np.delete(labels, test_idx, axis=0)\n",
    "    return train_set, train_labels, test_set, test_labels\n",
    "\n",
    "# Áp dụng cho dữ liệu phishing và benign\n",
    "phishing_train, phishing_labels_train, phishing_test, phishing_labels_test = split_and_remove(phishing_data, phishing_labels)\n",
    "benign_train, benign_labels_train, benign_test, benign_labels_test = split_and_remove(benign_data, benign_labels)\n",
    "\n",
    "# Gộp dữ liệu huấn luyện và kiểm thử\n",
    "train_data = np.concatenate((phishing_train, benign_train))\n",
    "train_labels = np.concatenate((phishing_labels_train, benign_labels_train))\n",
    "test_data = np.concatenate((phishing_test, benign_test))\n",
    "test_labels = np.concatenate((phishing_labels_test, benign_labels_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(956, 512)\n",
      "(956,)\n",
      "(239, 512)\n",
      "(239,)\n",
      "(7490, 512)\n",
      "(7490,)\n",
      "(1873, 512)\n",
      "(1873,)\n",
      "======\n",
      "(2112, 512)\n",
      "(2112,)\n"
     ]
    }
   ],
   "source": [
    "print(phishing_train.shape)\n",
    "print(phishing_labels_train.shape)\n",
    "print(phishing_test.shape)\n",
    "print(phishing_labels_test.shape)\n",
    "print(benign_train.shape)\n",
    "print(benign_labels_train.shape)\n",
    "print(benign_test.shape)\n",
    "print(benign_labels_test.shape)\n",
    "print('======')\n",
    "print(test_data.shape)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def showFeature(features):\n",
    "  image = features.reshape(16, 32)\n",
    "  #print(image.shape)\n",
    "  #print(image)\n",
    "  \n",
    "  plt.subplot(1, 1, 1)  # Tạo subplot\n",
    "  plt.imshow(image, cmap='viridis')  # Sử dụng viridis làm bảng màu\n",
    "  plt.title(f'Feature map')\n",
    "  plt.axis('off')  # Ẩn trục tọa độ\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_model():\n",
    "    DNN_model = Sequential([\n",
    "        Dense(1024, activation='relu', input_shape=(512,)),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return DNN_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored on calling ctypes callback function: <function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.<locals>.match_module_callback at 0x7f8f942ac160>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/haohao/miniconda3/envs/tf/lib/python3.9/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n",
      "    self._make_module_from_path(filepath)\n",
      "  File \"/home/haohao/miniconda3/envs/tf/lib/python3.9/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n",
      "    module = module_class(filepath, prefix, user_api, internal_api)\n",
      "  File \"/home/haohao/miniconda3/envs/tf/lib/python3.9/site-packages/threadpoolctl.py\", line 606, in __init__\n",
      "    self.version = self.get_version()\n",
      "  File \"/home/haohao/miniconda3/envs/tf/lib/python3.9/site-packages/threadpoolctl.py\", line 646, in get_version\n",
      "    config = get_config().split()\n",
      "AttributeError: 'NoneType' object has no attribute 'split'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client  0 : \n",
      "(14980, 512)\n",
      "(14980,)\n"
     ]
    }
   ],
   "source": [
    "# Chia dữ liệu thành 10 phần\n",
    "\n",
    "federated_data = []\n",
    "\n",
    "# Giả sử X, y là dữ liệu của bạn\n",
    "num_clients = 1\n",
    "\n",
    "num_samples_phishing_per_client = len(phishing_train) // num_clients\n",
    "num_samples_benign_per_client = len(benign_train) // num_clients\n",
    "\n",
    "for i in range(num_clients):\n",
    "    start_idx = i * num_samples_phishing_per_client\n",
    "    end_idx = (i + 1) * num_samples_phishing_per_client if i != num_clients - 1 else len(phishing_train)\n",
    "    \n",
    "    start_idx_benign = i * num_samples_benign_per_client\n",
    "    end_idx_benign = (i + 1) * num_samples_benign_per_client if i != num_clients - 1 else len(benign_train)\n",
    "\n",
    "    X_client_phishing = phishing_train[start_idx:end_idx]\n",
    "    y_client_phishing = phishing_labels_train[start_idx:end_idx]\n",
    "\n",
    "    X_client_benign = benign_train[start_idx_benign:end_idx_benign]\n",
    "    y_client_benign = benign_labels_train[start_idx_benign:end_idx_benign]\n",
    "\n",
    "    print(\"Client \", i, \": \")\n",
    "    smote = SMOTE()\n",
    "    X_client, y_client = smote.fit_resample(np.concatenate((X_client_phishing, X_client_benign), axis=0), \n",
    "                                            np.concatenate((y_client_phishing, y_client_benign), axis=0))\n",
    "    print(X_client.shape)\n",
    "    print(y_client.shape)\n",
    "    federated_data.append((X_client, y_client))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(X_data, y_data):\n",
    "    # Chuyển đổi sang TensorFlow Dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X_data, y_data))\n",
    "    dataset = dataset.batch(64) # BatchSize\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "federated_data_train = [preprocessing(X_client, y_client) for X_client, y_client in federated_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_weights_changed(old_weights, new_weights, tolerance=1e-5):\n",
    "    \"\"\"\n",
    "    Kiểm tra xem trọng số của mô hình có thay đổi giữa các vòng đào tạo hay không.\n",
    "    :param old_weights: Trọng số của mô hình từ vòng trước.\n",
    "    :param new_weights: Trọng số mới của mô hình.\n",
    "    :param tolerance: Ngưỡng dung sai cho sự thay đổi.\n",
    "    :return: True nếu có sự thay đổi, ngược lại False.\n",
    "    \"\"\"\n",
    "    for old_layer, new_layer in zip(old_weights, new_weights):\n",
    "        if np.any(np.abs(new_layer - old_layer) > tolerance):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2112\n"
     ]
    }
   ],
   "source": [
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo tập dữ liệu từ test_data và test_labels\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_data, test_labels))\n",
    "\n",
    "# Chia thành các batch (ví dụ: batch_size=32)\n",
    "test_dataset = test_dataset.batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "  7/235 [..............................] - ETA: 4s - loss: 0.6465 - accuracy: 0.9866"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 5s 20ms/step - loss: 0.6647 - accuracy: 0.6029 - val_loss: 0.7889 - val_accuracy: 0.1132\n",
      "Epoch 2/1000\n",
      "235/235 [==============================] - 5s 19ms/step - loss: 0.7176 - accuracy: 0.5835 - val_loss: 0.7823 - val_accuracy: 0.1132\n",
      "Epoch 3/1000\n",
      "235/235 [==============================] - 5s 19ms/step - loss: 0.7047 - accuracy: 0.5007 - val_loss: 0.7611 - val_accuracy: 0.1136\n",
      "Epoch 4/1000\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 0.7009 - accuracy: 0.5009 - val_loss: 0.7529 - val_accuracy: 0.1136\n",
      "Epoch 5/1000\n",
      "235/235 [==============================] - 5s 19ms/step - loss: 0.6998 - accuracy: 0.5011 - val_loss: 0.7473 - val_accuracy: 0.1136\n",
      "Epoch 6/1000\n",
      "235/235 [==============================] - 5s 19ms/step - loss: 0.6991 - accuracy: 0.5013 - val_loss: 0.7427 - val_accuracy: 0.1141\n",
      "Epoch 7/1000\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 0.6038 - accuracy: 0.8268 - val_loss: 21.7319 - val_accuracy: 0.1132\n",
      "Epoch 8/1000\n",
      "235/235 [==============================] - 4s 19ms/step - loss: 0.8524 - accuracy: 0.4391 - val_loss: 0.7288 - val_accuracy: 0.1250\n",
      "Epoch 9/1000\n",
      "235/235 [==============================] - 5s 19ms/step - loss: 0.7113 - accuracy: 0.5298 - val_loss: 0.8666 - val_accuracy: 0.1132\n",
      "Epoch 10/1000\n",
      "235/235 [==============================] - 4s 19ms/step - loss: 0.7225 - accuracy: 0.4826 - val_loss: 0.7926 - val_accuracy: 0.1132\n",
      "Epoch 11/1000\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 0.7114 - accuracy: 0.4443 - val_loss: 0.7464 - val_accuracy: 0.1132\n",
      "Epoch 12/1000\n",
      "235/235 [==============================] - 4s 19ms/step - loss: 0.7023 - accuracy: 0.4358 - val_loss: 0.7365 - val_accuracy: 0.1132\n",
      "Epoch 13/1000\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.7566 - accuracy: 0.6977 - val_loss: 514.4444 - val_accuracy: 0.1132\n",
      "Epoch 14/1000\n",
      "235/235 [==============================] - 4s 19ms/step - loss: 5.9746 - accuracy: 0.9066 - val_loss: 32.6679 - val_accuracy: 0.1132\n",
      "Epoch 15/1000\n",
      "235/235 [==============================] - 4s 19ms/step - loss: 1.1892 - accuracy: 0.5126 - val_loss: 0.7675 - val_accuracy: 0.1449\n",
      "Epoch 16/1000\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 0.8408 - accuracy: 0.6617 - val_loss: 0.9447 - val_accuracy: 0.1132\n",
      "Epoch 17/1000\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 0.6602 - accuracy: 0.4824 - val_loss: 2.3045 - val_accuracy: 0.1132\n",
      "Epoch 18/1000\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.8241 - accuracy: 0.6064Restoring model weights from the end of the best epoch: 8.\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 0.8240 - accuracy: 0.6065 - val_loss: 0.9675 - val_accuracy: 0.1146\n",
      "Epoch 18: early stopping\n",
      "Số lượng epochs đã thực hiện: 18\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Tạo một callback EarlyStopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',   # Giám sát mất mát của tập kiểm định\n",
    "    min_delta=0.0000001,      # Thay đổi tối thiểu để coi là cải thiện\n",
    "    patience=10,          # Số lượng epochs không cải thiện trước khi dừng\n",
    "    verbose=1,            # Hiển thị thông báo khi dừng\n",
    "    mode='min',           # Dừng quá trình huấn luyện khi giá trị 'val_loss' không giảm\n",
    "    restore_best_weights=True # Khôi phục trọng số tốt nhất khi kết thúc\n",
    ")\n",
    "\n",
    "# Giả sử 'model' là mô hình của bạn và bạn đã chuẩn bị dữ liệu\n",
    "# model = ...\n",
    "\n",
    "# Huấn luyện mô hình với dữ liệu và callback EarlyStopping\n",
    "history = model.fit(\n",
    "    federated_data_train[0],\n",
    "    validation_data=test_dataset,\n",
    "    epochs=1000,          # Số lượng epochs tối đa\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "# In số lượng epochs đã thực hiện\n",
    "print(\"Số lượng epochs đã thực hiện:\", len(history.history['loss']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 0s 6ms/step - loss: 0.7514 - accuracy: 0.1132\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7513643503189087, 0.11316287517547607]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_dataset)\n",
    "# test_data => wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, roc_auc_score\n",
    "\n",
    "def evaluate_model(model, test_dataset, loss_fn):\n",
    "    # Đánh giá mô hình trên tập dữ liệu kiểm tra\n",
    "    test_loss, test_accuracy = model.evaluate(test_dataset, verbose=0)\n",
    "\n",
    "    # Dự đoán nhãn và tính toán các chỉ số\n",
    "    y_true = np.concatenate([y_batch.numpy().flatten() for _, y_batch in test_dataset])\n",
    "    y_pred = np.concatenate([model.predict(x_batch, verbose=0).flatten() for x_batch, _ in test_dataset])\n",
    "\n",
    "    y_pred_rounded = np.array([1 if y > 0.5 else 0 for y in y_pred])\n",
    "\n",
    "    print(len(y_true))\n",
    "    print(len(y_pred))\n",
    "    cnt0 = 0\n",
    "    cnt1 = 0\n",
    "    for y in y_pred_rounded:\n",
    "        if y == 0:\n",
    "            cnt0 += 1\n",
    "        else:\n",
    "            cnt1 += 1\n",
    "    print(cnt0)\n",
    "    print(cnt1)\n",
    "\n",
    "    # Tính toán các chỉ số đánh giá\n",
    "    f1 = f1_score(y_true, y_pred_rounded)\n",
    "    recall = recall_score(y_true, y_pred_rounded)\n",
    "    precision = precision_score(y_true, y_pred_rounded)\n",
    "    auc = roc_auc_score(y_true, y_pred_rounded)\n",
    "\n",
    "    return test_loss, test_accuracy, f1, recall, precision, auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2112\n",
      "2112\n",
      "27\n",
      "2085\n",
      "Test Loss: 0.7288432121276855\n",
      "Test Accuracy: 0.125\n",
      "Test F1: 0.20481927710843373\n",
      "Test Recall: 0.99581589958159\n",
      "Test Precision: 0.11414868105515588\n",
      "Test Auc: 0.5048486865766999\n"
     ]
    }
   ],
   "source": [
    "loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "test_loss, test_accuracy, f1, recall, precision, auc = evaluate_model(model, test_dataset, loss_fn)\n",
    "\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "print(f\"Test F1: {f1}\")\n",
    "print(f\"Test Recall: {recall}\")\n",
    "print(f\"Test Precision: {precision}\")\n",
    "print(f\"Test Auc: {auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/////////////////////////////ROUND  1 ///////////////////////////\n",
      "e: 0\n",
      "Loss: 0.0, Accuracy: 1.0\n",
      "Client i weight thay doi\n",
      "2112\n",
      "2112\n",
      "Test Loss: 20.750492095947266\n",
      "Test Accuracy: 0.8868371248245239\n",
      "Test F1: 0.0\n",
      "Test Recall: 0.0\n",
      "Test Precision: 0.0\n",
      "Test Auc: 0.5\n",
      "Trọng số thay đổi sau vòng 1\n",
      "res\n",
      "24/33 [====================>.........] - ETA: 0s - loss: 28.5319 - accuracy: 0.8444   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haohao/miniconda3/envs/tf/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 0s 7ms/step - loss: 20.7505 - accuracy: 0.8868\n",
      "33/33 [==============================] - 0s 6ms/step\n",
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def train_model_on_client(model, client_dataset, epochs=1):\n",
    "    for epoch in range(epochs):\n",
    "        print(\"e:\",epoch)\n",
    "        for step, (x_batch_train, y_batch_train) in enumerate(client_dataset):\n",
    "            loss, accuracy = model.train_on_batch(x_batch_train, y_batch_train)\n",
    "        print(f'Loss: {loss}, Accuracy: {accuracy}')\n",
    "    \n",
    "    return model.get_weights(), loss, accuracy\n",
    "    \n",
    "def aggregate_weights(client_weights):\n",
    "    new_weights = [np.mean([client_weights[i][layer] for i in range(len(client_weights))], axis=0) for layer in range(len(client_weights[0]))]\n",
    "    return new_weights\n",
    "\n",
    "# Training loop\n",
    "global_model = create_model()\n",
    "\n",
    "global_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "previous_weights = global_model.get_weights()\n",
    "\n",
    "for round_num in range(1):\n",
    "    print(\"/////////////////////////////ROUND \", round_num + 1, \"///////////////////////////\")\n",
    "    client_weights = []\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "\n",
    "    for client_data in federated_data_train:\n",
    "        weights, loss, accuracy = train_model_on_client(global_model, client_data, epochs=1)\n",
    "        client_weights.append(weights)\n",
    "        losses.append(loss)\n",
    "        accuracies.append(accuracy)\n",
    "        if has_weights_changed(weights, previous_weights):\n",
    "            print(\"Client i weight thay doi\")\n",
    "        else:\n",
    "            print(\"Client i weight khong doi\")\n",
    "    \n",
    "    # Aggregate the weights and update the global model\n",
    "    averaged_weights = aggregate_weights(client_weights)\n",
    "    global_model.set_weights(averaged_weights)\n",
    "    \n",
    "\n",
    "    loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    test_loss, test_accuracy, f1, recall, precision, auc = evaluate_model(global_model, test_dataset, loss_fn)\n",
    "\n",
    "    print(f\"Test Loss: {test_loss}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy}\")\n",
    "    print(f\"Test F1: {f1}\")\n",
    "    print(f\"Test Recall: {recall}\")\n",
    "    print(f\"Test Precision: {precision}\")\n",
    "    print(f\"Test Auc: {auc}\")\n",
    "\n",
    "    if has_weights_changed(previous_weights, averaged_weights):\n",
    "        print(f\"Trọng số thay đổi sau vòng {round_num + 1}\")\n",
    "    else:\n",
    "        print(f\"Không có sự thay đổi trong trọng số sau vòng {round_num + 1}\")\n",
    "\n",
    "    # Cập nhật trọng số cho vòng tiếp theo\n",
    "    \n",
    "    previous_weights = averaged_weights\n",
    "    \n",
    "    # Evaluate the model on the test data\n",
    "    print(\"res\")\n",
    "    \n",
    "    global_model.evaluate(test_dataset)\n",
    "    res = global_model.predict(test_dataset).flatten()\n",
    "    print(res)\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
