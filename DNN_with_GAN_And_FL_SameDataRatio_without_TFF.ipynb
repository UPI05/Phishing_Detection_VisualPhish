{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-02 23:06:35.105231: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-02 23:06:35.179822: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-02 23:06:35.179878: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-02 23:06:35.179944: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-02 23:06:35.194991: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-02 23:06:35.195940: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-02 23:06:36.664638: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Reshape, Flatten, Conv2D, Conv2DTranspose, LeakyReLU, BatchNormalization\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import Accuracy\n",
    "from tensorflow.keras.metrics import BinaryAccuracy, Precision, Recall, AUC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_data(folder):\n",
    "    data = []\n",
    "    labels = []\n",
    "    for subfolder in os.listdir(folder):\n",
    "        subfolder_path = os.path.join(folder, subfolder)\n",
    "        if os.path.isdir(subfolder_path):\n",
    "            for file in os.listdir(subfolder_path):\n",
    "                file_path = os.path.join(subfolder_path, file)\n",
    "                if file.endswith('.npy'):\n",
    "                    # Load numpy array\n",
    "                    array = np.load(file_path).flatten()\n",
    "                    data.append(array)\n",
    "                    # Label phishing as 1, benign as 0\n",
    "                    label = 1 if (folder.find('phishing') != -1) else 0\n",
    "                    labels.append(label)\n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "# Đường dẫn đến thư mục chứa dữ liệu\n",
    "phishing_path = 'VisualPhish/phishing_features'\n",
    "benign_path = 'VisualPhish/trusted_list_features'\n",
    "\n",
    "# Load dữ liệu\n",
    "phishing_data, phishing_labels = load_data(phishing_path)\n",
    "benign_data, benign_labels = load_data(benign_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1195, 512)\n",
      "(1195,)\n",
      "(9363, 512)\n",
      "(9363,)\n"
     ]
    }
   ],
   "source": [
    "print(phishing_data.shape)\n",
    "print(phishing_labels.shape)\n",
    "print(benign_data.shape)\n",
    "print(benign_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_and_remove(data, labels):\n",
    "    # Tách 20% dữ liệu cho test set và lưu chỉ mục\n",
    "    train_idx, test_idx = train_test_split(\n",
    "        np.arange(len(labels)), test_size=0.2, random_state=42, stratify=labels)\n",
    "\n",
    "    # Phân chia dữ liệu dựa trên chỉ mục\n",
    "    test_set = data[test_idx]\n",
    "    test_labels = labels[test_idx]\n",
    "    train_set = np.delete(data, test_idx, axis=0)\n",
    "    train_labels = np.delete(labels, test_idx, axis=0)\n",
    "    return train_set, train_labels, test_set, test_labels\n",
    "\n",
    "# Áp dụng cho dữ liệu phishing và benign\n",
    "phishing_train, phishing_labels_train, phishing_test, phishing_labels_test = split_and_remove(phishing_data, phishing_labels)\n",
    "benign_train, benign_labels_train, benign_test, benign_labels_test = split_and_remove(benign_data, benign_labels)\n",
    "\n",
    "# Gộp dữ liệu huấn luyện và kiểm thử\n",
    "train_data = np.concatenate((phishing_train, benign_train))\n",
    "train_labels = np.concatenate((phishing_labels_train, benign_labels_train))\n",
    "test_data = np.concatenate((phishing_test, benign_test))\n",
    "test_labels = np.concatenate((phishing_labels_test, benign_labels_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(956, 512)\n",
      "(956,)\n",
      "(239, 512)\n",
      "(239,)\n",
      "(7490, 512)\n",
      "(7490,)\n",
      "(1873, 512)\n",
      "(1873,)\n",
      "======\n",
      "(2112, 512)\n",
      "(2112,)\n"
     ]
    }
   ],
   "source": [
    "print(phishing_train.shape)\n",
    "print(phishing_labels_train.shape)\n",
    "print(phishing_test.shape)\n",
    "print(phishing_labels_test.shape)\n",
    "print(benign_train.shape)\n",
    "print(benign_labels_train.shape)\n",
    "print(benign_test.shape)\n",
    "print(benign_labels_test.shape)\n",
    "print('======')\n",
    "print(test_data.shape)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(noise_dim):\n",
    "    model = tf.keras.Sequential([\n",
    "        Dense(256, input_shape=(noise_dim,)),\n",
    "        LeakyReLU(alpha=0.2),\n",
    "        Dense(512),  # Tăng số lượng neuron\n",
    "        LeakyReLU(alpha=0.2),\n",
    "        Dense(1024),  # Tăng thêm số lượng neuron\n",
    "        LeakyReLU(alpha=0.2),\n",
    "        Dense(512)  # Output layer với 512 units\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def build_discriminator():\n",
    "    model = tf.keras.Sequential([\n",
    "        Dense(1024, input_shape=(512,)),\n",
    "        LeakyReLU(alpha=0.2),\n",
    "        Dense(512),  # Giảm số lượng neuron\n",
    "        LeakyReLU(alpha=0.2),\n",
    "        Dense(256),  # Giảm thêm số lượng neuron\n",
    "        LeakyReLU(alpha=0.2),\n",
    "        Dense(1, activation='sigmoid')  # Output layer\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gan(generator, discriminator):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(generator)\n",
    "    model.add(discriminator)\n",
    "\n",
    "    return model\n",
    "\n",
    "z_dim = 100\n",
    "batch_size = 32\n",
    "learning_rate = 0.0002\n",
    "\n",
    "discriminator = build_discriminator()\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate), metrics=['accuracy'])\n",
    "\n",
    "generator = build_generator(z_dim)\n",
    "\n",
    "discriminator.trainable = False\n",
    "gan = build_gan(generator, discriminator)\n",
    "gan.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 256)               25856     \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 256)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 512)               131584    \n",
      "                                                                 \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 512)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1024)              525312    \n",
      "                                                                 \n",
      " leaky_re_lu_5 (LeakyReLU)   (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1207552 (4.61 MB)\n",
      "Trainable params: 1207552 (4.61 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 1024)              525312    \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 512)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 256)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1181697 (4.51 MB)\n",
      "Trainable params: 0 (0.00 Byte)\n",
      "Non-trainable params: 1181697 (4.51 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator.summary()\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, D Loss: 6.669939994812012, G Loss: 0.011001106351613998\n",
      "Epoch: 2, D Loss: 6.902899742126465, G Loss: 9.865272521972656\n",
      "Epoch: 3, D Loss: -6.721072196960449, G Loss: 33.344146728515625\n",
      "Epoch: 4, D Loss: -27.434661865234375, G Loss: 103.53710174560547\n",
      "Epoch: 5, D Loss: -19.604625701904297, G Loss: 108.72645568847656\n",
      "Epoch: 6, D Loss: 6.727492809295654, G Loss: 83.29505920410156\n",
      "Epoch: 7, D Loss: 17.852798461914062, G Loss: 1.1009833812713623\n",
      "Epoch: 8, D Loss: 1.807241439819336, G Loss: 58.1837272644043\n",
      "Epoch: 9, D Loss: 113.80995178222656, G Loss: 0.0\n",
      "Epoch: 10, D Loss: -0.8085185289382935, G Loss: 1.224791407585144\n",
      "Epoch: 11, D Loss: -1.6941297054290771, G Loss: 5.567266464233398\n",
      "Epoch: 12, D Loss: 4.1105475425720215, G Loss: 0.03337419033050537\n",
      "Epoch: 13, D Loss: 0.314744770526886, G Loss: 0.9535006284713745\n",
      "Epoch: 14, D Loss: -0.1762608140707016, G Loss: 3.7234604358673096\n",
      "Epoch: 15, D Loss: 0.0027856826782226562, G Loss: 2.3643980026245117\n",
      "Epoch: 16, D Loss: -0.9133782386779785, G Loss: 4.939338207244873\n",
      "Epoch: 17, D Loss: 4.262908935546875, G Loss: 5.082266807556152\n",
      "Epoch: 18, D Loss: 0.7241819500923157, G Loss: 6.036013603210449\n",
      "Epoch: 19, D Loss: -1.083255410194397, G Loss: 7.829519271850586\n",
      "Epoch: 20, D Loss: 3.3558084964752197, G Loss: 2.3397884368896484\n",
      "Epoch: 21, D Loss: -2.390984058380127, G Loss: 7.07830810546875\n",
      "Epoch: 22, D Loss: 4.3138203620910645, G Loss: 5.334742546081543\n",
      "Epoch: 23, D Loss: 3.2228188514709473, G Loss: 13.919374465942383\n",
      "Epoch: 24, D Loss: -1.369085431098938, G Loss: 11.693763732910156\n",
      "Epoch: 25, D Loss: 19.207744598388672, G Loss: 7.741971969604492\n",
      "Epoch: 26, D Loss: 1.0452172756195068, G Loss: 17.99298858642578\n",
      "Epoch: 27, D Loss: 0.6537414789199829, G Loss: 25.56581687927246\n",
      "Epoch: 28, D Loss: 0.1731637716293335, G Loss: 25.550880432128906\n",
      "Epoch: 29, D Loss: 32.512550354003906, G Loss: 2.197821617126465\n",
      "Epoch: 30, D Loss: -1.4366521835327148, G Loss: 10.882219314575195\n",
      "Epoch: 31, D Loss: -1.2279338836669922, G Loss: 14.318906784057617\n",
      "Epoch: 32, D Loss: -0.0014916658401489258, G Loss: 11.887956619262695\n",
      "Epoch: 33, D Loss: 0.7183024883270264, G Loss: 24.104698181152344\n",
      "Epoch: 34, D Loss: 2.1871228218078613, G Loss: 17.579748153686523\n",
      "Epoch: 35, D Loss: -3.7189114093780518, G Loss: 18.32870864868164\n",
      "Epoch: 36, D Loss: -13.172309875488281, G Loss: 41.320125579833984\n",
      "Epoch: 37, D Loss: 105.13365173339844, G Loss: 31.812042236328125\n",
      "Epoch: 38, D Loss: -10.352642059326172, G Loss: 103.20515441894531\n",
      "Epoch: 39, D Loss: -2.067136764526367, G Loss: 45.69810104370117\n",
      "Epoch: 40, D Loss: 8.96811294555664, G Loss: 26.484935760498047\n",
      "Epoch: 41, D Loss: -11.856135368347168, G Loss: 31.813329696655273\n",
      "Epoch: 42, D Loss: 8.739805221557617, G Loss: 0.11297494173049927\n",
      "Epoch: 43, D Loss: -2.9266111850738525, G Loss: 11.008167266845703\n",
      "Epoch: 44, D Loss: -1.9241753816604614, G Loss: 10.082721710205078\n",
      "Epoch: 45, D Loss: -4.980891227722168, G Loss: 16.813016891479492\n",
      "Epoch: 46, D Loss: -2.660902976989746, G Loss: 10.464006423950195\n",
      "Epoch: 47, D Loss: -7.76474142074585, G Loss: 23.13858985900879\n",
      "Epoch: 48, D Loss: -10.888429641723633, G Loss: 15.337698936462402\n",
      "Epoch: 49, D Loss: -6.836358070373535, G Loss: 11.740751266479492\n",
      "Epoch: 50, D Loss: -11.25913143157959, G Loss: 27.201953887939453\n",
      "Epoch: 51, D Loss: -7.49418830871582, G Loss: 27.3721923828125\n",
      "Epoch: 52, D Loss: -5.208914279937744, G Loss: 26.999557495117188\n",
      "Epoch: 53, D Loss: -0.08471333980560303, G Loss: 2.095003843307495\n",
      "Epoch: 54, D Loss: -1.5103161334991455, G Loss: 19.147579193115234\n",
      "Epoch: 55, D Loss: -1.8051941394805908, G Loss: 3.9295051097869873\n",
      "Epoch: 56, D Loss: -1.9122776985168457, G Loss: 12.027913093566895\n",
      "Epoch: 57, D Loss: -2.6688029766082764, G Loss: 17.297447204589844\n",
      "Epoch: 58, D Loss: -2.9171199798583984, G Loss: 7.533028602600098\n",
      "Epoch: 59, D Loss: 39.122962951660156, G Loss: 0.823032557964325\n",
      "Epoch: 60, D Loss: -2.779674530029297, G Loss: 6.664461135864258\n",
      "Epoch: 61, D Loss: 14.899930953979492, G Loss: 0.7621975541114807\n",
      "Epoch: 62, D Loss: -8.945127487182617, G Loss: 35.947296142578125\n",
      "Epoch: 63, D Loss: -5.706644058227539, G Loss: 36.811676025390625\n",
      "Epoch: 64, D Loss: -10.051358222961426, G Loss: 24.804927825927734\n",
      "Epoch: 65, D Loss: -3.1114678382873535, G Loss: 39.24560546875\n",
      "Epoch: 66, D Loss: -4.278562545776367, G Loss: 63.13645935058594\n",
      "Epoch: 67, D Loss: 29.62686538696289, G Loss: 64.00546264648438\n",
      "Epoch: 68, D Loss: -3.9007296562194824, G Loss: 70.94853973388672\n",
      "Epoch: 69, D Loss: -12.230806350708008, G Loss: 73.8257827758789\n",
      "Epoch: 70, D Loss: 51.24102020263672, G Loss: 44.93916320800781\n",
      "Epoch: 71, D Loss: -5.52197265625, G Loss: 101.01128387451172\n",
      "Epoch: 72, D Loss: -12.82564926147461, G Loss: 135.8275146484375\n",
      "Epoch: 73, D Loss: 11.184568405151367, G Loss: 14.581659317016602\n",
      "Epoch: 74, D Loss: -8.581104278564453, G Loss: 52.47992706298828\n",
      "Epoch: 75, D Loss: -12.541576385498047, G Loss: 60.59996032714844\n",
      "Epoch: 76, D Loss: -10.915071487426758, G Loss: 70.85000610351562\n",
      "Epoch: 77, D Loss: -25.464353561401367, G Loss: 52.17591857910156\n",
      "Epoch: 78, D Loss: -16.42792510986328, G Loss: 11.618682861328125\n",
      "Epoch: 79, D Loss: -8.984354019165039, G Loss: 53.18268585205078\n",
      "Epoch: 80, D Loss: 53.95094680786133, G Loss: 2.540154457092285\n",
      "Epoch: 81, D Loss: -15.611675262451172, G Loss: 45.12789535522461\n",
      "Epoch: 82, D Loss: 18.318864822387695, G Loss: 74.26937866210938\n",
      "Epoch: 83, D Loss: -40.3157844543457, G Loss: 57.0511474609375\n",
      "Epoch: 84, D Loss: -14.686362266540527, G Loss: 140.78497314453125\n",
      "Epoch: 85, D Loss: 314.09588623046875, G Loss: 226.16539001464844\n",
      "Epoch: 86, D Loss: 47.24905014038086, G Loss: 186.32635498046875\n",
      "Epoch: 87, D Loss: -13.793794631958008, G Loss: 214.8502197265625\n",
      "Epoch: 88, D Loss: 98.79158782958984, G Loss: 50.073585510253906\n",
      "Epoch: 89, D Loss: 18.892436981201172, G Loss: 83.80034637451172\n",
      "Epoch: 90, D Loss: -18.110294342041016, G Loss: 60.99089431762695\n",
      "Epoch: 91, D Loss: -46.79927444458008, G Loss: 45.95902633666992\n",
      "Epoch: 92, D Loss: -73.5597915649414, G Loss: 107.01319122314453\n",
      "Epoch: 93, D Loss: -118.94851684570312, G Loss: 122.6170425415039\n",
      "Epoch: 94, D Loss: -125.70887756347656, G Loss: 197.48068237304688\n",
      "Epoch: 95, D Loss: 18.505870819091797, G Loss: 364.6480407714844\n",
      "Epoch: 96, D Loss: 326.7611999511719, G Loss: 155.12179565429688\n",
      "Epoch: 97, D Loss: -110.28115844726562, G Loss: 204.425048828125\n",
      "Epoch: 98, D Loss: -32.108882904052734, G Loss: 271.80096435546875\n",
      "Epoch: 99, D Loss: 152.11253356933594, G Loss: 25.05883026123047\n",
      "Epoch: 100, D Loss: -9.879645347595215, G Loss: 207.37454223632812\n",
      "Epoch: 101, D Loss: 39.37750244140625, G Loss: 126.0598373413086\n",
      "Epoch: 102, D Loss: -31.662357330322266, G Loss: 293.03509521484375\n",
      "Epoch: 103, D Loss: -55.49707794189453, G Loss: 265.3340759277344\n",
      "Epoch: 104, D Loss: -48.57923126220703, G Loss: 328.2484130859375\n",
      "Epoch: 105, D Loss: -52.37794876098633, G Loss: 326.7033996582031\n",
      "Epoch: 106, D Loss: 38.42217254638672, G Loss: 204.09957885742188\n",
      "Epoch: 107, D Loss: -130.20428466796875, G Loss: 182.0380859375\n",
      "Epoch: 108, D Loss: -296.4791259765625, G Loss: 424.33038330078125\n",
      "Epoch: 109, D Loss: -251.85830688476562, G Loss: 362.4534912109375\n",
      "Epoch: 110, D Loss: 188.47198486328125, G Loss: 382.4320373535156\n",
      "Epoch: 111, D Loss: 208.60472106933594, G Loss: 417.3294372558594\n",
      "Epoch: 112, D Loss: 49.45697021484375, G Loss: 611.077392578125\n",
      "Epoch: 113, D Loss: -258.0950927734375, G Loss: 209.4717559814453\n",
      "Epoch: 114, D Loss: -101.39274597167969, G Loss: 385.95416259765625\n",
      "Epoch: 115, D Loss: 102.47306823730469, G Loss: 234.83091735839844\n",
      "Epoch: 116, D Loss: -166.68218994140625, G Loss: 253.38021850585938\n",
      "Epoch: 117, D Loss: -424.87921142578125, G Loss: 235.71365356445312\n",
      "Epoch: 118, D Loss: -256.4027099609375, G Loss: 338.5026550292969\n",
      "Epoch: 119, D Loss: -261.9950256347656, G Loss: 142.65371704101562\n",
      "Epoch: 120, D Loss: -61.828895568847656, G Loss: 32.5544548034668\n",
      "Epoch: 121, D Loss: -114.23162078857422, G Loss: 463.28973388671875\n",
      "Epoch: 122, D Loss: -136.5426025390625, G Loss: 90.11509704589844\n",
      "Epoch: 123, D Loss: -200.98306274414062, G Loss: 152.7381591796875\n",
      "Epoch: 124, D Loss: 72.69578552246094, G Loss: 307.9676513671875\n",
      "Epoch: 125, D Loss: 745.10546875, G Loss: 176.48648071289062\n",
      "Epoch: 126, D Loss: 91.84085083007812, G Loss: 525.7507934570312\n",
      "Epoch: 127, D Loss: 166.89102172851562, G Loss: 479.9108581542969\n",
      "Epoch: 128, D Loss: 998.2574462890625, G Loss: 528.23193359375\n",
      "Epoch: 129, D Loss: 195.31829833984375, G Loss: 118.533935546875\n",
      "Epoch: 130, D Loss: 359.7741394042969, G Loss: 8.667940139770508\n",
      "Epoch: 131, D Loss: -20.047536849975586, G Loss: 266.1815185546875\n",
      "Epoch: 132, D Loss: 30.973827362060547, G Loss: 129.427978515625\n",
      "Epoch: 133, D Loss: -8.739154815673828, G Loss: 299.6440734863281\n",
      "Epoch: 134, D Loss: -6.209778785705566, G Loss: 122.76448059082031\n",
      "Epoch: 135, D Loss: -5.964150905609131, G Loss: 121.08670043945312\n",
      "Epoch: 136, D Loss: -10.932149887084961, G Loss: 196.45944213867188\n",
      "Epoch: 137, D Loss: -10.654577255249023, G Loss: 64.33126831054688\n",
      "Epoch: 138, D Loss: -5.792274475097656, G Loss: 52.096702575683594\n",
      "Epoch: 139, D Loss: -12.749882698059082, G Loss: 134.84056091308594\n",
      "Epoch: 140, D Loss: -16.726884841918945, G Loss: 60.051971435546875\n",
      "Epoch: 141, D Loss: -17.958377838134766, G Loss: 138.1290740966797\n",
      "Epoch: 142, D Loss: -14.47900104522705, G Loss: 107.09109497070312\n",
      "Epoch: 143, D Loss: -12.286348342895508, G Loss: 140.87838745117188\n",
      "Epoch: 144, D Loss: -5.934814453125, G Loss: 71.77291107177734\n",
      "Epoch: 145, D Loss: -6.747237205505371, G Loss: 33.360740661621094\n",
      "Epoch: 146, D Loss: 20.329200744628906, G Loss: 36.04151916503906\n",
      "Epoch: 147, D Loss: -3.20881724357605, G Loss: 50.44247817993164\n",
      "Epoch: 148, D Loss: 10.969326972961426, G Loss: 34.620758056640625\n",
      "Epoch: 149, D Loss: 1.560243844985962, G Loss: 20.59496307373047\n",
      "Epoch: 150, D Loss: 3.8708629608154297, G Loss: 24.410234451293945\n",
      "Epoch: 151, D Loss: -3.691542625427246, G Loss: 45.456031799316406\n",
      "Epoch: 152, D Loss: 23.629810333251953, G Loss: 19.116931915283203\n",
      "Epoch: 153, D Loss: -13.679664611816406, G Loss: 142.78762817382812\n",
      "Epoch: 154, D Loss: -10.909442901611328, G Loss: 58.59223175048828\n",
      "Epoch: 155, D Loss: -14.31942367553711, G Loss: 83.15293884277344\n",
      "Epoch: 156, D Loss: -15.542658805847168, G Loss: 207.88259887695312\n",
      "Epoch: 157, D Loss: -23.311471939086914, G Loss: 178.02622985839844\n",
      "Epoch: 158, D Loss: -18.474525451660156, G Loss: 109.71861267089844\n",
      "Epoch: 159, D Loss: -11.907943725585938, G Loss: 106.26988983154297\n",
      "Epoch: 160, D Loss: 9.94986343383789, G Loss: 39.82793426513672\n",
      "Epoch: 161, D Loss: -2.6021692752838135, G Loss: 72.43450927734375\n",
      "Epoch: 162, D Loss: -0.10875582695007324, G Loss: 15.82800006866455\n",
      "Epoch: 163, D Loss: -10.516091346740723, G Loss: 69.69456481933594\n",
      "Epoch: 164, D Loss: -13.527900695800781, G Loss: 79.54370880126953\n",
      "Epoch: 165, D Loss: -24.116863250732422, G Loss: 117.43611907958984\n",
      "Epoch: 166, D Loss: -18.543651580810547, G Loss: 188.25192260742188\n",
      "Epoch: 167, D Loss: -7.667314529418945, G Loss: 125.44840240478516\n",
      "Epoch: 168, D Loss: 24.502548217773438, G Loss: 221.10293579101562\n",
      "Epoch: 169, D Loss: 14.842758178710938, G Loss: 168.30056762695312\n",
      "Epoch: 170, D Loss: 23.161251068115234, G Loss: 132.6702117919922\n",
      "Epoch: 171, D Loss: -3.569444417953491, G Loss: 118.42202758789062\n",
      "Epoch: 172, D Loss: -9.365618705749512, G Loss: 188.4388427734375\n",
      "Epoch: 173, D Loss: -12.997276306152344, G Loss: 121.36637878417969\n",
      "Epoch: 174, D Loss: -21.776010513305664, G Loss: 261.5693054199219\n",
      "Epoch: 175, D Loss: 59.58039474487305, G Loss: 103.61215209960938\n",
      "Epoch: 176, D Loss: 77.474365234375, G Loss: 47.8008918762207\n",
      "Epoch: 177, D Loss: -6.685553550720215, G Loss: 131.44033813476562\n",
      "Epoch: 178, D Loss: -12.524751663208008, G Loss: 306.4234313964844\n",
      "Epoch: 179, D Loss: -12.199413299560547, G Loss: 140.17935180664062\n",
      "Epoch: 180, D Loss: 7.503576755523682, G Loss: 180.28152465820312\n",
      "Epoch: 181, D Loss: 1.022402286529541, G Loss: 52.505615234375\n",
      "Epoch: 182, D Loss: -5.028936386108398, G Loss: 100.20919036865234\n",
      "Epoch: 183, D Loss: -10.703027725219727, G Loss: 52.3286247253418\n",
      "Epoch: 184, D Loss: -5.5536017417907715, G Loss: 119.74513244628906\n",
      "Epoch: 185, D Loss: -1.5695245265960693, G Loss: 97.12930297851562\n",
      "Epoch: 186, D Loss: -2.853623151779175, G Loss: 106.75740051269531\n",
      "Epoch: 187, D Loss: 11.678240776062012, G Loss: 47.68901824951172\n",
      "Epoch: 188, D Loss: 2.4523110389709473, G Loss: 128.3353729248047\n",
      "Epoch: 189, D Loss: 15.729061126708984, G Loss: 57.754329681396484\n",
      "Epoch: 190, D Loss: -7.285098075866699, G Loss: 107.4665756225586\n",
      "Epoch: 191, D Loss: -4.653531551361084, G Loss: 85.50247192382812\n",
      "Epoch: 192, D Loss: 3.060377836227417, G Loss: 86.4764404296875\n",
      "Epoch: 193, D Loss: 10.959864616394043, G Loss: 88.9063720703125\n",
      "Epoch: 194, D Loss: -1.1571934223175049, G Loss: 94.26362609863281\n",
      "Epoch: 195, D Loss: 7.195803642272949, G Loss: 43.90406799316406\n",
      "Epoch: 196, D Loss: -6.5687360763549805, G Loss: 125.34925842285156\n",
      "Epoch: 197, D Loss: -3.653385639190674, G Loss: 58.49357223510742\n",
      "Epoch: 198, D Loss: 23.05002212524414, G Loss: 146.96568298339844\n",
      "Epoch: 199, D Loss: -6.134296894073486, G Loss: 90.93315124511719\n",
      "Epoch: 200, D Loss: -8.595222473144531, G Loss: 40.01020050048828\n",
      "Epoch: 201, D Loss: -9.59776496887207, G Loss: 67.91127014160156\n",
      "Epoch: 202, D Loss: 1.1649751663208008, G Loss: 145.3334503173828\n",
      "Epoch: 203, D Loss: -169.45489501953125, G Loss: 439.35687255859375\n",
      "Epoch: 204, D Loss: -74.47344970703125, G Loss: 291.74481201171875\n",
      "Epoch: 205, D Loss: -85.88485717773438, G Loss: 145.5454559326172\n",
      "Epoch: 206, D Loss: -55.14449691772461, G Loss: 187.24642944335938\n",
      "Epoch: 207, D Loss: 5.5882110595703125, G Loss: 159.165283203125\n",
      "Epoch: 208, D Loss: -10.082635879516602, G Loss: 129.5701904296875\n",
      "Epoch: 209, D Loss: 1.0381025075912476, G Loss: 95.1888427734375\n",
      "Epoch: 210, D Loss: 31.08961296081543, G Loss: 448.80157470703125\n",
      "Epoch: 211, D Loss: -19.11688995361328, G Loss: 429.0743713378906\n",
      "Epoch: 212, D Loss: 4.484483242034912, G Loss: 383.51409912109375\n",
      "Epoch: 213, D Loss: -3.685516834259033, G Loss: 184.27835083007812\n",
      "Epoch: 214, D Loss: 60.45347595214844, G Loss: 140.09799194335938\n",
      "Epoch: 215, D Loss: -145.52406311035156, G Loss: 123.22122192382812\n",
      "Epoch: 216, D Loss: 65.12617492675781, G Loss: 240.8301544189453\n",
      "Epoch: 217, D Loss: 265.875732421875, G Loss: 214.52879333496094\n",
      "Epoch: 218, D Loss: 359.0153503417969, G Loss: 137.06689453125\n",
      "Epoch: 219, D Loss: 30.255666732788086, G Loss: 137.06214904785156\n",
      "Epoch: 220, D Loss: 79.47901916503906, G Loss: 93.19416046142578\n",
      "Epoch: 221, D Loss: 8.639841079711914, G Loss: 352.9872741699219\n",
      "Epoch: 222, D Loss: 93.228759765625, G Loss: 329.8017578125\n",
      "Epoch: 223, D Loss: -1.689072608947754, G Loss: 355.38482666015625\n",
      "Epoch: 224, D Loss: 163.71249389648438, G Loss: 172.99449157714844\n",
      "Epoch: 225, D Loss: 36.88837432861328, G Loss: 92.3944091796875\n",
      "Epoch: 226, D Loss: 2.9370508193969727, G Loss: 70.79588317871094\n",
      "Epoch: 227, D Loss: 4.4197306632995605, G Loss: 107.1822509765625\n",
      "Epoch: 228, D Loss: 17.774282455444336, G Loss: 63.8697509765625\n",
      "Epoch: 229, D Loss: 1.4272961616516113, G Loss: 94.62921142578125\n",
      "Epoch: 230, D Loss: 21.792152404785156, G Loss: 131.0984649658203\n",
      "Epoch: 231, D Loss: 15.68426513671875, G Loss: 137.56240844726562\n",
      "Epoch: 232, D Loss: -8.356622695922852, G Loss: 184.4851837158203\n",
      "Epoch: 233, D Loss: -5.3770270347595215, G Loss: 202.9944305419922\n",
      "Epoch: 234, D Loss: -4.505772590637207, G Loss: 172.23538208007812\n",
      "Epoch: 235, D Loss: -5.980452537536621, G Loss: 198.09620666503906\n",
      "Epoch: 236, D Loss: 7.522777557373047, G Loss: 115.50180053710938\n",
      "Epoch: 237, D Loss: 14.40436840057373, G Loss: 119.99494934082031\n",
      "Epoch: 238, D Loss: 24.39113998413086, G Loss: 102.63584899902344\n",
      "Epoch: 239, D Loss: 16.54087257385254, G Loss: 356.19293212890625\n",
      "Epoch: 240, D Loss: -3.998086452484131, G Loss: 145.49893188476562\n",
      "Epoch: 241, D Loss: 4.2595319747924805, G Loss: 177.18482971191406\n",
      "Epoch: 242, D Loss: 5.363142013549805, G Loss: 122.2855224609375\n",
      "Epoch: 243, D Loss: 16.658584594726562, G Loss: 64.6763687133789\n",
      "Epoch: 244, D Loss: 14.536857604980469, G Loss: 49.40723419189453\n",
      "Epoch: 245, D Loss: 2.9326720237731934, G Loss: 63.415428161621094\n",
      "Epoch: 246, D Loss: 7.771509647369385, G Loss: 174.54946899414062\n",
      "Epoch: 247, D Loss: -0.6760296821594238, G Loss: 165.2870635986328\n",
      "Epoch: 248, D Loss: -5.005786418914795, G Loss: 160.739990234375\n",
      "Epoch: 249, D Loss: 2.146682024002075, G Loss: 240.10104370117188\n",
      "Epoch: 250, D Loss: -0.6306648254394531, G Loss: 96.98355102539062\n",
      "Epoch: 251, D Loss: -12.343887329101562, G Loss: 259.1759033203125\n",
      "Epoch: 252, D Loss: -3.089277744293213, G Loss: 190.29464721679688\n",
      "Epoch: 253, D Loss: 3.978405475616455, G Loss: 122.69519805908203\n",
      "Epoch: 254, D Loss: 11.428187370300293, G Loss: 179.90147399902344\n",
      "Epoch: 255, D Loss: -5.382399559020996, G Loss: 160.4371337890625\n",
      "Epoch: 256, D Loss: 12.488213539123535, G Loss: 128.11936950683594\n",
      "Epoch: 257, D Loss: 515.1259765625, G Loss: 228.5821075439453\n",
      "Epoch: 258, D Loss: 20.182132720947266, G Loss: 287.8893127441406\n",
      "Epoch: 259, D Loss: -9.36376667022705, G Loss: 261.65478515625\n",
      "Epoch: 260, D Loss: -38.203895568847656, G Loss: 218.9839324951172\n",
      "Epoch: 261, D Loss: -153.48910522460938, G Loss: 171.18521118164062\n",
      "Epoch: 262, D Loss: -342.8196716308594, G Loss: 389.9778137207031\n",
      "Epoch: 263, D Loss: -587.2442016601562, G Loss: 283.24920654296875\n",
      "Epoch: 264, D Loss: -348.18402099609375, G Loss: 431.3369140625\n",
      "Epoch: 265, D Loss: 74.75717163085938, G Loss: 182.98019409179688\n",
      "Epoch: 266, D Loss: -1.480633020401001, G Loss: 272.0946044921875\n",
      "Epoch: 267, D Loss: -24.07506561279297, G Loss: 368.0191650390625\n",
      "Epoch: 268, D Loss: -9.647794723510742, G Loss: 150.80978393554688\n",
      "Epoch: 269, D Loss: 0.4696958065032959, G Loss: 50.104087829589844\n",
      "Epoch: 270, D Loss: 7.505759239196777, G Loss: 91.03094482421875\n",
      "Epoch: 271, D Loss: 13.288590431213379, G Loss: 84.2829818725586\n",
      "Epoch: 272, D Loss: 12.022034645080566, G Loss: 100.18072509765625\n",
      "Epoch: 273, D Loss: 1.9653198719024658, G Loss: 97.54818725585938\n",
      "Epoch: 274, D Loss: 10.072267532348633, G Loss: 109.7105712890625\n",
      "Epoch: 275, D Loss: -2.046086549758911, G Loss: 123.89118957519531\n",
      "Epoch: 276, D Loss: -12.43748664855957, G Loss: 91.93655395507812\n",
      "Epoch: 277, D Loss: -10.915664672851562, G Loss: 131.633056640625\n",
      "Epoch: 278, D Loss: -16.049957275390625, G Loss: 57.20244598388672\n",
      "Epoch: 279, D Loss: -7.4598708152771, G Loss: 56.49433898925781\n",
      "Epoch: 280, D Loss: -15.987401962280273, G Loss: 71.22139739990234\n",
      "Epoch: 281, D Loss: 2.824681282043457, G Loss: 41.92570114135742\n",
      "Epoch: 282, D Loss: 7.250601768493652, G Loss: 92.88815307617188\n",
      "Epoch: 283, D Loss: -15.339908599853516, G Loss: 117.41607666015625\n",
      "Epoch: 284, D Loss: -11.226581573486328, G Loss: 112.07516479492188\n",
      "Epoch: 285, D Loss: -4.079713821411133, G Loss: 101.51197814941406\n",
      "Epoch: 286, D Loss: -2.9781713485717773, G Loss: 92.87194061279297\n",
      "Epoch: 287, D Loss: -11.702192306518555, G Loss: 169.91973876953125\n",
      "Epoch: 288, D Loss: -2.660717010498047, G Loss: 160.0638427734375\n",
      "Epoch: 289, D Loss: 9.141441345214844, G Loss: 95.54936218261719\n",
      "Epoch: 290, D Loss: 2.147005319595337, G Loss: 163.40191650390625\n",
      "Epoch: 291, D Loss: -2.076575517654419, G Loss: 81.13601684570312\n",
      "Epoch: 292, D Loss: 16.870447158813477, G Loss: 72.31101989746094\n",
      "Epoch: 293, D Loss: 8.323606491088867, G Loss: 180.99969482421875\n",
      "Epoch: 294, D Loss: 7.473012447357178, G Loss: 136.3814697265625\n",
      "Epoch: 295, D Loss: -10.171134948730469, G Loss: 135.1259765625\n",
      "Epoch: 296, D Loss: -2.205411434173584, G Loss: 147.85691833496094\n",
      "Epoch: 297, D Loss: 2.811370611190796, G Loss: 171.5343017578125\n",
      "Epoch: 298, D Loss: 17.176132202148438, G Loss: 423.9678955078125\n",
      "Epoch: 299, D Loss: 2.88136625289917, G Loss: 82.78433227539062\n",
      "Epoch: 300, D Loss: 53.11301040649414, G Loss: 129.13748168945312\n",
      "Epoch: 301, D Loss: 1.373152732849121, G Loss: 103.92054748535156\n",
      "Epoch: 302, D Loss: 3.639063596725464, G Loss: 112.66191864013672\n",
      "Epoch: 303, D Loss: 2.3405911922454834, G Loss: 115.60420989990234\n",
      "Epoch: 304, D Loss: 2.5670418739318848, G Loss: 105.46900939941406\n",
      "Epoch: 305, D Loss: 1.4364814758300781, G Loss: 109.27456665039062\n",
      "Epoch: 306, D Loss: 7.542497158050537, G Loss: 73.5110092163086\n",
      "Epoch: 307, D Loss: 3.5127997398376465, G Loss: 182.2458953857422\n",
      "Epoch: 308, D Loss: 4.69499397277832, G Loss: 67.80056762695312\n",
      "Epoch: 309, D Loss: -0.317518949508667, G Loss: 166.01365661621094\n",
      "Epoch: 310, D Loss: 4.441800117492676, G Loss: 152.89984130859375\n",
      "Epoch: 311, D Loss: 3.028649091720581, G Loss: 222.2596893310547\n",
      "Epoch: 312, D Loss: -5.86796760559082, G Loss: 111.0455551147461\n",
      "Epoch: 313, D Loss: -0.7692594528198242, G Loss: 144.71218872070312\n",
      "Epoch: 314, D Loss: -3.574857711791992, G Loss: 173.89462280273438\n",
      "Epoch: 315, D Loss: -9.523904800415039, G Loss: 158.95782470703125\n",
      "Epoch: 316, D Loss: -7.4885406494140625, G Loss: 297.0062255859375\n",
      "Epoch: 317, D Loss: -5.645201683044434, G Loss: 277.79840087890625\n",
      "Epoch: 318, D Loss: 39.717018127441406, G Loss: 268.6782531738281\n",
      "Epoch: 319, D Loss: 37.08125305175781, G Loss: 272.0137023925781\n",
      "Epoch: 320, D Loss: 49.426578521728516, G Loss: 140.62852478027344\n",
      "Epoch: 321, D Loss: 80.28961944580078, G Loss: 119.72822570800781\n",
      "Epoch: 322, D Loss: 1.9854092597961426, G Loss: 122.88117980957031\n",
      "Epoch: 323, D Loss: 18.051448822021484, G Loss: 188.69662475585938\n",
      "Epoch: 324, D Loss: -14.203115463256836, G Loss: 171.32376098632812\n",
      "Epoch: 325, D Loss: -19.464014053344727, G Loss: 221.43954467773438\n",
      "Epoch: 326, D Loss: -20.48391342163086, G Loss: 117.48867797851562\n",
      "Epoch: 327, D Loss: -20.001127243041992, G Loss: 284.2734375\n",
      "Epoch: 328, D Loss: -4.693910598754883, G Loss: 124.96177673339844\n",
      "Epoch: 329, D Loss: -8.2787446975708, G Loss: 88.78059387207031\n",
      "Epoch: 330, D Loss: -9.156463623046875, G Loss: 84.89607238769531\n",
      "Epoch: 331, D Loss: -6.957294940948486, G Loss: 138.40289306640625\n",
      "Epoch: 332, D Loss: -10.38010025024414, G Loss: 100.26322174072266\n",
      "Epoch: 333, D Loss: -11.8780517578125, G Loss: 77.43933868408203\n",
      "Epoch: 334, D Loss: -5.124738693237305, G Loss: 249.9310760498047\n",
      "Epoch: 335, D Loss: -12.139810562133789, G Loss: 224.27322387695312\n",
      "Epoch: 336, D Loss: -18.768478393554688, G Loss: 137.74899291992188\n",
      "Epoch: 337, D Loss: -57.01716995239258, G Loss: 657.6298828125\n",
      "Epoch: 338, D Loss: 119.81341552734375, G Loss: 226.07293701171875\n",
      "Epoch: 339, D Loss: 17.170589447021484, G Loss: 310.29412841796875\n",
      "Epoch: 340, D Loss: 10.151447296142578, G Loss: 545.7093505859375\n",
      "Epoch: 341, D Loss: 26.882652282714844, G Loss: 237.50497436523438\n",
      "Epoch: 342, D Loss: 85.30828857421875, G Loss: 132.68687438964844\n",
      "Epoch: 343, D Loss: 17.385971069335938, G Loss: 191.115234375\n",
      "Epoch: 344, D Loss: 36.590003967285156, G Loss: 170.24288940429688\n",
      "Epoch: 345, D Loss: -6.311389923095703, G Loss: 227.40858459472656\n",
      "Epoch: 346, D Loss: 131.1199493408203, G Loss: 127.80009460449219\n",
      "Epoch: 347, D Loss: 35.70820999145508, G Loss: 163.59796142578125\n",
      "Epoch: 348, D Loss: -9.240358352661133, G Loss: 262.9736328125\n",
      "Epoch: 349, D Loss: -6.978955268859863, G Loss: 164.51126098632812\n",
      "Epoch: 350, D Loss: 1.45560884475708, G Loss: 122.94890594482422\n",
      "Epoch: 351, D Loss: 23.175220489501953, G Loss: 120.82577514648438\n",
      "Epoch: 352, D Loss: -3.3221521377563477, G Loss: 73.85810089111328\n",
      "Epoch: 353, D Loss: 8.337017059326172, G Loss: 61.812137603759766\n",
      "Epoch: 354, D Loss: 1.7555387020111084, G Loss: 90.57745361328125\n",
      "Epoch: 355, D Loss: -2.8412528038024902, G Loss: 131.35528564453125\n",
      "Epoch: 356, D Loss: -0.7436693906784058, G Loss: 99.05216217041016\n",
      "Epoch: 357, D Loss: -2.825777053833008, G Loss: 115.7717514038086\n",
      "Epoch: 358, D Loss: 9.9900484085083, G Loss: 104.58824920654297\n",
      "Epoch: 359, D Loss: 35.0191535949707, G Loss: 71.95785522460938\n",
      "Epoch: 360, D Loss: 26.96478271484375, G Loss: 90.85169219970703\n",
      "Epoch: 361, D Loss: 114.31871032714844, G Loss: 75.93840026855469\n",
      "Epoch: 362, D Loss: -6.921367168426514, G Loss: 130.62612915039062\n",
      "Epoch: 363, D Loss: -3.706246852874756, G Loss: 65.03575897216797\n",
      "Epoch: 364, D Loss: -15.189774513244629, G Loss: 110.10836791992188\n",
      "Epoch: 365, D Loss: -29.746213912963867, G Loss: 152.44723510742188\n",
      "Epoch: 366, D Loss: -27.487350463867188, G Loss: 135.4056396484375\n",
      "Epoch: 367, D Loss: -55.423789978027344, G Loss: 141.03021240234375\n",
      "Epoch: 368, D Loss: 289.2213134765625, G Loss: 62.78933334350586\n",
      "Epoch: 369, D Loss: -46.0098991394043, G Loss: 65.57838439941406\n",
      "Epoch: 370, D Loss: -68.23374938964844, G Loss: 195.2079620361328\n",
      "Epoch: 371, D Loss: -53.46719741821289, G Loss: 215.60385131835938\n",
      "Epoch: 372, D Loss: -46.14449691772461, G Loss: 159.27154541015625\n",
      "Epoch: 373, D Loss: -29.628366470336914, G Loss: 151.6149139404297\n",
      "Epoch: 374, D Loss: -63.85710906982422, G Loss: 249.74053955078125\n",
      "Epoch: 375, D Loss: -96.97177124023438, G Loss: 110.78829193115234\n",
      "Epoch: 376, D Loss: -271.51995849609375, G Loss: 214.42385864257812\n",
      "Epoch: 377, D Loss: -18.41460418701172, G Loss: 100.12071228027344\n",
      "Epoch: 378, D Loss: -19.061182022094727, G Loss: 271.6437072753906\n",
      "Epoch: 379, D Loss: 133.5718231201172, G Loss: 53.82443618774414\n",
      "Epoch: 380, D Loss: -115.48280334472656, G Loss: 84.55072021484375\n",
      "Epoch: 381, D Loss: -136.43331909179688, G Loss: 264.0765075683594\n",
      "Epoch: 382, D Loss: -187.38055419921875, G Loss: 361.1019592285156\n",
      "Epoch: 383, D Loss: -72.31254577636719, G Loss: 341.7142333984375\n",
      "Epoch: 384, D Loss: -94.92930603027344, G Loss: 282.0550537109375\n",
      "Epoch: 385, D Loss: -373.9765930175781, G Loss: 370.09490966796875\n",
      "Epoch: 386, D Loss: 0.43408870697021484, G Loss: 373.9013671875\n",
      "Epoch: 387, D Loss: -73.84642028808594, G Loss: 373.8408203125\n",
      "Epoch: 388, D Loss: 8.892463684082031, G Loss: 526.8458251953125\n",
      "Epoch: 389, D Loss: -93.3165512084961, G Loss: 65.53627014160156\n",
      "Epoch: 390, D Loss: -96.4001693725586, G Loss: 146.78634643554688\n",
      "Epoch: 391, D Loss: -199.38902282714844, G Loss: 348.6872863769531\n",
      "Epoch: 392, D Loss: -44.299991607666016, G Loss: 120.57733154296875\n",
      "Epoch: 393, D Loss: 406.5561218261719, G Loss: 208.44906616210938\n",
      "Epoch: 394, D Loss: -41.08274841308594, G Loss: 274.8717956542969\n",
      "Epoch: 395, D Loss: -54.81324005126953, G Loss: 324.175048828125\n",
      "Epoch: 396, D Loss: -59.56031036376953, G Loss: 137.7802276611328\n",
      "Epoch: 397, D Loss: -425.98291015625, G Loss: 66.81192779541016\n",
      "Epoch: 398, D Loss: -216.27301025390625, G Loss: 91.87992095947266\n",
      "Epoch: 399, D Loss: -348.8326110839844, G Loss: 171.49240112304688\n",
      "Epoch: 400, D Loss: 98.2624740600586, G Loss: 764.7066040039062\n",
      "Epoch: 401, D Loss: 349.3370361328125, G Loss: 128.9371795654297\n",
      "Epoch: 402, D Loss: 22.001163482666016, G Loss: 158.98626708984375\n",
      "Epoch: 403, D Loss: -17.86170196533203, G Loss: 589.9596557617188\n",
      "Epoch: 404, D Loss: -28.30147361755371, G Loss: 388.55426025390625\n",
      "Epoch: 405, D Loss: 24.459697723388672, G Loss: 183.95294189453125\n",
      "Epoch: 406, D Loss: -163.52346801757812, G Loss: 346.98028564453125\n",
      "Epoch: 407, D Loss: -168.60025024414062, G Loss: 168.69198608398438\n",
      "Epoch: 408, D Loss: -92.79851531982422, G Loss: 117.51429748535156\n",
      "Epoch: 409, D Loss: -98.41571044921875, G Loss: 157.0478515625\n",
      "Epoch: 410, D Loss: 200.29910278320312, G Loss: 267.259765625\n",
      "Epoch: 411, D Loss: -37.23507308959961, G Loss: 872.478271484375\n",
      "Epoch: 412, D Loss: 34.63725280761719, G Loss: 457.86529541015625\n",
      "Epoch: 413, D Loss: -275.44183349609375, G Loss: 181.94329833984375\n",
      "Epoch: 414, D Loss: -129.11526489257812, G Loss: 201.3243408203125\n",
      "Epoch: 415, D Loss: -210.47972106933594, G Loss: 316.79644775390625\n",
      "Epoch: 416, D Loss: -196.72422790527344, G Loss: 142.04660034179688\n",
      "Epoch: 417, D Loss: -377.2901611328125, G Loss: 332.9652404785156\n",
      "Epoch: 418, D Loss: -5.059581756591797, G Loss: 364.050537109375\n",
      "Epoch: 419, D Loss: -131.52890014648438, G Loss: 364.4251403808594\n",
      "Epoch: 420, D Loss: -253.26815795898438, G Loss: 575.956298828125\n",
      "Epoch: 421, D Loss: -288.6483459472656, G Loss: 167.29611206054688\n",
      "Epoch: 422, D Loss: -320.505859375, G Loss: 446.79034423828125\n",
      "Epoch: 423, D Loss: -233.69754028320312, G Loss: 423.01190185546875\n",
      "Epoch: 424, D Loss: -524.4869384765625, G Loss: 523.060546875\n",
      "Epoch: 425, D Loss: -332.8395690917969, G Loss: 124.9527587890625\n",
      "Epoch: 426, D Loss: -629.9751586914062, G Loss: 131.54122924804688\n",
      "Epoch: 427, D Loss: 235.3665771484375, G Loss: 253.54359436035156\n",
      "Epoch: 428, D Loss: 996.8812255859375, G Loss: 221.52586364746094\n",
      "Epoch: 429, D Loss: -45.375, G Loss: 332.21331787109375\n",
      "Epoch: 430, D Loss: 50.69019317626953, G Loss: 720.112060546875\n",
      "Epoch: 431, D Loss: -58.28597640991211, G Loss: 665.0426025390625\n",
      "Epoch: 432, D Loss: -779.35791015625, G Loss: 709.7970581054688\n",
      "Epoch: 433, D Loss: -575.358154296875, G Loss: 235.3826141357422\n",
      "Epoch: 434, D Loss: -167.31536865234375, G Loss: 551.8050537109375\n",
      "Epoch: 435, D Loss: -628.8740844726562, G Loss: 815.0477294921875\n",
      "Epoch: 436, D Loss: -202.62954711914062, G Loss: 739.9392700195312\n",
      "Epoch: 437, D Loss: -249.5751953125, G Loss: 284.2037353515625\n",
      "Epoch: 438, D Loss: -920.0062255859375, G Loss: 247.3982391357422\n",
      "Epoch: 439, D Loss: -1130.690185546875, G Loss: 396.70672607421875\n",
      "Epoch: 440, D Loss: -1437.247802734375, G Loss: 695.2188720703125\n",
      "Epoch: 441, D Loss: 473.2924499511719, G Loss: 726.7471923828125\n",
      "Epoch: 442, D Loss: 385.75567626953125, G Loss: 625.146484375\n",
      "Epoch: 443, D Loss: 67.91629028320312, G Loss: 113.59822845458984\n",
      "Epoch: 444, D Loss: 16.565074920654297, G Loss: 83.518310546875\n",
      "Epoch: 445, D Loss: 76.29460906982422, G Loss: 74.78034973144531\n",
      "Epoch: 446, D Loss: -3.5974321365356445, G Loss: 146.646240234375\n",
      "Epoch: 447, D Loss: 16.001676559448242, G Loss: 131.4111785888672\n",
      "Epoch: 448, D Loss: 15.69100284576416, G Loss: 324.042236328125\n",
      "Epoch: 449, D Loss: 13.752264976501465, G Loss: 169.65682983398438\n",
      "Epoch: 450, D Loss: -15.899442672729492, G Loss: 200.3106231689453\n",
      "Epoch: 451, D Loss: -44.033321380615234, G Loss: 152.55764770507812\n",
      "Epoch: 452, D Loss: -52.48982238769531, G Loss: 97.33651733398438\n",
      "Epoch: 453, D Loss: -132.728271484375, G Loss: 446.53350830078125\n",
      "Epoch: 454, D Loss: -208.76138305664062, G Loss: 301.26007080078125\n",
      "Epoch: 455, D Loss: 379.3907165527344, G Loss: 312.69384765625\n",
      "Epoch: 456, D Loss: -262.3038330078125, G Loss: 323.3595886230469\n",
      "Epoch: 457, D Loss: -307.79644775390625, G Loss: 348.13482666015625\n",
      "Epoch: 458, D Loss: -378.4305419921875, G Loss: 306.49542236328125\n",
      "Epoch: 459, D Loss: 294.2109375, G Loss: 229.08509826660156\n",
      "Epoch: 460, D Loss: -205.6815185546875, G Loss: 146.2967987060547\n",
      "Epoch: 461, D Loss: -150.9425506591797, G Loss: 282.3149108886719\n",
      "Epoch: 462, D Loss: -304.0560302734375, G Loss: 372.5827331542969\n",
      "Epoch: 463, D Loss: -1060.9027099609375, G Loss: 427.5228271484375\n",
      "Epoch: 464, D Loss: -653.54296875, G Loss: 795.0867309570312\n",
      "Epoch: 465, D Loss: -142.79159545898438, G Loss: 480.03973388671875\n",
      "Epoch: 466, D Loss: -284.9932861328125, G Loss: 304.292236328125\n",
      "Epoch: 467, D Loss: -96.35655212402344, G Loss: 208.63323974609375\n",
      "Epoch: 468, D Loss: -271.63623046875, G Loss: 143.35879516601562\n",
      "Epoch: 469, D Loss: 142.86354064941406, G Loss: 349.46942138671875\n",
      "Epoch: 470, D Loss: -796.36376953125, G Loss: 227.05477905273438\n",
      "Epoch: 471, D Loss: -1079.680908203125, G Loss: 182.12551879882812\n",
      "Epoch: 472, D Loss: -784.5233154296875, G Loss: 384.2991027832031\n",
      "Epoch: 473, D Loss: -794.0120849609375, G Loss: 150.667236328125\n",
      "Epoch: 474, D Loss: -434.0070495605469, G Loss: 188.97720336914062\n",
      "Epoch: 475, D Loss: -753.5409545898438, G Loss: 247.88487243652344\n",
      "Epoch: 476, D Loss: -528.17236328125, G Loss: 337.5809326171875\n",
      "Epoch: 477, D Loss: -839.5702514648438, G Loss: 607.0587158203125\n",
      "Epoch: 478, D Loss: -668.1490478515625, G Loss: 239.99539184570312\n",
      "Epoch: 479, D Loss: -1274.063720703125, G Loss: 172.52218627929688\n",
      "Epoch: 480, D Loss: -1826.005615234375, G Loss: 278.4404296875\n",
      "Epoch: 481, D Loss: -696.8980102539062, G Loss: 159.77496337890625\n",
      "Epoch: 482, D Loss: -877.0573120117188, G Loss: 483.1102294921875\n",
      "Epoch: 483, D Loss: -1445.047119140625, G Loss: 212.47303771972656\n",
      "Epoch: 484, D Loss: -1611.1485595703125, G Loss: 240.74554443359375\n",
      "Epoch: 485, D Loss: -1165.8582763671875, G Loss: 315.912841796875\n",
      "Epoch: 486, D Loss: -633.3873291015625, G Loss: 296.3014221191406\n",
      "Epoch: 487, D Loss: -1456.2236328125, G Loss: 483.61114501953125\n",
      "Epoch: 488, D Loss: -945.2127685546875, G Loss: 238.85267639160156\n",
      "Epoch: 489, D Loss: 836.4017333984375, G Loss: 1062.8885498046875\n",
      "Epoch: 490, D Loss: 3443.0615234375, G Loss: 2189.91748046875\n",
      "Epoch: 491, D Loss: 5318.72021484375, G Loss: 0.0\n",
      "Epoch: 492, D Loss: 429.7891845703125, G Loss: 123.58780670166016\n",
      "Epoch: 493, D Loss: 1337.42236328125, G Loss: 269.1171569824219\n",
      "Epoch: 494, D Loss: 150.30392456054688, G Loss: 1697.4840087890625\n",
      "Epoch: 495, D Loss: 117.18561553955078, G Loss: 84.46683502197266\n",
      "Epoch: 496, D Loss: 1.3827457427978516, G Loss: 197.51856994628906\n",
      "Epoch: 497, D Loss: -34.588226318359375, G Loss: 410.63458251953125\n",
      "Epoch: 498, D Loss: 322.1275634765625, G Loss: 171.52691650390625\n",
      "Epoch: 499, D Loss: 156.05950927734375, G Loss: 520.8626708984375\n",
      "Epoch: 500, D Loss: 226.68931579589844, G Loss: 759.2252197265625\n",
      "Epoch: 501, D Loss: 359.4351806640625, G Loss: 500.2919921875\n",
      "Epoch: 502, D Loss: 154.64901733398438, G Loss: 99.78326416015625\n",
      "Epoch: 503, D Loss: 67.32643127441406, G Loss: 351.65911865234375\n",
      "Epoch: 504, D Loss: 119.8968276977539, G Loss: 608.583984375\n",
      "Epoch: 505, D Loss: -22.665956497192383, G Loss: 278.9905090332031\n",
      "Epoch: 506, D Loss: -12.226840019226074, G Loss: 178.31448364257812\n",
      "Epoch: 507, D Loss: 10.519615173339844, G Loss: 102.36958312988281\n",
      "Epoch: 508, D Loss: -11.925143241882324, G Loss: 546.1950073242188\n",
      "Epoch: 509, D Loss: 37.6848258972168, G Loss: 88.49333190917969\n",
      "Epoch: 510, D Loss: -15.912954330444336, G Loss: 257.9967041015625\n",
      "Epoch: 511, D Loss: -12.143339157104492, G Loss: 239.42001342773438\n",
      "Epoch: 512, D Loss: 47.0342903137207, G Loss: 124.41905212402344\n",
      "Epoch: 513, D Loss: 70.74998474121094, G Loss: 204.54165649414062\n",
      "Epoch: 514, D Loss: 22.592681884765625, G Loss: 112.8213119506836\n",
      "Epoch: 515, D Loss: 33.672149658203125, G Loss: 221.14178466796875\n",
      "Epoch: 516, D Loss: 74.01995086669922, G Loss: 375.4208984375\n",
      "Epoch: 517, D Loss: 65.35285186767578, G Loss: 380.61676025390625\n",
      "Epoch: 518, D Loss: 40.53105163574219, G Loss: 453.1794738769531\n",
      "Epoch: 519, D Loss: 21.323976516723633, G Loss: 229.45428466796875\n",
      "Epoch: 520, D Loss: 16.2497615814209, G Loss: 101.29937744140625\n",
      "Epoch: 521, D Loss: 130.93238830566406, G Loss: 120.02806091308594\n",
      "Epoch: 522, D Loss: 38.478843688964844, G Loss: 666.1798095703125\n",
      "Epoch: 523, D Loss: 35.121055603027344, G Loss: 321.88232421875\n",
      "Epoch: 524, D Loss: -2.399265766143799, G Loss: 171.45823669433594\n",
      "Epoch: 525, D Loss: -19.445995330810547, G Loss: 356.4923095703125\n",
      "Epoch: 526, D Loss: 22.24814796447754, G Loss: 277.6315612792969\n",
      "Epoch: 527, D Loss: 17.409101486206055, G Loss: 136.12762451171875\n",
      "Epoch: 528, D Loss: 32.39747619628906, G Loss: 123.39952850341797\n",
      "Epoch: 529, D Loss: -14.941059112548828, G Loss: 83.05873107910156\n",
      "Epoch: 530, D Loss: -6.238345623016357, G Loss: 322.4682312011719\n",
      "Epoch: 531, D Loss: -15.679195404052734, G Loss: 331.7312316894531\n",
      "Epoch: 532, D Loss: -24.35427474975586, G Loss: 461.853515625\n",
      "Epoch: 533, D Loss: -23.422271728515625, G Loss: 520.2286987304688\n",
      "Epoch: 534, D Loss: -13.882862091064453, G Loss: 230.81854248046875\n",
      "Epoch: 535, D Loss: 16.839916229248047, G Loss: 198.83892822265625\n",
      "Epoch: 536, D Loss: 36.781219482421875, G Loss: 347.65277099609375\n",
      "Epoch: 537, D Loss: 64.15081787109375, G Loss: 53.83189392089844\n",
      "Epoch: 538, D Loss: 69.1151123046875, G Loss: 109.81011199951172\n",
      "Epoch: 539, D Loss: 59.2406005859375, G Loss: 202.47213745117188\n",
      "Epoch: 540, D Loss: 28.70490074157715, G Loss: 199.63525390625\n",
      "Epoch: 541, D Loss: 1.253239631652832, G Loss: 607.51611328125\n",
      "Epoch: 542, D Loss: 9.620626449584961, G Loss: 493.49871826171875\n",
      "Epoch: 543, D Loss: 11.667503356933594, G Loss: 527.4496459960938\n",
      "Epoch: 544, D Loss: 40.747989654541016, G Loss: 125.10192108154297\n",
      "Epoch: 545, D Loss: 76.74452209472656, G Loss: 104.37162780761719\n",
      "Epoch: 546, D Loss: 12.021210670471191, G Loss: 325.4231262207031\n",
      "Epoch: 547, D Loss: 26.202617645263672, G Loss: 442.8199462890625\n",
      "Epoch: 548, D Loss: 37.9166145324707, G Loss: 240.48895263671875\n",
      "Epoch: 549, D Loss: 18.5743408203125, G Loss: 354.9317626953125\n",
      "Epoch: 550, D Loss: 47.71490478515625, G Loss: 24.821393966674805\n",
      "Epoch: 551, D Loss: 52.13837432861328, G Loss: 106.61338806152344\n",
      "Epoch: 552, D Loss: 52.44010925292969, G Loss: 304.55657958984375\n",
      "Epoch: 553, D Loss: 106.12356567382812, G Loss: 310.6263427734375\n",
      "Epoch: 554, D Loss: 44.52994155883789, G Loss: 422.9088134765625\n",
      "Epoch: 555, D Loss: -20.684925079345703, G Loss: 558.6405029296875\n",
      "Epoch: 556, D Loss: 35.695648193359375, G Loss: 333.26715087890625\n",
      "Epoch: 557, D Loss: 145.38006591796875, G Loss: 34.407066345214844\n",
      "Epoch: 558, D Loss: 9.3861083984375, G Loss: 315.4222106933594\n",
      "Epoch: 559, D Loss: -12.097222328186035, G Loss: 71.44623565673828\n",
      "Epoch: 560, D Loss: 18.266136169433594, G Loss: 273.27081298828125\n",
      "Epoch: 561, D Loss: 8.856141090393066, G Loss: 318.2294921875\n",
      "Epoch: 562, D Loss: 5.055656433105469, G Loss: 231.53433227539062\n",
      "Epoch: 563, D Loss: -14.474845886230469, G Loss: 379.42724609375\n",
      "Epoch: 564, D Loss: -5.9140625, G Loss: 414.74041748046875\n",
      "Epoch: 565, D Loss: 17.542322158813477, G Loss: 560.5811157226562\n",
      "Epoch: 566, D Loss: -7.347179412841797, G Loss: 308.08917236328125\n",
      "Epoch: 567, D Loss: 4.660619258880615, G Loss: 346.0288391113281\n",
      "Epoch: 568, D Loss: -5.7389326095581055, G Loss: 520.1093139648438\n",
      "Epoch: 569, D Loss: -2.803217887878418, G Loss: 83.70233154296875\n",
      "Epoch: 570, D Loss: -8.351897239685059, G Loss: 211.74090576171875\n",
      "Epoch: 571, D Loss: -11.822236061096191, G Loss: 313.6223449707031\n",
      "Epoch: 572, D Loss: -9.765020370483398, G Loss: 41.31772232055664\n",
      "Epoch: 573, D Loss: -10.038420677185059, G Loss: 412.6526184082031\n",
      "Epoch: 574, D Loss: -10.37883186340332, G Loss: 102.16452026367188\n",
      "Epoch: 575, D Loss: -10.228629112243652, G Loss: 242.36248779296875\n",
      "Epoch: 576, D Loss: -3.0147671699523926, G Loss: 54.00299835205078\n",
      "Epoch: 577, D Loss: 4.5078325271606445, G Loss: 150.68360900878906\n",
      "Epoch: 578, D Loss: 27.672883987426758, G Loss: 382.6390075683594\n",
      "Epoch: 579, D Loss: 6.666597366333008, G Loss: 227.5986328125\n",
      "Epoch: 580, D Loss: -14.210561752319336, G Loss: 435.00537109375\n",
      "Epoch: 581, D Loss: -0.8657407760620117, G Loss: 380.45025634765625\n",
      "Epoch: 582, D Loss: 45.60980224609375, G Loss: 391.0742492675781\n",
      "Epoch: 583, D Loss: 72.64886474609375, G Loss: 119.47067260742188\n",
      "Epoch: 584, D Loss: 50.172645568847656, G Loss: 251.1399383544922\n",
      "Epoch: 585, D Loss: 40.59412384033203, G Loss: 229.55467224121094\n",
      "Epoch: 586, D Loss: 85.76243591308594, G Loss: 304.48272705078125\n",
      "Epoch: 587, D Loss: 74.4482421875, G Loss: 480.866455078125\n",
      "Epoch: 588, D Loss: 84.82389831542969, G Loss: 374.1845397949219\n",
      "Epoch: 589, D Loss: 79.90379333496094, G Loss: 605.9375610351562\n",
      "Epoch: 590, D Loss: 63.743125915527344, G Loss: 287.52239990234375\n",
      "Epoch: 591, D Loss: 71.79486083984375, G Loss: 313.77606201171875\n",
      "Epoch: 592, D Loss: 25.77155303955078, G Loss: 242.71446228027344\n",
      "Epoch: 593, D Loss: 28.568748474121094, G Loss: 443.8750305175781\n",
      "Epoch: 594, D Loss: 62.89739227294922, G Loss: 675.0807495117188\n",
      "Epoch: 595, D Loss: 14.407472610473633, G Loss: 167.42752075195312\n",
      "Epoch: 596, D Loss: 20.21649932861328, G Loss: 193.1156463623047\n",
      "Epoch: 597, D Loss: 13.256513595581055, G Loss: 317.438720703125\n",
      "Epoch: 598, D Loss: 26.92885971069336, G Loss: 479.2041931152344\n",
      "Epoch: 599, D Loss: 67.35787963867188, G Loss: 410.93804931640625\n",
      "Epoch: 600, D Loss: -10.533182144165039, G Loss: 157.30372619628906\n",
      "Epoch: 601, D Loss: -15.066062927246094, G Loss: 213.69850158691406\n",
      "Epoch: 602, D Loss: -13.821534156799316, G Loss: 222.93234252929688\n",
      "Epoch: 603, D Loss: -18.966148376464844, G Loss: 156.95498657226562\n",
      "Epoch: 604, D Loss: -26.713314056396484, G Loss: 363.9098205566406\n",
      "Epoch: 605, D Loss: -25.486452102661133, G Loss: 351.45538330078125\n",
      "Epoch: 606, D Loss: 8.983173370361328, G Loss: 342.36224365234375\n",
      "Epoch: 607, D Loss: 9.441370010375977, G Loss: 130.15162658691406\n",
      "Epoch: 608, D Loss: 2.607879161834717, G Loss: 105.8145523071289\n",
      "Epoch: 609, D Loss: 32.95627212524414, G Loss: 19.15772247314453\n",
      "Epoch: 610, D Loss: -2.2448039054870605, G Loss: 267.7681884765625\n",
      "Epoch: 611, D Loss: 12.654275894165039, G Loss: 287.72930908203125\n",
      "Epoch: 612, D Loss: -18.101335525512695, G Loss: 446.51531982421875\n",
      "Epoch: 613, D Loss: -13.620986938476562, G Loss: 634.599609375\n",
      "Epoch: 614, D Loss: -32.80918884277344, G Loss: 102.96395111083984\n",
      "Epoch: 615, D Loss: 9.23370361328125, G Loss: 398.4837341308594\n",
      "Epoch: 616, D Loss: 2.3216776847839355, G Loss: 163.14988708496094\n",
      "Epoch: 617, D Loss: 14.57992935180664, G Loss: 175.70919799804688\n",
      "Epoch: 618, D Loss: 9.157270431518555, G Loss: 165.83859252929688\n",
      "Epoch: 619, D Loss: -20.065515518188477, G Loss: 235.23330688476562\n",
      "Epoch: 620, D Loss: -36.39952087402344, G Loss: 312.831298828125\n",
      "Epoch: 621, D Loss: 1.1789941787719727, G Loss: 738.2217407226562\n",
      "Epoch: 622, D Loss: 33.59245300292969, G Loss: 389.61749267578125\n",
      "Epoch: 623, D Loss: 6.567264556884766, G Loss: 351.9285888671875\n",
      "Epoch: 624, D Loss: -102.16806030273438, G Loss: 430.9261474609375\n",
      "Epoch: 625, D Loss: -5057.279296875, G Loss: 978.5548706054688\n",
      "Epoch: 626, D Loss: -841.259765625, G Loss: 1582.876953125\n",
      "Epoch: 627, D Loss: 3881.05615234375, G Loss: 940.6856689453125\n",
      "Epoch: 628, D Loss: 1505.0322265625, G Loss: 159.5447235107422\n",
      "Epoch: 629, D Loss: 689.5927124023438, G Loss: 1125.5892333984375\n",
      "Epoch: 630, D Loss: 395.9482116699219, G Loss: 541.830810546875\n",
      "Epoch: 631, D Loss: 948.3615112304688, G Loss: 0.0\n",
      "Epoch: 632, D Loss: -14.786665916442871, G Loss: 1190.414794921875\n",
      "Epoch: 633, D Loss: 302.6213684082031, G Loss: 320.6918640136719\n",
      "Epoch: 634, D Loss: 70.03667449951172, G Loss: 66.03297424316406\n",
      "Epoch: 635, D Loss: 227.533447265625, G Loss: 17.197547912597656\n",
      "Epoch: 636, D Loss: 110.3485336303711, G Loss: 681.5882568359375\n",
      "Epoch: 637, D Loss: 50.98042297363281, G Loss: 244.6377716064453\n",
      "Epoch: 638, D Loss: 80.27680206298828, G Loss: 157.94342041015625\n",
      "Epoch: 639, D Loss: 0.8784565925598145, G Loss: 266.5444030761719\n",
      "Epoch: 640, D Loss: 40.62683868408203, G Loss: 819.5888671875\n",
      "Epoch: 641, D Loss: 96.72654724121094, G Loss: 764.8406982421875\n",
      "Epoch: 642, D Loss: 29.858535766601562, G Loss: 313.6235656738281\n",
      "Epoch: 643, D Loss: 15.344179153442383, G Loss: 182.33120727539062\n",
      "Epoch: 644, D Loss: 11.969863891601562, G Loss: 716.5419921875\n",
      "Epoch: 645, D Loss: -4.343554496765137, G Loss: 245.12716674804688\n",
      "Epoch: 646, D Loss: 23.57001304626465, G Loss: 502.9385681152344\n",
      "Epoch: 647, D Loss: 136.2000274658203, G Loss: 63.93946838378906\n",
      "Epoch: 648, D Loss: 53.3213005065918, G Loss: 1017.546630859375\n",
      "Epoch: 649, D Loss: 135.9873809814453, G Loss: 28.468643188476562\n",
      "Epoch: 650, D Loss: 89.72749328613281, G Loss: 83.48917388916016\n",
      "Epoch: 651, D Loss: 12.197709083557129, G Loss: 545.0330810546875\n",
      "Epoch: 652, D Loss: 86.2745361328125, G Loss: 277.65814208984375\n",
      "Epoch: 653, D Loss: 108.55091094970703, G Loss: 135.0460205078125\n",
      "Epoch: 654, D Loss: 12.026113510131836, G Loss: 337.5602111816406\n",
      "Epoch: 655, D Loss: 20.915878295898438, G Loss: 299.7684326171875\n",
      "Epoch: 656, D Loss: 130.87599182128906, G Loss: 100.3736572265625\n",
      "Epoch: 657, D Loss: 44.17818832397461, G Loss: 120.2958755493164\n",
      "Epoch: 658, D Loss: 24.366228103637695, G Loss: 101.21954345703125\n",
      "Epoch: 659, D Loss: 9.081454277038574, G Loss: 88.1989974975586\n",
      "Epoch: 660, D Loss: -12.8251314163208, G Loss: 579.2376708984375\n",
      "Epoch: 661, D Loss: -16.74634552001953, G Loss: 211.5955810546875\n",
      "Epoch: 662, D Loss: -9.470765113830566, G Loss: 137.8083038330078\n",
      "Epoch: 663, D Loss: -27.701160430908203, G Loss: 164.33792114257812\n",
      "Epoch: 664, D Loss: 14.747395515441895, G Loss: 79.65351104736328\n",
      "Epoch: 665, D Loss: 11.205421447753906, G Loss: 323.820556640625\n",
      "Epoch: 666, D Loss: 30.285449981689453, G Loss: 261.11309814453125\n",
      "Epoch: 667, D Loss: 36.537052154541016, G Loss: 118.97874450683594\n",
      "Epoch: 668, D Loss: -4.199331283569336, G Loss: 207.969482421875\n",
      "Epoch: 669, D Loss: 28.48834991455078, G Loss: 167.34552001953125\n",
      "Epoch: 670, D Loss: 2.3190841674804688, G Loss: 237.408203125\n",
      "Epoch: 671, D Loss: -11.040460586547852, G Loss: 418.67633056640625\n",
      "Epoch: 672, D Loss: 4.326665878295898, G Loss: 515.180419921875\n",
      "Epoch: 673, D Loss: 4.634726047515869, G Loss: 349.11444091796875\n",
      "Epoch: 674, D Loss: 11.73476505279541, G Loss: 86.31014251708984\n",
      "Epoch: 675, D Loss: -2.631284713745117, G Loss: 267.22314453125\n",
      "Epoch: 676, D Loss: 11.25731372833252, G Loss: 350.3533935546875\n",
      "Epoch: 677, D Loss: 6.027917861938477, G Loss: 263.5792541503906\n",
      "Epoch: 678, D Loss: 13.135692596435547, G Loss: 365.1282043457031\n",
      "Epoch: 679, D Loss: 60.30378341674805, G Loss: 563.0028686523438\n",
      "Epoch: 680, D Loss: 24.04595947265625, G Loss: 124.95478820800781\n",
      "Epoch: 681, D Loss: 9.3430814743042, G Loss: 447.1610107421875\n",
      "Epoch: 682, D Loss: 24.81229591369629, G Loss: 270.1839599609375\n",
      "Epoch: 683, D Loss: 7.089564323425293, G Loss: 486.0010070800781\n",
      "Epoch: 684, D Loss: 30.205902099609375, G Loss: 464.65533447265625\n",
      "Epoch: 685, D Loss: 24.98056983947754, G Loss: 67.63941955566406\n",
      "Epoch: 686, D Loss: 97.36524963378906, G Loss: 123.418212890625\n",
      "Epoch: 687, D Loss: 0.31219482421875, G Loss: 121.17898559570312\n",
      "Epoch: 688, D Loss: -0.3780860900878906, G Loss: 148.33120727539062\n",
      "Epoch: 689, D Loss: 18.52093505859375, G Loss: 562.0733032226562\n",
      "Epoch: 690, D Loss: 15.479508399963379, G Loss: 321.91168212890625\n",
      "Epoch: 691, D Loss: 18.874073028564453, G Loss: 1187.70947265625\n",
      "Epoch: 692, D Loss: -5.643220901489258, G Loss: 109.45730590820312\n",
      "Epoch: 693, D Loss: -10.38778305053711, G Loss: 76.92967224121094\n",
      "Epoch: 694, D Loss: 89.02240753173828, G Loss: 1238.306396484375\n",
      "Epoch: 695, D Loss: 28.30596923828125, G Loss: 445.6416015625\n",
      "Epoch: 696, D Loss: -8.37649917602539, G Loss: 559.3134155273438\n",
      "Epoch: 697, D Loss: 2.0064101219177246, G Loss: 204.14939880371094\n",
      "Epoch: 698, D Loss: 9.06538200378418, G Loss: 403.0464782714844\n",
      "Epoch: 699, D Loss: -13.838403701782227, G Loss: 561.977294921875\n",
      "Epoch: 700, D Loss: 65.60460662841797, G Loss: 120.60415649414062\n",
      "Epoch: 701, D Loss: 22.40683937072754, G Loss: 169.5423583984375\n",
      "Epoch: 702, D Loss: 7.036384105682373, G Loss: 407.9795227050781\n",
      "Epoch: 703, D Loss: 37.99163818359375, G Loss: 219.3082733154297\n",
      "Epoch: 704, D Loss: 15.913042068481445, G Loss: 347.6505126953125\n",
      "Epoch: 705, D Loss: -0.9688005447387695, G Loss: 355.9185791015625\n",
      "Epoch: 706, D Loss: -32.122718811035156, G Loss: 366.2083435058594\n",
      "Epoch: 707, D Loss: -22.932403564453125, G Loss: 311.8401184082031\n",
      "Epoch: 708, D Loss: -3.1302661895751953, G Loss: 496.030517578125\n",
      "Epoch: 709, D Loss: 7.6839823722839355, G Loss: 494.46258544921875\n",
      "Epoch: 710, D Loss: 80.56512451171875, G Loss: 85.80496978759766\n",
      "Epoch: 711, D Loss: 8.052035331726074, G Loss: 528.9259643554688\n",
      "Epoch: 712, D Loss: 12.068193435668945, G Loss: 317.1105651855469\n",
      "Epoch: 713, D Loss: -11.231882095336914, G Loss: 504.6667785644531\n",
      "Epoch: 714, D Loss: 13.464077949523926, G Loss: 858.7366333007812\n",
      "Epoch: 715, D Loss: -6.621891498565674, G Loss: 415.12841796875\n",
      "Epoch: 716, D Loss: -13.252533912658691, G Loss: 451.2437744140625\n",
      "Epoch: 717, D Loss: 17.023723602294922, G Loss: 296.6795654296875\n",
      "Epoch: 718, D Loss: -0.5189783573150635, G Loss: 107.26724243164062\n",
      "Epoch: 719, D Loss: 0.3005039691925049, G Loss: 164.86260986328125\n",
      "Epoch: 720, D Loss: 36.67668914794922, G Loss: 372.96856689453125\n",
      "Epoch: 721, D Loss: 86.45537567138672, G Loss: 149.94847106933594\n",
      "Epoch: 722, D Loss: 31.441404342651367, G Loss: 212.67282104492188\n",
      "Epoch: 723, D Loss: 23.377962112426758, G Loss: 360.84442138671875\n",
      "Epoch: 724, D Loss: 66.11822509765625, G Loss: 330.4569091796875\n",
      "Epoch: 725, D Loss: 54.95421600341797, G Loss: 867.18212890625\n",
      "Epoch: 726, D Loss: 13.796182632446289, G Loss: 569.07080078125\n",
      "Epoch: 727, D Loss: 14.592413902282715, G Loss: 502.5711975097656\n",
      "Epoch: 728, D Loss: 26.07549285888672, G Loss: 448.7713928222656\n",
      "Epoch: 729, D Loss: -12.587610244750977, G Loss: 325.738037109375\n",
      "Epoch: 730, D Loss: 25.613771438598633, G Loss: 449.33538818359375\n",
      "Epoch: 731, D Loss: 19.61181640625, G Loss: 430.31182861328125\n",
      "Epoch: 732, D Loss: 3.9407289028167725, G Loss: 318.5546569824219\n",
      "Epoch: 733, D Loss: -2.5774264335632324, G Loss: 399.0223388671875\n",
      "Epoch: 734, D Loss: -6.802024841308594, G Loss: 799.349609375\n",
      "Epoch: 735, D Loss: 42.183189392089844, G Loss: 415.9717712402344\n",
      "Epoch: 736, D Loss: 52.46253204345703, G Loss: 671.4832763671875\n",
      "Epoch: 737, D Loss: 6.766387939453125, G Loss: 444.2249450683594\n",
      "Epoch: 738, D Loss: -11.284469604492188, G Loss: 540.9826049804688\n",
      "Epoch: 739, D Loss: -23.46534538269043, G Loss: 689.39404296875\n",
      "Epoch: 740, D Loss: 16.075546264648438, G Loss: 592.1649780273438\n",
      "Epoch: 741, D Loss: 3.832352638244629, G Loss: 493.05230712890625\n",
      "Epoch: 742, D Loss: -0.7139921188354492, G Loss: 696.1766967773438\n",
      "Epoch: 743, D Loss: -30.099292755126953, G Loss: 230.04745483398438\n",
      "Epoch: 744, D Loss: -14.776900291442871, G Loss: 174.7854461669922\n",
      "Epoch: 745, D Loss: -22.710678100585938, G Loss: 160.5169219970703\n",
      "Epoch: 746, D Loss: -21.058259963989258, G Loss: 173.3307342529297\n",
      "Epoch: 747, D Loss: -28.73975372314453, G Loss: 219.7568817138672\n",
      "Epoch: 748, D Loss: -20.870546340942383, G Loss: 459.54266357421875\n",
      "Epoch: 749, D Loss: -26.365680694580078, G Loss: 293.5829162597656\n",
      "Epoch: 750, D Loss: -38.461212158203125, G Loss: 202.21145629882812\n",
      "Epoch: 751, D Loss: -20.42302131652832, G Loss: 199.489013671875\n",
      "Epoch: 752, D Loss: 11.245800018310547, G Loss: 264.05145263671875\n",
      "Epoch: 753, D Loss: 7.611799716949463, G Loss: 291.6781921386719\n",
      "Epoch: 754, D Loss: -23.285818099975586, G Loss: 273.6607666015625\n",
      "Epoch: 755, D Loss: -21.979154586791992, G Loss: 195.283203125\n",
      "Epoch: 756, D Loss: -3.353072166442871, G Loss: 454.9147644042969\n",
      "Epoch: 757, D Loss: -18.085065841674805, G Loss: 249.98516845703125\n",
      "Epoch: 758, D Loss: -26.417362213134766, G Loss: 410.3654479980469\n",
      "Epoch: 759, D Loss: -29.348894119262695, G Loss: 430.572509765625\n",
      "Epoch: 760, D Loss: -25.638031005859375, G Loss: 623.595947265625\n",
      "Epoch: 761, D Loss: -28.169002532958984, G Loss: 505.9635009765625\n",
      "Epoch: 762, D Loss: -33.439170837402344, G Loss: 334.1436767578125\n",
      "Epoch: 763, D Loss: -2.701056480407715, G Loss: 447.04888916015625\n",
      "Epoch: 764, D Loss: -17.80608558654785, G Loss: 337.4841003417969\n",
      "Epoch: 765, D Loss: -13.32402229309082, G Loss: 477.65325927734375\n",
      "Epoch: 766, D Loss: -34.640724182128906, G Loss: 192.484130859375\n",
      "Epoch: 767, D Loss: -36.614959716796875, G Loss: 511.5725402832031\n",
      "Epoch: 768, D Loss: -22.21770477294922, G Loss: 424.89471435546875\n",
      "Epoch: 769, D Loss: -8.251540184020996, G Loss: 448.4172058105469\n",
      "Epoch: 770, D Loss: -40.83872604370117, G Loss: 288.88677978515625\n",
      "Epoch: 771, D Loss: -7.496241569519043, G Loss: 268.0538330078125\n",
      "Epoch: 772, D Loss: 23.834453582763672, G Loss: 362.91571044921875\n",
      "Epoch: 773, D Loss: -0.5445780754089355, G Loss: 270.6907958984375\n",
      "Epoch: 774, D Loss: 19.3008975982666, G Loss: 309.0094299316406\n",
      "Epoch: 775, D Loss: -17.839157104492188, G Loss: 379.36541748046875\n",
      "Epoch: 776, D Loss: -26.144926071166992, G Loss: 436.1220397949219\n",
      "Epoch: 777, D Loss: -29.76746368408203, G Loss: 430.377685546875\n",
      "Epoch: 778, D Loss: 405.0848388671875, G Loss: 357.35980224609375\n",
      "Epoch: 779, D Loss: 62.96178436279297, G Loss: 790.44921875\n",
      "Epoch: 780, D Loss: 283.2071838378906, G Loss: 358.71734619140625\n",
      "Epoch: 781, D Loss: 3.094874382019043, G Loss: 327.4922180175781\n",
      "Epoch: 782, D Loss: 68.67941284179688, G Loss: 254.04464721679688\n",
      "Epoch: 783, D Loss: 42.95969009399414, G Loss: 429.8744812011719\n",
      "Epoch: 784, D Loss: 25.52609634399414, G Loss: 702.880615234375\n",
      "Epoch: 785, D Loss: 37.81648635864258, G Loss: 276.0671691894531\n",
      "Epoch: 786, D Loss: -11.285234451293945, G Loss: 437.01275634765625\n",
      "Epoch: 787, D Loss: 76.305419921875, G Loss: 750.6483154296875\n",
      "Epoch: 788, D Loss: 28.103038787841797, G Loss: 585.9832763671875\n",
      "Epoch: 789, D Loss: 106.33248901367188, G Loss: 198.18190002441406\n",
      "Epoch: 790, D Loss: -11.181428909301758, G Loss: 377.65362548828125\n",
      "Epoch: 791, D Loss: 120.80787658691406, G Loss: 244.39280700683594\n",
      "Epoch: 792, D Loss: 11.597248077392578, G Loss: 306.7566223144531\n",
      "Epoch: 793, D Loss: 9.664098739624023, G Loss: 249.5793914794922\n",
      "Epoch: 794, D Loss: 11.48862075805664, G Loss: 125.73948669433594\n",
      "Epoch: 795, D Loss: 2.777970314025879, G Loss: 253.85984802246094\n",
      "Epoch: 796, D Loss: 7.1762189865112305, G Loss: 97.82530212402344\n",
      "Epoch: 797, D Loss: -34.810237884521484, G Loss: 250.7561798095703\n",
      "Epoch: 798, D Loss: 12.316136360168457, G Loss: 237.9921875\n",
      "Epoch: 799, D Loss: 158.64111328125, G Loss: 153.44302368164062\n",
      "Epoch: 800, D Loss: 19.53018569946289, G Loss: 428.722412109375\n",
      "Epoch: 801, D Loss: 67.32212829589844, G Loss: 691.1445922851562\n",
      "Epoch: 802, D Loss: 21.86223030090332, G Loss: 134.72604370117188\n",
      "Epoch: 803, D Loss: 3.7000691890716553, G Loss: 386.47332763671875\n",
      "Epoch: 804, D Loss: 41.68135070800781, G Loss: 366.3380432128906\n",
      "Epoch: 805, D Loss: -12.49629020690918, G Loss: 337.10382080078125\n",
      "Epoch: 806, D Loss: 49.517520904541016, G Loss: 892.3282470703125\n",
      "Epoch: 807, D Loss: -11.516435623168945, G Loss: 591.4708862304688\n",
      "Epoch: 808, D Loss: -23.667198181152344, G Loss: 436.0238037109375\n",
      "Epoch: 809, D Loss: -17.0177001953125, G Loss: 138.7377471923828\n",
      "Epoch: 810, D Loss: -5.22989559173584, G Loss: 419.193603515625\n",
      "Epoch: 811, D Loss: -5.11476993560791, G Loss: 394.1195068359375\n",
      "Epoch: 812, D Loss: 24.029207229614258, G Loss: 226.03317260742188\n",
      "Epoch: 813, D Loss: 49.549015045166016, G Loss: 75.531494140625\n",
      "Epoch: 814, D Loss: 10.474801063537598, G Loss: 457.085693359375\n",
      "Epoch: 815, D Loss: 6.294535160064697, G Loss: 407.7496032714844\n",
      "Epoch: 816, D Loss: 5.10642147064209, G Loss: 911.155517578125\n",
      "Epoch: 817, D Loss: -13.359537124633789, G Loss: 402.8656005859375\n",
      "Epoch: 818, D Loss: 45.9174919128418, G Loss: 264.8424072265625\n",
      "Epoch: 819, D Loss: 87.72427368164062, G Loss: 254.79437255859375\n",
      "Epoch: 820, D Loss: 69.46469116210938, G Loss: 172.65032958984375\n",
      "Epoch: 821, D Loss: 62.731388092041016, G Loss: 96.83694458007812\n",
      "Epoch: 822, D Loss: 65.15933227539062, G Loss: 349.11431884765625\n",
      "Epoch: 823, D Loss: -11.933865547180176, G Loss: 287.543212890625\n",
      "Epoch: 824, D Loss: 7.292257308959961, G Loss: 513.1149291992188\n",
      "Epoch: 825, D Loss: 30.19937515258789, G Loss: 61.24213409423828\n",
      "Epoch: 826, D Loss: 25.221534729003906, G Loss: 471.9171142578125\n",
      "Epoch: 827, D Loss: 15.997751235961914, G Loss: 209.71231079101562\n",
      "Epoch: 828, D Loss: 13.30443000793457, G Loss: 123.2259521484375\n",
      "Epoch: 829, D Loss: -0.8582642078399658, G Loss: 165.45347595214844\n",
      "Epoch: 830, D Loss: -7.089613914489746, G Loss: 350.641845703125\n",
      "Epoch: 831, D Loss: -14.03635025024414, G Loss: 354.4534912109375\n",
      "Epoch: 832, D Loss: -8.78303337097168, G Loss: 345.12542724609375\n",
      "Epoch: 833, D Loss: 0.22041034698486328, G Loss: 322.59552001953125\n",
      "Epoch: 834, D Loss: 2.620645523071289, G Loss: 317.60797119140625\n",
      "Epoch: 835, D Loss: -27.812911987304688, G Loss: 317.12530517578125\n",
      "Epoch: 836, D Loss: -35.0810546875, G Loss: 272.83001708984375\n",
      "Epoch: 837, D Loss: -327.298583984375, G Loss: 249.53541564941406\n",
      "Epoch: 838, D Loss: -255.21527099609375, G Loss: 218.56069946289062\n",
      "Epoch: 839, D Loss: -203.11196899414062, G Loss: 538.27099609375\n",
      "Epoch: 840, D Loss: 59.652732849121094, G Loss: 389.49444580078125\n",
      "Epoch: 841, D Loss: -27.172138214111328, G Loss: 475.7689514160156\n",
      "Epoch: 842, D Loss: 5.628561019897461, G Loss: 125.46561431884766\n",
      "Epoch: 843, D Loss: -9.395366668701172, G Loss: 176.61184692382812\n",
      "Epoch: 844, D Loss: -9.132631301879883, G Loss: 437.9214782714844\n",
      "Epoch: 845, D Loss: -1.45672607421875, G Loss: 521.4798583984375\n",
      "Epoch: 846, D Loss: -12.432587623596191, G Loss: 357.4973449707031\n",
      "Epoch: 847, D Loss: 34.4102783203125, G Loss: 158.3947296142578\n",
      "Epoch: 848, D Loss: -1.1665520668029785, G Loss: 234.315673828125\n",
      "Epoch: 849, D Loss: -0.0002200603485107422, G Loss: 154.14761352539062\n",
      "Epoch: 850, D Loss: 88.69609069824219, G Loss: 183.84878540039062\n",
      "Epoch: 851, D Loss: 11.164855003356934, G Loss: 177.1439971923828\n",
      "Epoch: 852, D Loss: -38.531532287597656, G Loss: 339.1484069824219\n",
      "Epoch: 853, D Loss: 11.395702362060547, G Loss: 200.2707061767578\n",
      "Epoch: 854, D Loss: -14.562935829162598, G Loss: 394.28997802734375\n",
      "Epoch: 855, D Loss: 447.04595947265625, G Loss: 558.0423583984375\n",
      "Epoch: 856, D Loss: -51.15699005126953, G Loss: 462.37994384765625\n",
      "Epoch: 857, D Loss: 1.316019058227539, G Loss: 371.81744384765625\n",
      "Epoch: 858, D Loss: 53.81539535522461, G Loss: 64.75019836425781\n",
      "Epoch: 859, D Loss: 123.87376403808594, G Loss: 364.1418151855469\n",
      "Epoch: 860, D Loss: 165.06251525878906, G Loss: 145.4414520263672\n",
      "Epoch: 861, D Loss: 122.11412811279297, G Loss: 187.52890014648438\n",
      "Epoch: 862, D Loss: 2.527031898498535, G Loss: 606.278076171875\n",
      "Epoch: 863, D Loss: 14.648780822753906, G Loss: 809.740478515625\n",
      "Epoch: 864, D Loss: -8.725546836853027, G Loss: 331.38372802734375\n",
      "Epoch: 865, D Loss: 21.074174880981445, G Loss: 163.8766632080078\n",
      "Epoch: 866, D Loss: 138.69715881347656, G Loss: 423.57366943359375\n",
      "Epoch: 867, D Loss: 31.434932708740234, G Loss: 232.29620361328125\n",
      "Epoch: 868, D Loss: -7.101228713989258, G Loss: 614.4110717773438\n",
      "Epoch: 869, D Loss: 32.60468292236328, G Loss: 481.6018371582031\n",
      "Epoch: 870, D Loss: -7.059311389923096, G Loss: 251.13534545898438\n",
      "Epoch: 871, D Loss: -3.6085896492004395, G Loss: 693.2037963867188\n",
      "Epoch: 872, D Loss: 65.36378479003906, G Loss: 444.54254150390625\n",
      "Epoch: 873, D Loss: -55.007164001464844, G Loss: 281.5750732421875\n",
      "Epoch: 874, D Loss: -44.68832015991211, G Loss: 432.306640625\n",
      "Epoch: 875, D Loss: -12.194561004638672, G Loss: 109.09829711914062\n",
      "Epoch: 876, D Loss: -31.714048385620117, G Loss: 215.2845916748047\n",
      "Epoch: 877, D Loss: 150.17166137695312, G Loss: 55.898963928222656\n",
      "Epoch: 878, D Loss: 129.1698760986328, G Loss: 325.6557922363281\n",
      "Epoch: 879, D Loss: 2591.571044921875, G Loss: 376.2813720703125\n",
      "Epoch: 880, D Loss: 0.27700185775756836, G Loss: 777.9276123046875\n",
      "Epoch: 881, D Loss: -12.22078800201416, G Loss: 501.88995361328125\n",
      "Epoch: 882, D Loss: 65.76129150390625, G Loss: 426.98626708984375\n",
      "Epoch: 883, D Loss: 18.693363189697266, G Loss: 159.98989868164062\n",
      "Epoch: 884, D Loss: -31.339387893676758, G Loss: 110.21267700195312\n",
      "Epoch: 885, D Loss: 18.329891204833984, G Loss: 112.85408020019531\n",
      "Epoch: 886, D Loss: 93.70323181152344, G Loss: 105.19325256347656\n",
      "Epoch: 887, D Loss: 56.2986946105957, G Loss: 151.34158325195312\n",
      "Epoch: 888, D Loss: 3612.6455078125, G Loss: 394.88140869140625\n",
      "Epoch: 889, D Loss: -27.68583869934082, G Loss: 462.021240234375\n",
      "Epoch: 890, D Loss: -14.466436386108398, G Loss: 213.28164672851562\n",
      "Epoch: 891, D Loss: 1.634894847869873, G Loss: 531.3592529296875\n",
      "Epoch: 892, D Loss: 4.143575668334961, G Loss: 342.23626708984375\n",
      "Epoch: 893, D Loss: 24.12388801574707, G Loss: 406.6639709472656\n",
      "Epoch: 894, D Loss: -1.487771987915039, G Loss: 175.96981811523438\n",
      "Epoch: 895, D Loss: 12.640176773071289, G Loss: 161.5846710205078\n",
      "Epoch: 896, D Loss: -2.7374649047851562, G Loss: 311.66131591796875\n",
      "Epoch: 897, D Loss: 6.15679931640625, G Loss: 489.7030029296875\n",
      "Epoch: 898, D Loss: -0.14416885375976562, G Loss: 212.81907653808594\n",
      "Epoch: 899, D Loss: 16.711732864379883, G Loss: 422.7359924316406\n",
      "Epoch: 900, D Loss: -43.09443664550781, G Loss: 306.5520324707031\n",
      "Epoch: 901, D Loss: -26.917072296142578, G Loss: 983.994873046875\n",
      "Epoch: 902, D Loss: -8.512115478515625, G Loss: 420.8257141113281\n",
      "Epoch: 903, D Loss: -50.92325973510742, G Loss: 658.163818359375\n",
      "Epoch: 904, D Loss: 139.85055541992188, G Loss: 216.86697387695312\n",
      "Epoch: 905, D Loss: -25.84857940673828, G Loss: 176.71812438964844\n",
      "Epoch: 906, D Loss: -34.22005081176758, G Loss: 441.867919921875\n",
      "Epoch: 907, D Loss: -95.53036499023438, G Loss: 385.06793212890625\n",
      "Epoch: 908, D Loss: 16.626134872436523, G Loss: 583.040771484375\n",
      "Epoch: 909, D Loss: 54.43314743041992, G Loss: 245.71214294433594\n",
      "Epoch: 910, D Loss: 165.81878662109375, G Loss: 952.121337890625\n",
      "Epoch: 911, D Loss: 129.80886840820312, G Loss: 275.6156921386719\n",
      "Epoch: 912, D Loss: 78.67012023925781, G Loss: 205.97750854492188\n",
      "Epoch: 913, D Loss: 30.9146785736084, G Loss: 318.38629150390625\n",
      "Epoch: 914, D Loss: 16.51010513305664, G Loss: 407.6247253417969\n",
      "Epoch: 915, D Loss: 63.71936798095703, G Loss: 129.38037109375\n",
      "Epoch: 916, D Loss: 11.593250274658203, G Loss: 357.92071533203125\n",
      "Epoch: 917, D Loss: -16.64244842529297, G Loss: 790.4733276367188\n",
      "Epoch: 918, D Loss: -40.48797607421875, G Loss: 440.40252685546875\n",
      "Epoch: 919, D Loss: -50.355628967285156, G Loss: 530.9287109375\n",
      "Epoch: 920, D Loss: -21.48113250732422, G Loss: 331.1307373046875\n",
      "Epoch: 921, D Loss: 56.15819549560547, G Loss: 173.44586181640625\n",
      "Epoch: 922, D Loss: 32.58024597167969, G Loss: 256.54107666015625\n",
      "Epoch: 923, D Loss: 74.93412780761719, G Loss: 181.86941528320312\n",
      "Epoch: 924, D Loss: 37.898094177246094, G Loss: 450.7387390136719\n",
      "Epoch: 925, D Loss: -35.52012634277344, G Loss: 828.5189819335938\n",
      "Epoch: 926, D Loss: 17.03862190246582, G Loss: 646.3936767578125\n",
      "Epoch: 927, D Loss: -0.6791305541992188, G Loss: 287.2613525390625\n",
      "Epoch: 928, D Loss: 14.580574035644531, G Loss: 39.81254959106445\n",
      "Epoch: 929, D Loss: 10.914207458496094, G Loss: 169.75787353515625\n",
      "Epoch: 930, D Loss: -21.34087371826172, G Loss: 551.7645263671875\n",
      "Epoch: 931, D Loss: 25.14131736755371, G Loss: 524.3316650390625\n",
      "Epoch: 932, D Loss: 131.6975860595703, G Loss: 511.59698486328125\n",
      "Epoch: 933, D Loss: -6.934626579284668, G Loss: 887.8529052734375\n",
      "Epoch: 934, D Loss: -47.8129768371582, G Loss: 721.740966796875\n",
      "Epoch: 935, D Loss: -176.4322509765625, G Loss: 945.7655029296875\n",
      "Epoch: 936, D Loss: 968.9043579101562, G Loss: 1310.20166015625\n",
      "Epoch: 937, D Loss: 5.515159606933594, G Loss: 997.2137451171875\n",
      "Epoch: 938, D Loss: 1040.5836181640625, G Loss: 774.1281127929688\n",
      "Epoch: 939, D Loss: 139.74920654296875, G Loss: 1313.87646484375\n",
      "Epoch: 940, D Loss: 55.71527099609375, G Loss: 480.9832763671875\n",
      "Epoch: 941, D Loss: 111.55323791503906, G Loss: 197.46063232421875\n",
      "Epoch: 942, D Loss: 36.939727783203125, G Loss: 849.072265625\n",
      "Epoch: 943, D Loss: 181.06640625, G Loss: 315.98919677734375\n",
      "Epoch: 944, D Loss: 42.529335021972656, G Loss: 742.1392822265625\n",
      "Epoch: 945, D Loss: 84.56947326660156, G Loss: 124.50067138671875\n",
      "Epoch: 946, D Loss: -64.69264221191406, G Loss: 1227.9066162109375\n",
      "Epoch: 947, D Loss: -46.98992919921875, G Loss: 278.1641540527344\n",
      "Epoch: 948, D Loss: 40.02381134033203, G Loss: 737.1336059570312\n",
      "Epoch: 949, D Loss: 43.422996520996094, G Loss: 294.1138916015625\n",
      "Epoch: 950, D Loss: 58.169532775878906, G Loss: 589.7528076171875\n",
      "Epoch: 951, D Loss: 10.18869686126709, G Loss: 950.9461059570312\n",
      "Epoch: 952, D Loss: -26.155241012573242, G Loss: 852.7675170898438\n",
      "Epoch: 953, D Loss: 30.039703369140625, G Loss: 512.8699951171875\n",
      "Epoch: 954, D Loss: -4.497718334197998, G Loss: 968.771728515625\n",
      "Epoch: 955, D Loss: 2.1741232872009277, G Loss: 334.80230712890625\n",
      "Epoch: 956, D Loss: 32.32801818847656, G Loss: 604.0192260742188\n",
      "Epoch: 957, D Loss: 103.56604766845703, G Loss: 354.10748291015625\n",
      "Epoch: 958, D Loss: 29.52676010131836, G Loss: 433.2577819824219\n",
      "Epoch: 959, D Loss: 24.113941192626953, G Loss: 378.9492492675781\n",
      "Epoch: 960, D Loss: 10.6419095993042, G Loss: 205.90548706054688\n",
      "Epoch: 961, D Loss: 8.562030792236328, G Loss: 289.0917663574219\n",
      "Epoch: 962, D Loss: -18.743812561035156, G Loss: 345.15655517578125\n",
      "Epoch: 963, D Loss: 30.3101806640625, G Loss: 306.2899169921875\n",
      "Epoch: 964, D Loss: 20.6472225189209, G Loss: 665.5139770507812\n",
      "Epoch: 965, D Loss: 14.876595497131348, G Loss: 263.0996398925781\n",
      "Epoch: 966, D Loss: 19.48229217529297, G Loss: 478.55120849609375\n",
      "Epoch: 967, D Loss: -14.514618873596191, G Loss: 250.1827392578125\n",
      "Epoch: 968, D Loss: -19.57444190979004, G Loss: 252.28781127929688\n",
      "Epoch: 969, D Loss: -59.697479248046875, G Loss: 216.43397521972656\n",
      "Epoch: 970, D Loss: -41.568458557128906, G Loss: 235.233642578125\n",
      "Epoch: 971, D Loss: -248.10150146484375, G Loss: 441.280029296875\n",
      "Epoch: 972, D Loss: -93.05709838867188, G Loss: 685.979248046875\n",
      "Epoch: 973, D Loss: -88.63349151611328, G Loss: 563.2913208007812\n",
      "Epoch: 974, D Loss: 20.31329917907715, G Loss: 754.8541259765625\n",
      "Epoch: 975, D Loss: 6929.048828125, G Loss: 526.50537109375\n",
      "Epoch: 976, D Loss: -28.966197967529297, G Loss: 333.2735290527344\n",
      "Epoch: 977, D Loss: -48.163448333740234, G Loss: 211.46954345703125\n",
      "Epoch: 978, D Loss: -62.697044372558594, G Loss: 1015.6304931640625\n",
      "Epoch: 979, D Loss: -145.7062530517578, G Loss: 121.64063262939453\n",
      "Epoch: 980, D Loss: 55.62245178222656, G Loss: 1606.446044921875\n",
      "Epoch: 981, D Loss: 191.64930725097656, G Loss: 859.1962890625\n",
      "Epoch: 982, D Loss: 54.920318603515625, G Loss: 388.1893310546875\n",
      "Epoch: 983, D Loss: 35.00373458862305, G Loss: 586.8599853515625\n",
      "Epoch: 984, D Loss: -12.219099044799805, G Loss: 575.1793212890625\n",
      "Epoch: 985, D Loss: 45.91393280029297, G Loss: 336.78338623046875\n",
      "Epoch: 986, D Loss: -20.85487174987793, G Loss: 372.8564453125\n",
      "Epoch: 987, D Loss: -20.142478942871094, G Loss: 532.650146484375\n",
      "Epoch: 988, D Loss: -13.92560863494873, G Loss: 424.1534729003906\n",
      "Epoch: 989, D Loss: -16.462013244628906, G Loss: 510.32147216796875\n",
      "Epoch: 990, D Loss: 0.5123653411865234, G Loss: 158.666748046875\n",
      "Epoch: 991, D Loss: 22.868196487426758, G Loss: 399.0682678222656\n",
      "Epoch: 992, D Loss: 108.50769805908203, G Loss: 298.247314453125\n",
      "Epoch: 993, D Loss: 27.618593215942383, G Loss: 755.130859375\n",
      "Epoch: 994, D Loss: 29.52422332763672, G Loss: 923.1729736328125\n",
      "Epoch: 995, D Loss: 18.441364288330078, G Loss: 449.99566650390625\n",
      "Epoch: 996, D Loss: 10.92939567565918, G Loss: 896.9842529296875\n",
      "Epoch: 997, D Loss: 14.003883361816406, G Loss: 319.6141052246094\n",
      "Epoch: 998, D Loss: 63.70033264160156, G Loss: 387.2929992675781\n",
      "Epoch: 999, D Loss: 1039.5538330078125, G Loss: 251.26516723632812\n",
      "Epoch: 1000, D Loss: 29.707300186157227, G Loss: 138.4565887451172\n",
      "Epoch: 1001, D Loss: 31.238441467285156, G Loss: 422.80255126953125\n",
      "Epoch: 1002, D Loss: -15.465446472167969, G Loss: 534.295654296875\n",
      "Epoch: 1003, D Loss: -28.903718948364258, G Loss: 419.3707580566406\n",
      "Epoch: 1004, D Loss: -38.30274200439453, G Loss: 1048.5340576171875\n",
      "Epoch: 1005, D Loss: -33.43620300292969, G Loss: 572.9495849609375\n",
      "Epoch: 1006, D Loss: 0.28820037841796875, G Loss: 464.66705322265625\n",
      "Epoch: 1007, D Loss: -18.193052291870117, G Loss: 273.558837890625\n",
      "Epoch: 1008, D Loss: -37.563316345214844, G Loss: 469.52435302734375\n",
      "Epoch: 1009, D Loss: 1468.5181884765625, G Loss: 570.4783935546875\n",
      "Epoch: 1010, D Loss: -32.1069450378418, G Loss: 507.1397705078125\n",
      "Epoch: 1011, D Loss: -72.48368835449219, G Loss: 679.2493896484375\n",
      "Epoch: 1012, D Loss: -27.108659744262695, G Loss: 1157.984619140625\n",
      "Epoch: 1013, D Loss: -39.252628326416016, G Loss: 595.3192138671875\n",
      "Epoch: 1014, D Loss: -15.551485061645508, G Loss: 612.8624267578125\n",
      "Epoch: 1015, D Loss: 3.3813045024871826, G Loss: 482.62933349609375\n",
      "Epoch: 1016, D Loss: -92.12633514404297, G Loss: 538.7218017578125\n",
      "Epoch: 1017, D Loss: -46.63771438598633, G Loss: 413.927734375\n",
      "Epoch: 1018, D Loss: 4.675270080566406, G Loss: 407.44293212890625\n",
      "Epoch: 1019, D Loss: -37.34172058105469, G Loss: 219.24441528320312\n",
      "Epoch: 1020, D Loss: 2778.860595703125, G Loss: 434.285400390625\n",
      "Epoch: 1021, D Loss: 142.94236755371094, G Loss: 475.890380859375\n",
      "Epoch: 1022, D Loss: 44.128292083740234, G Loss: 613.8316650390625\n",
      "Epoch: 1023, D Loss: 68.71778869628906, G Loss: 750.84130859375\n",
      "Epoch: 1024, D Loss: 84.2654800415039, G Loss: 110.62591552734375\n",
      "Epoch: 1025, D Loss: 50.18381118774414, G Loss: 588.6220703125\n",
      "Epoch: 1026, D Loss: -5.962170600891113, G Loss: 587.112060546875\n",
      "Epoch: 1027, D Loss: -19.839096069335938, G Loss: 453.02069091796875\n",
      "Epoch: 1028, D Loss: -34.83599853515625, G Loss: 359.66619873046875\n",
      "Epoch: 1029, D Loss: -21.560523986816406, G Loss: 379.8453063964844\n",
      "Epoch: 1030, D Loss: -8.91659927368164, G Loss: 159.33209228515625\n",
      "Epoch: 1031, D Loss: -77.01487731933594, G Loss: 515.347900390625\n",
      "Epoch: 1032, D Loss: -45.31001281738281, G Loss: 287.70233154296875\n",
      "Epoch: 1033, D Loss: -23.098474502563477, G Loss: 664.6187744140625\n",
      "Epoch: 1034, D Loss: 6059.087890625, G Loss: 609.619873046875\n",
      "Epoch: 1035, D Loss: -4.1980671882629395, G Loss: 930.9224853515625\n",
      "Epoch: 1036, D Loss: -36.205101013183594, G Loss: 610.7172241210938\n",
      "Epoch: 1037, D Loss: 90.22486877441406, G Loss: 915.1619873046875\n",
      "Epoch: 1038, D Loss: 2912.260009765625, G Loss: 874.456298828125\n",
      "Epoch: 1039, D Loss: 9152.8779296875, G Loss: 1408.29833984375\n",
      "Epoch: 1040, D Loss: 7167.7265625, G Loss: 1371.912109375\n",
      "Epoch: 1041, D Loss: 10850.5849609375, G Loss: 877.940185546875\n",
      "Epoch: 1042, D Loss: 652.8675537109375, G Loss: 1059.062744140625\n",
      "Epoch: 1043, D Loss: -12.137090682983398, G Loss: 666.6104125976562\n",
      "Epoch: 1044, D Loss: 416.79803466796875, G Loss: 681.0062866210938\n",
      "Epoch: 1045, D Loss: 402.17822265625, G Loss: 689.8513793945312\n",
      "Epoch: 1046, D Loss: 446.4202880859375, G Loss: 2079.518798828125\n",
      "Epoch: 1047, D Loss: 397.32598876953125, G Loss: 2485.4892578125\n",
      "Epoch: 1048, D Loss: 133.31765747070312, G Loss: 1185.105224609375\n",
      "Epoch: 1049, D Loss: 363.7775573730469, G Loss: 1242.886474609375\n",
      "Epoch: 1050, D Loss: 206.8704376220703, G Loss: 839.5224609375\n",
      "Epoch: 1051, D Loss: 188.3751220703125, G Loss: 1192.29296875\n",
      "Epoch: 1052, D Loss: 275.9688415527344, G Loss: 527.439697265625\n",
      "Epoch: 1053, D Loss: 12.73674488067627, G Loss: 1086.791748046875\n",
      "Epoch: 1054, D Loss: -9.571207046508789, G Loss: 888.735107421875\n",
      "Epoch: 1055, D Loss: -31.225662231445312, G Loss: 520.2457275390625\n",
      "Epoch: 1056, D Loss: 41.15304946899414, G Loss: 322.6360778808594\n",
      "Epoch: 1057, D Loss: 10.544740676879883, G Loss: 491.6982421875\n",
      "Epoch: 1058, D Loss: 19.91546630859375, G Loss: 402.33343505859375\n",
      "Epoch: 1059, D Loss: -11.595754623413086, G Loss: 740.904052734375\n",
      "Epoch: 1060, D Loss: 12.64191722869873, G Loss: 865.7818603515625\n",
      "Epoch: 1061, D Loss: -9.480622291564941, G Loss: 871.1394653320312\n",
      "Epoch: 1062, D Loss: 8.562702178955078, G Loss: 370.5339660644531\n",
      "Epoch: 1063, D Loss: 2.5919947624206543, G Loss: 372.2143249511719\n",
      "Epoch: 1064, D Loss: -24.541664123535156, G Loss: 407.6698913574219\n",
      "Epoch: 1065, D Loss: 12.921266555786133, G Loss: 1618.158203125\n",
      "Epoch: 1066, D Loss: -19.255088806152344, G Loss: 342.576416015625\n",
      "Epoch: 1067, D Loss: 5.9220428466796875, G Loss: 917.3873291015625\n",
      "Epoch: 1068, D Loss: -16.036821365356445, G Loss: 416.1971435546875\n",
      "Epoch: 1069, D Loss: -27.393817901611328, G Loss: 301.553955078125\n",
      "Epoch: 1070, D Loss: 11.880495071411133, G Loss: 269.7825927734375\n",
      "Epoch: 1071, D Loss: 8.729475021362305, G Loss: 449.5049133300781\n",
      "Epoch: 1072, D Loss: 4.48395299911499, G Loss: 480.0393981933594\n",
      "Epoch: 1073, D Loss: -7.0231547355651855, G Loss: 226.4415283203125\n",
      "Epoch: 1074, D Loss: -9.43294906616211, G Loss: 586.5362548828125\n",
      "Epoch: 1075, D Loss: -7.842189788818359, G Loss: 353.59075927734375\n",
      "Epoch: 1076, D Loss: -33.06333923339844, G Loss: 337.895751953125\n",
      "Epoch: 1077, D Loss: 14.054862976074219, G Loss: 1544.44921875\n",
      "Epoch: 1078, D Loss: 17.313331604003906, G Loss: 876.7090454101562\n",
      "Epoch: 1079, D Loss: 45.8247184753418, G Loss: 546.4758911132812\n",
      "Epoch: 1080, D Loss: 18.06229019165039, G Loss: 462.12005615234375\n",
      "Epoch: 1081, D Loss: 53.23033905029297, G Loss: 919.89404296875\n",
      "Epoch: 1082, D Loss: 19.15447998046875, G Loss: 304.79412841796875\n",
      "Epoch: 1083, D Loss: 63.496826171875, G Loss: 668.0775756835938\n",
      "Epoch: 1084, D Loss: 2.390665054321289, G Loss: 322.8009033203125\n",
      "Epoch: 1085, D Loss: -2.5089988708496094, G Loss: 294.8472595214844\n",
      "Epoch: 1086, D Loss: 44.200218200683594, G Loss: 449.30413818359375\n",
      "Epoch: 1087, D Loss: 34.05515670776367, G Loss: 487.4227294921875\n",
      "Epoch: 1088, D Loss: 7.770981788635254, G Loss: 649.4285888671875\n",
      "Epoch: 1089, D Loss: -24.128145217895508, G Loss: 461.12469482421875\n",
      "Epoch: 1090, D Loss: -20.377708435058594, G Loss: 300.6944274902344\n",
      "Epoch: 1091, D Loss: -13.473767280578613, G Loss: 210.6395263671875\n",
      "Epoch: 1092, D Loss: 3.967106342315674, G Loss: 163.55755615234375\n",
      "Epoch: 1093, D Loss: 103.24150848388672, G Loss: 1245.5576171875\n",
      "Epoch: 1094, D Loss: 31.592700958251953, G Loss: 499.5135498046875\n",
      "Epoch: 1095, D Loss: 27.781402587890625, G Loss: 388.0830383300781\n",
      "Epoch: 1096, D Loss: 47.08324432373047, G Loss: 88.68758392333984\n",
      "Epoch: 1097, D Loss: 45.30921936035156, G Loss: 96.9997787475586\n",
      "Epoch: 1098, D Loss: 36.67192077636719, G Loss: 566.6848754882812\n",
      "Epoch: 1099, D Loss: -28.981618881225586, G Loss: 541.4910278320312\n",
      "Epoch: 1100, D Loss: -24.089962005615234, G Loss: 718.1785888671875\n",
      "Epoch: 1101, D Loss: -20.822540283203125, G Loss: 1065.7568359375\n",
      "Epoch: 1102, D Loss: -11.747880935668945, G Loss: 739.1074829101562\n",
      "Epoch: 1103, D Loss: -8.894558906555176, G Loss: 410.6544189453125\n",
      "Epoch: 1104, D Loss: 7.079862594604492, G Loss: 311.67022705078125\n",
      "Epoch: 1105, D Loss: 23.48898696899414, G Loss: 769.38427734375\n",
      "Epoch: 1106, D Loss: 5.339982032775879, G Loss: 129.37191772460938\n",
      "Epoch: 1107, D Loss: -22.885921478271484, G Loss: 90.69512176513672\n",
      "Epoch: 1108, D Loss: -18.064241409301758, G Loss: 124.57131958007812\n",
      "Epoch: 1109, D Loss: -18.514263153076172, G Loss: 308.089111328125\n",
      "Epoch: 1110, D Loss: -7.193871974945068, G Loss: 172.3198699951172\n",
      "Epoch: 1111, D Loss: -53.0438232421875, G Loss: 398.40704345703125\n",
      "Epoch: 1112, D Loss: -75.56163024902344, G Loss: 209.26437377929688\n",
      "Epoch: 1113, D Loss: -45.483741760253906, G Loss: 836.888916015625\n",
      "Epoch: 1114, D Loss: -4.561674118041992, G Loss: 1077.337646484375\n",
      "Epoch: 1115, D Loss: -16.66702651977539, G Loss: 678.35009765625\n",
      "Epoch: 1116, D Loss: -5.907598972320557, G Loss: 567.0506591796875\n",
      "Epoch: 1117, D Loss: -4.533660888671875, G Loss: 387.1673583984375\n",
      "Epoch: 1118, D Loss: -21.792320251464844, G Loss: 822.8516235351562\n",
      "Epoch: 1119, D Loss: -17.046512603759766, G Loss: 843.5966186523438\n",
      "Epoch: 1120, D Loss: -23.03687286376953, G Loss: 670.5328979492188\n",
      "Epoch: 1121, D Loss: -45.34077835083008, G Loss: 752.337646484375\n",
      "Epoch: 1122, D Loss: -28.69931983947754, G Loss: 1095.436279296875\n",
      "Epoch: 1123, D Loss: -84.78739929199219, G Loss: 500.13409423828125\n",
      "Epoch: 1124, D Loss: -43.36175537109375, G Loss: 738.0096435546875\n",
      "Epoch: 1125, D Loss: -46.34949493408203, G Loss: 783.076416015625\n",
      "Epoch: 1126, D Loss: 43.320735931396484, G Loss: 84.29198455810547\n",
      "Epoch: 1127, D Loss: 26.705108642578125, G Loss: 217.91293334960938\n",
      "Epoch: 1128, D Loss: -14.632959365844727, G Loss: 520.0598754882812\n",
      "Epoch: 1129, D Loss: -18.940404891967773, G Loss: 215.9017333984375\n",
      "Epoch: 1130, D Loss: -6.614415645599365, G Loss: 726.03466796875\n",
      "Epoch: 1131, D Loss: -43.79338836669922, G Loss: 556.40673828125\n",
      "Epoch: 1132, D Loss: -50.26908874511719, G Loss: 532.4758911132812\n",
      "Epoch: 1133, D Loss: -62.38597869873047, G Loss: 1125.984375\n",
      "Epoch: 1134, D Loss: -52.281890869140625, G Loss: 1055.99365234375\n",
      "Epoch: 1135, D Loss: -31.804758071899414, G Loss: 470.0186767578125\n",
      "Epoch: 1136, D Loss: 122.17041015625, G Loss: 1001.7422485351562\n",
      "Epoch: 1137, D Loss: 21.480609893798828, G Loss: 934.7150268554688\n",
      "Epoch: 1138, D Loss: 18.24029541015625, G Loss: 1313.514892578125\n",
      "Epoch: 1139, D Loss: -14.112215042114258, G Loss: 463.294677734375\n",
      "Epoch: 1140, D Loss: -1.5138757228851318, G Loss: 746.861328125\n",
      "Epoch: 1141, D Loss: 129.85243225097656, G Loss: 472.6827392578125\n",
      "Epoch: 1142, D Loss: 111.35601043701172, G Loss: 438.78778076171875\n",
      "Epoch: 1143, D Loss: -16.42074966430664, G Loss: 854.4544677734375\n",
      "Epoch: 1144, D Loss: 18.808515548706055, G Loss: 241.1612091064453\n",
      "Epoch: 1145, D Loss: -10.60550308227539, G Loss: 1083.5220947265625\n",
      "Epoch: 1146, D Loss: 91.97953796386719, G Loss: 893.9471435546875\n",
      "Epoch: 1147, D Loss: 29.982906341552734, G Loss: 348.1607666015625\n",
      "Epoch: 1148, D Loss: 0.7345596551895142, G Loss: 737.1785888671875\n",
      "Epoch: 1149, D Loss: -10.377847671508789, G Loss: 1653.1689453125\n",
      "Epoch: 1150, D Loss: 2.0036230087280273, G Loss: 334.1044921875\n",
      "Epoch: 1151, D Loss: 69.62129211425781, G Loss: 525.2515869140625\n",
      "Epoch: 1152, D Loss: 75.67704772949219, G Loss: 163.59922790527344\n",
      "Epoch: 1153, D Loss: 0.8101930618286133, G Loss: 661.654052734375\n",
      "Epoch: 1154, D Loss: -12.183879852294922, G Loss: 318.6663818359375\n",
      "Epoch: 1155, D Loss: -30.675453186035156, G Loss: 690.4531860351562\n",
      "Epoch: 1156, D Loss: -3.506490707397461, G Loss: 482.3215637207031\n",
      "Epoch: 1157, D Loss: -28.361215591430664, G Loss: 543.1875610351562\n",
      "Epoch: 1158, D Loss: -31.096538543701172, G Loss: 493.373779296875\n",
      "Epoch: 1159, D Loss: -22.40851402282715, G Loss: 731.8059692382812\n",
      "Epoch: 1160, D Loss: -19.616146087646484, G Loss: 774.5980224609375\n",
      "Epoch: 1161, D Loss: -9.524118423461914, G Loss: 425.67047119140625\n",
      "Epoch: 1162, D Loss: -4.083021640777588, G Loss: 413.1231689453125\n",
      "Epoch: 1163, D Loss: -29.58696937561035, G Loss: 456.93548583984375\n",
      "Epoch: 1164, D Loss: -8.681328773498535, G Loss: 595.0794677734375\n",
      "Epoch: 1165, D Loss: -30.58124542236328, G Loss: 896.93603515625\n",
      "Epoch: 1166, D Loss: -19.472904205322266, G Loss: 357.2221374511719\n",
      "Epoch: 1167, D Loss: -43.354530334472656, G Loss: 638.5789794921875\n",
      "Epoch: 1168, D Loss: -39.34486389160156, G Loss: 235.44119262695312\n",
      "Epoch: 1169, D Loss: -38.576744079589844, G Loss: 281.1297607421875\n",
      "Epoch: 1170, D Loss: -56.43042755126953, G Loss: 197.38136291503906\n",
      "Epoch: 1171, D Loss: -28.83153533935547, G Loss: 230.42840576171875\n",
      "Epoch: 1172, D Loss: -31.90276527404785, G Loss: 283.0873718261719\n",
      "Epoch: 1173, D Loss: -21.206695556640625, G Loss: 327.2913818359375\n",
      "Epoch: 1174, D Loss: 29.46269416809082, G Loss: 146.6170654296875\n",
      "Epoch: 1175, D Loss: -52.91263961791992, G Loss: 464.1028137207031\n",
      "Epoch: 1176, D Loss: -92.48359680175781, G Loss: 269.7003479003906\n",
      "Epoch: 1177, D Loss: -71.48883056640625, G Loss: 255.19317626953125\n",
      "Epoch: 1178, D Loss: -1743.4547119140625, G Loss: 611.80029296875\n",
      "Epoch: 1179, D Loss: -8924.8916015625, G Loss: 128.73460388183594\n",
      "Epoch: 1180, D Loss: -2920.04931640625, G Loss: 839.177001953125\n",
      "Epoch: 1181, D Loss: 127.69732666015625, G Loss: 443.62347412109375\n",
      "Epoch: 1182, D Loss: 99.40841674804688, G Loss: 360.798583984375\n",
      "Epoch: 1183, D Loss: 2.079111099243164, G Loss: 756.612060546875\n",
      "Epoch: 1184, D Loss: 43.61545181274414, G Loss: 543.8326416015625\n",
      "Epoch: 1185, D Loss: 29.357776641845703, G Loss: 454.35638427734375\n",
      "Epoch: 1186, D Loss: 13.008214950561523, G Loss: 558.0260009765625\n",
      "Epoch: 1187, D Loss: 19.749847412109375, G Loss: 165.6792755126953\n",
      "Epoch: 1188, D Loss: 29.911794662475586, G Loss: 484.03668212890625\n",
      "Epoch: 1189, D Loss: 131.05294799804688, G Loss: 351.48779296875\n",
      "Epoch: 1190, D Loss: 15.051751136779785, G Loss: 239.80914306640625\n",
      "Epoch: 1191, D Loss: 15.60527229309082, G Loss: 156.53347778320312\n",
      "Epoch: 1192, D Loss: 207.3341064453125, G Loss: 199.32937622070312\n",
      "Epoch: 1193, D Loss: 121.93092346191406, G Loss: 229.6905059814453\n",
      "Epoch: 1194, D Loss: 41.163211822509766, G Loss: 791.5745239257812\n",
      "Epoch: 1195, D Loss: 7.703982353210449, G Loss: 895.2041625976562\n",
      "Epoch: 1196, D Loss: 13.082817077636719, G Loss: 864.273193359375\n",
      "Epoch: 1197, D Loss: 68.56492614746094, G Loss: 714.02880859375\n",
      "Epoch: 1198, D Loss: 41.8707389831543, G Loss: 373.2376403808594\n",
      "Epoch: 1199, D Loss: 34.619163513183594, G Loss: 329.55084228515625\n",
      "Epoch: 1200, D Loss: 28.567733764648438, G Loss: 143.98126220703125\n",
      "Epoch: 1201, D Loss: 22.44693374633789, G Loss: 179.99038696289062\n",
      "Epoch: 1202, D Loss: 76.591064453125, G Loss: 137.79751586914062\n",
      "Epoch: 1203, D Loss: 56.60821533203125, G Loss: 501.5456237792969\n",
      "Epoch: 1204, D Loss: -1.5517802238464355, G Loss: 571.3919067382812\n",
      "Epoch: 1205, D Loss: -14.659400939941406, G Loss: 837.8804321289062\n",
      "Epoch: 1206, D Loss: -1.7575852870941162, G Loss: 630.630859375\n",
      "Epoch: 1207, D Loss: -2.256866455078125, G Loss: 797.986328125\n",
      "Epoch: 1208, D Loss: -4.528911590576172, G Loss: 698.1311645507812\n",
      "Epoch: 1209, D Loss: -9.27833366394043, G Loss: 828.3837890625\n",
      "Epoch: 1210, D Loss: 1.2650718688964844, G Loss: 295.7129821777344\n",
      "Epoch: 1211, D Loss: -25.46640396118164, G Loss: 634.7791748046875\n",
      "Epoch: 1212, D Loss: -14.35677719116211, G Loss: 792.4501953125\n",
      "Epoch: 1213, D Loss: 67.84407806396484, G Loss: 322.8779602050781\n",
      "Epoch: 1214, D Loss: 21.392900466918945, G Loss: 404.7173767089844\n",
      "Epoch: 1215, D Loss: 16.57715606689453, G Loss: 403.45831298828125\n",
      "Epoch: 1216, D Loss: -18.772865295410156, G Loss: 421.84100341796875\n",
      "Epoch: 1217, D Loss: -32.31554412841797, G Loss: 277.688720703125\n",
      "Epoch: 1218, D Loss: -37.828338623046875, G Loss: 334.83154296875\n",
      "Epoch: 1219, D Loss: -53.24522018432617, G Loss: 250.6942138671875\n",
      "Epoch: 1220, D Loss: -67.01612854003906, G Loss: 457.097900390625\n",
      "Epoch: 1221, D Loss: -65.35131072998047, G Loss: 1015.3688354492188\n",
      "Epoch: 1222, D Loss: 54.37600326538086, G Loss: 626.7835693359375\n",
      "Epoch: 1223, D Loss: -37.945579528808594, G Loss: 259.58685302734375\n",
      "Epoch: 1224, D Loss: 58.45552062988281, G Loss: 283.38360595703125\n",
      "Epoch: 1225, D Loss: -8.834396362304688, G Loss: 541.954833984375\n",
      "Epoch: 1226, D Loss: 12.986332893371582, G Loss: 433.2829284667969\n",
      "Epoch: 1227, D Loss: 46.97163009643555, G Loss: 220.32278442382812\n",
      "Epoch: 1228, D Loss: 54.84759521484375, G Loss: 622.8978881835938\n",
      "Epoch: 1229, D Loss: -10.033266067504883, G Loss: 537.5213623046875\n",
      "Epoch: 1230, D Loss: 0.3994588851928711, G Loss: 795.3856201171875\n",
      "Epoch: 1231, D Loss: 129.78399658203125, G Loss: 606.8043212890625\n",
      "Epoch: 1232, D Loss: 60.41228103637695, G Loss: 388.9969482421875\n",
      "Epoch: 1233, D Loss: 36.39946746826172, G Loss: 628.1447143554688\n",
      "Epoch: 1234, D Loss: -35.580322265625, G Loss: 352.3230285644531\n",
      "Epoch: 1235, D Loss: -48.439735412597656, G Loss: 484.14105224609375\n",
      "Epoch: 1236, D Loss: -11.234596252441406, G Loss: 229.3724365234375\n",
      "Epoch: 1237, D Loss: -6.662410736083984, G Loss: 166.22360229492188\n",
      "Epoch: 1238, D Loss: -55.94001007080078, G Loss: 122.3631591796875\n",
      "Epoch: 1239, D Loss: -21.683868408203125, G Loss: 499.60821533203125\n",
      "Epoch: 1240, D Loss: -26.538421630859375, G Loss: 725.7466430664062\n",
      "Epoch: 1241, D Loss: -20.78769302368164, G Loss: 272.72894287109375\n",
      "Epoch: 1242, D Loss: -72.12447357177734, G Loss: 381.889404296875\n",
      "Epoch: 1243, D Loss: -12.746541976928711, G Loss: 235.21873474121094\n",
      "Epoch: 1244, D Loss: 69.42853546142578, G Loss: 355.6820068359375\n",
      "Epoch: 1245, D Loss: 32.08811569213867, G Loss: 318.7320556640625\n",
      "Epoch: 1246, D Loss: 1.8543291091918945, G Loss: 641.965087890625\n",
      "Epoch: 1247, D Loss: 16.862232208251953, G Loss: 521.9461059570312\n",
      "Epoch: 1248, D Loss: -13.75949478149414, G Loss: 453.39215087890625\n",
      "Epoch: 1249, D Loss: -257.1307373046875, G Loss: 383.16436767578125\n",
      "Epoch: 1250, D Loss: -221.453857421875, G Loss: 186.57810974121094\n",
      "Epoch: 1251, D Loss: -3005.398193359375, G Loss: 145.7407989501953\n",
      "Epoch: 1252, D Loss: -7087.19189453125, G Loss: 389.18292236328125\n",
      "Epoch: 1253, D Loss: -11749.751953125, G Loss: 690.0723876953125\n",
      "Epoch: 1254, D Loss: -9016.0732421875, G Loss: 2140.9208984375\n",
      "Epoch: 1255, D Loss: -2506.094970703125, G Loss: 1504.6070556640625\n",
      "Epoch: 1256, D Loss: -1.4816856384277344, G Loss: 1559.7431640625\n",
      "Epoch: 1257, D Loss: 247.61842346191406, G Loss: 1034.7213134765625\n",
      "Epoch: 1258, D Loss: 518.2117919921875, G Loss: 787.8629150390625\n",
      "Epoch: 1259, D Loss: 507.4407653808594, G Loss: 1733.7052001953125\n",
      "Epoch: 1260, D Loss: -34.23914337158203, G Loss: 2472.671875\n",
      "Epoch: 1261, D Loss: 380.534912109375, G Loss: 436.0633544921875\n",
      "Epoch: 1262, D Loss: 92.42268371582031, G Loss: 585.8033447265625\n",
      "Epoch: 1263, D Loss: 169.00241088867188, G Loss: 1168.241943359375\n",
      "Epoch: 1264, D Loss: 324.51806640625, G Loss: 389.72454833984375\n",
      "Epoch: 1265, D Loss: 69.42076110839844, G Loss: 1011.043701171875\n",
      "Epoch: 1266, D Loss: 124.35017395019531, G Loss: 223.4514923095703\n",
      "Epoch: 1267, D Loss: 4.460844993591309, G Loss: 229.08154296875\n",
      "Epoch: 1268, D Loss: -29.641096115112305, G Loss: 113.414794921875\n",
      "Epoch: 1269, D Loss: 5.080089569091797, G Loss: 379.2956848144531\n",
      "Epoch: 1270, D Loss: 90.08642578125, G Loss: 155.08392333984375\n",
      "Epoch: 1271, D Loss: 133.06114196777344, G Loss: 226.39584350585938\n",
      "Epoch: 1272, D Loss: 23.80005645751953, G Loss: 137.73751831054688\n",
      "Epoch: 1273, D Loss: 34.26966857910156, G Loss: 371.5953369140625\n",
      "Epoch: 1274, D Loss: 21.949254989624023, G Loss: 560.9761352539062\n",
      "Epoch: 1275, D Loss: -22.41519546508789, G Loss: 703.7357177734375\n",
      "Epoch: 1276, D Loss: -100.38314056396484, G Loss: 393.6117248535156\n",
      "Epoch: 1277, D Loss: -130.35401916503906, G Loss: 774.4931640625\n",
      "Epoch: 1278, D Loss: -314.1377868652344, G Loss: 529.80419921875\n",
      "Epoch: 1279, D Loss: -885.4215698242188, G Loss: 379.85784912109375\n",
      "Epoch: 1280, D Loss: -2409.838134765625, G Loss: 355.6502990722656\n",
      "Epoch: 1281, D Loss: -4666.6875, G Loss: 373.28948974609375\n",
      "Epoch: 1282, D Loss: -9256.2958984375, G Loss: 704.2491455078125\n",
      "Epoch: 1283, D Loss: -2731.4892578125, G Loss: 404.26715087890625\n",
      "Epoch: 1284, D Loss: -2663.009521484375, G Loss: 804.825439453125\n",
      "Epoch: 1285, D Loss: 792.66357421875, G Loss: 346.125\n",
      "Epoch: 1286, D Loss: 65.08209991455078, G Loss: 951.7822265625\n",
      "Epoch: 1287, D Loss: 103.41043853759766, G Loss: 547.541259765625\n",
      "Epoch: 1288, D Loss: -21.55054473876953, G Loss: 455.76898193359375\n",
      "Epoch: 1289, D Loss: -17.20743179321289, G Loss: 369.8390197753906\n",
      "Epoch: 1290, D Loss: -1.5177749395370483, G Loss: 453.1299743652344\n",
      "Epoch: 1291, D Loss: 52.82258605957031, G Loss: 268.7210693359375\n",
      "Epoch: 1292, D Loss: 35.38172149658203, G Loss: 253.80679321289062\n",
      "Epoch: 1293, D Loss: 52.40570831298828, G Loss: 681.079833984375\n",
      "Epoch: 1294, D Loss: 2.474308729171753, G Loss: 888.2979125976562\n",
      "Epoch: 1295, D Loss: -17.10904312133789, G Loss: 506.1146545410156\n",
      "Epoch: 1296, D Loss: 19.872394561767578, G Loss: 334.95001220703125\n",
      "Epoch: 1297, D Loss: -5.801552772521973, G Loss: 291.68328857421875\n",
      "Epoch: 1298, D Loss: 4.963115692138672, G Loss: 414.1356506347656\n",
      "Epoch: 1299, D Loss: 28.48189926147461, G Loss: 312.24420166015625\n",
      "Epoch: 1300, D Loss: 21.343563079833984, G Loss: 531.384033203125\n",
      "Epoch: 1301, D Loss: 27.172908782958984, G Loss: 285.8779296875\n",
      "Epoch: 1302, D Loss: -10.086275100708008, G Loss: 482.3662109375\n",
      "Epoch: 1303, D Loss: 5.0421648025512695, G Loss: 680.5665893554688\n",
      "Epoch: 1304, D Loss: 42.48099899291992, G Loss: 370.8774719238281\n",
      "Epoch: 1305, D Loss: -13.094042778015137, G Loss: 715.5330200195312\n",
      "Epoch: 1306, D Loss: 69.52359771728516, G Loss: 58.979774475097656\n",
      "Epoch: 1307, D Loss: 19.7360897064209, G Loss: 305.3294677734375\n",
      "Epoch: 1308, D Loss: 79.76529693603516, G Loss: 279.897705078125\n",
      "Epoch: 1309, D Loss: 96.42816162109375, G Loss: 45.10304260253906\n",
      "Epoch: 1310, D Loss: 201.4436492919922, G Loss: 581.3639526367188\n",
      "Epoch: 1311, D Loss: 161.93089294433594, G Loss: 574.3886108398438\n",
      "Epoch: 1312, D Loss: 105.234375, G Loss: 677.9923706054688\n",
      "Epoch: 1313, D Loss: 51.75354766845703, G Loss: 902.4921264648438\n",
      "Epoch: 1314, D Loss: -11.307586669921875, G Loss: 416.2218322753906\n",
      "Epoch: 1315, D Loss: -2.095789909362793, G Loss: 805.3889770507812\n",
      "Epoch: 1316, D Loss: 111.06204223632812, G Loss: 371.4705505371094\n",
      "Epoch: 1317, D Loss: -15.712671279907227, G Loss: 301.59783935546875\n",
      "Epoch: 1318, D Loss: -57.27664566040039, G Loss: 782.7304077148438\n",
      "Epoch: 1319, D Loss: -29.446056365966797, G Loss: 432.818115234375\n",
      "Epoch: 1320, D Loss: -55.38629150390625, G Loss: 728.0582275390625\n",
      "Epoch: 1321, D Loss: -12.617399215698242, G Loss: 949.031005859375\n",
      "Epoch: 1322, D Loss: 140.53857421875, G Loss: 188.528076171875\n",
      "Epoch: 1323, D Loss: 20.3052921295166, G Loss: 825.9588623046875\n",
      "Epoch: 1324, D Loss: -10.066720962524414, G Loss: 733.7474365234375\n",
      "Epoch: 1325, D Loss: 213.49105834960938, G Loss: 479.72418212890625\n",
      "Epoch: 1326, D Loss: 27.47701644897461, G Loss: 376.76495361328125\n",
      "Epoch: 1327, D Loss: -48.92042541503906, G Loss: 1103.907958984375\n",
      "Epoch: 1328, D Loss: -47.903934478759766, G Loss: 861.738037109375\n",
      "Epoch: 1329, D Loss: -49.05241394042969, G Loss: 731.078369140625\n",
      "Epoch: 1330, D Loss: 47.385169982910156, G Loss: 773.7833862304688\n",
      "Epoch: 1331, D Loss: 125.25282287597656, G Loss: 507.45281982421875\n",
      "Epoch: 1332, D Loss: 8.956642150878906, G Loss: 637.2972412109375\n",
      "Epoch: 1333, D Loss: 13.468494415283203, G Loss: 472.740234375\n",
      "Epoch: 1334, D Loss: -7.981868743896484, G Loss: 778.7684936523438\n",
      "Epoch: 1335, D Loss: 14.441961288452148, G Loss: 409.0343017578125\n",
      "Epoch: 1336, D Loss: -5.33568000793457, G Loss: 167.42288208007812\n",
      "Epoch: 1337, D Loss: 6.8714518547058105, G Loss: 713.1363525390625\n",
      "Epoch: 1338, D Loss: -15.709341049194336, G Loss: 636.46533203125\n",
      "Epoch: 1339, D Loss: 56.212440490722656, G Loss: 493.14886474609375\n",
      "Epoch: 1340, D Loss: 26.818984985351562, G Loss: 696.2125244140625\n",
      "Epoch: 1341, D Loss: 45.64961624145508, G Loss: 584.571044921875\n",
      "Epoch: 1342, D Loss: 78.83477783203125, G Loss: 755.8833618164062\n",
      "Epoch: 1343, D Loss: -45.83038330078125, G Loss: 1127.273193359375\n",
      "Epoch: 1344, D Loss: -42.956443786621094, G Loss: 591.986572265625\n",
      "Epoch: 1345, D Loss: -73.69300842285156, G Loss: 264.0526428222656\n",
      "Epoch: 1346, D Loss: -23.639984130859375, G Loss: 418.61279296875\n",
      "Epoch: 1347, D Loss: -33.637847900390625, G Loss: 782.5951538085938\n",
      "Epoch: 1348, D Loss: -41.24071502685547, G Loss: 395.22491455078125\n",
      "Epoch: 1349, D Loss: -72.263427734375, G Loss: 474.1703796386719\n",
      "Epoch: 1350, D Loss: -54.573028564453125, G Loss: 418.22265625\n",
      "Epoch: 1351, D Loss: -69.8103256225586, G Loss: 465.705322265625\n",
      "Epoch: 1352, D Loss: -74.55743408203125, G Loss: 640.3585205078125\n",
      "Epoch: 1353, D Loss: -64.75559997558594, G Loss: 463.804931640625\n",
      "Epoch: 1354, D Loss: -86.74186706542969, G Loss: 429.25372314453125\n",
      "Epoch: 1355, D Loss: -98.86602783203125, G Loss: 604.827880859375\n",
      "Epoch: 1356, D Loss: -117.43479919433594, G Loss: 525.7569580078125\n",
      "Epoch: 1357, D Loss: -90.01170349121094, G Loss: 348.7452392578125\n",
      "Epoch: 1358, D Loss: -1332.927001953125, G Loss: 198.17510986328125\n",
      "Epoch: 1359, D Loss: -3360.39208984375, G Loss: 1115.39013671875\n",
      "Epoch: 1360, D Loss: -10658.705078125, G Loss: 299.8282775878906\n",
      "Epoch: 1361, D Loss: -11739.744140625, G Loss: 128.83499145507812\n",
      "Epoch: 1362, D Loss: -16759.955078125, G Loss: 1036.5908203125\n",
      "Epoch: 1363, D Loss: 2800.522705078125, G Loss: 1075.1671142578125\n",
      "Epoch: 1364, D Loss: 500.8067626953125, G Loss: 1763.67333984375\n",
      "Epoch: 1365, D Loss: 100.45230102539062, G Loss: 938.0173950195312\n",
      "Epoch: 1366, D Loss: 191.90953063964844, G Loss: 1436.1943359375\n",
      "Epoch: 1367, D Loss: 219.8282470703125, G Loss: 1029.2476806640625\n",
      "Epoch: 1368, D Loss: 986.2730712890625, G Loss: 1879.8154296875\n",
      "Epoch: 1369, D Loss: 75.36772155761719, G Loss: 2037.4385986328125\n",
      "Epoch: 1370, D Loss: 412.89349365234375, G Loss: 4256.1376953125\n",
      "Epoch: 1371, D Loss: 645.6429443359375, G Loss: 1332.750244140625\n",
      "Epoch: 1372, D Loss: 531.2646484375, G Loss: 90.21897888183594\n",
      "Epoch: 1373, D Loss: 80.92430877685547, G Loss: 1441.9031982421875\n",
      "Epoch: 1374, D Loss: 210.33091735839844, G Loss: 685.1875\n",
      "Epoch: 1375, D Loss: 263.0555419921875, G Loss: 459.9221496582031\n",
      "Epoch: 1376, D Loss: 146.59068298339844, G Loss: 291.4897766113281\n",
      "Epoch: 1377, D Loss: -40.144187927246094, G Loss: 519.7072143554688\n",
      "Epoch: 1378, D Loss: -41.42509460449219, G Loss: 997.4853515625\n",
      "Epoch: 1379, D Loss: -25.083656311035156, G Loss: 1261.583984375\n",
      "Epoch: 1380, D Loss: -15.195188522338867, G Loss: 508.64422607421875\n",
      "Epoch: 1381, D Loss: -67.80939483642578, G Loss: 448.581298828125\n",
      "Epoch: 1382, D Loss: -64.73345184326172, G Loss: 606.4685668945312\n",
      "Epoch: 1383, D Loss: -48.61781311035156, G Loss: 377.9052429199219\n",
      "Epoch: 1384, D Loss: -76.42385864257812, G Loss: 393.0805969238281\n",
      "Epoch: 1385, D Loss: 45.67793655395508, G Loss: 41.525997161865234\n",
      "Epoch: 1386, D Loss: -75.0020523071289, G Loss: 111.88465118408203\n",
      "Epoch: 1387, D Loss: -33.08721923828125, G Loss: 421.8477783203125\n",
      "Epoch: 1388, D Loss: -88.14981079101562, G Loss: 649.3157348632812\n",
      "Epoch: 1389, D Loss: 69.70276641845703, G Loss: 237.0369873046875\n",
      "Epoch: 1390, D Loss: -19.509077072143555, G Loss: 408.156982421875\n",
      "Epoch: 1391, D Loss: -7.61958122253418, G Loss: 825.005615234375\n",
      "Epoch: 1392, D Loss: -10.907332420349121, G Loss: 226.1470184326172\n",
      "Epoch: 1393, D Loss: 45.59177780151367, G Loss: 217.07952880859375\n",
      "Epoch: 1394, D Loss: -34.64937973022461, G Loss: 230.9479217529297\n",
      "Epoch: 1395, D Loss: -36.406471252441406, G Loss: 324.1909484863281\n",
      "Epoch: 1396, D Loss: -14.411340713500977, G Loss: 631.345947265625\n",
      "Epoch: 1397, D Loss: -42.5317268371582, G Loss: 741.7978515625\n",
      "Epoch: 1398, D Loss: -23.826868057250977, G Loss: 445.1231994628906\n",
      "Epoch: 1399, D Loss: -31.92168426513672, G Loss: 462.3293762207031\n",
      "Epoch: 1400, D Loss: -30.80794334411621, G Loss: 767.4029541015625\n",
      "Epoch: 1401, D Loss: -18.65559959411621, G Loss: 1131.0743408203125\n",
      "Epoch: 1402, D Loss: -34.53778839111328, G Loss: 725.0418701171875\n",
      "Epoch: 1403, D Loss: -8.29244327545166, G Loss: 800.3493041992188\n",
      "Epoch: 1404, D Loss: 26.063291549682617, G Loss: 726.903564453125\n",
      "Epoch: 1405, D Loss: -40.38517761230469, G Loss: 225.07394409179688\n",
      "Epoch: 1406, D Loss: 5.893059253692627, G Loss: 304.6207275390625\n",
      "Epoch: 1407, D Loss: -29.81559181213379, G Loss: 402.4979248046875\n",
      "Epoch: 1408, D Loss: -65.72540283203125, G Loss: 343.30645751953125\n",
      "Epoch: 1409, D Loss: -80.84012603759766, G Loss: 285.7711181640625\n",
      "Epoch: 1410, D Loss: -44.6003532409668, G Loss: 318.9254150390625\n",
      "Epoch: 1411, D Loss: -62.64106369018555, G Loss: 135.70596313476562\n",
      "Epoch: 1412, D Loss: -79.77555847167969, G Loss: 429.58154296875\n",
      "Epoch: 1413, D Loss: -65.41220092773438, G Loss: 297.7990417480469\n",
      "Epoch: 1414, D Loss: -70.73513793945312, G Loss: 379.3730163574219\n",
      "Epoch: 1415, D Loss: -113.82247924804688, G Loss: 507.98858642578125\n",
      "Epoch: 1416, D Loss: -65.38643646240234, G Loss: 972.7427978515625\n",
      "Epoch: 1417, D Loss: -43.0675048828125, G Loss: 592.4866333007812\n",
      "Epoch: 1418, D Loss: -91.04786682128906, G Loss: 581.7432861328125\n",
      "Epoch: 1419, D Loss: -78.85686492919922, G Loss: 487.85906982421875\n",
      "Epoch: 1420, D Loss: -107.50018310546875, G Loss: 120.512939453125\n",
      "Epoch: 1421, D Loss: -68.9385986328125, G Loss: 877.31787109375\n",
      "Epoch: 1422, D Loss: -21.00394058227539, G Loss: 296.447265625\n",
      "Epoch: 1423, D Loss: -40.50257873535156, G Loss: 255.33729553222656\n",
      "Epoch: 1424, D Loss: -102.43913269042969, G Loss: 765.2772216796875\n",
      "Epoch: 1425, D Loss: -47.5267333984375, G Loss: 612.0491333007812\n",
      "Epoch: 1426, D Loss: -53.20525360107422, G Loss: 1107.3798828125\n",
      "Epoch: 1427, D Loss: -42.12593078613281, G Loss: 469.0176086425781\n",
      "Epoch: 1428, D Loss: -48.64983367919922, G Loss: 900.18505859375\n",
      "Epoch: 1429, D Loss: 56.9167366027832, G Loss: 277.40142822265625\n",
      "Epoch: 1430, D Loss: -50.9318962097168, G Loss: 874.0241088867188\n",
      "Epoch: 1431, D Loss: -81.4979248046875, G Loss: 856.8265380859375\n",
      "Epoch: 1432, D Loss: -62.26807403564453, G Loss: 309.9761047363281\n",
      "Epoch: 1433, D Loss: 13.49260139465332, G Loss: 509.49566650390625\n",
      "Epoch: 1434, D Loss: -31.73554801940918, G Loss: 167.65182495117188\n",
      "Epoch: 1435, D Loss: -15.888278007507324, G Loss: 283.1208190917969\n",
      "Epoch: 1436, D Loss: -23.69774055480957, G Loss: 515.3262939453125\n",
      "Epoch: 1437, D Loss: -48.692806243896484, G Loss: 361.06646728515625\n",
      "Epoch: 1438, D Loss: -52.94481658935547, G Loss: 271.70361328125\n",
      "Epoch: 1439, D Loss: -11.312891006469727, G Loss: 319.8554992675781\n",
      "Epoch: 1440, D Loss: -34.68507385253906, G Loss: 690.7352294921875\n",
      "Epoch: 1441, D Loss: -80.81342315673828, G Loss: 211.24717712402344\n",
      "Epoch: 1442, D Loss: -63.40076446533203, G Loss: 366.368896484375\n",
      "Epoch: 1443, D Loss: 2.2597618103027344, G Loss: 873.35009765625\n",
      "Epoch: 1444, D Loss: -106.38079833984375, G Loss: 457.9925537109375\n",
      "Epoch: 1445, D Loss: -87.3609619140625, G Loss: 612.99267578125\n",
      "Epoch: 1446, D Loss: -77.1875, G Loss: 1000.459228515625\n",
      "Epoch: 1447, D Loss: -155.5054473876953, G Loss: 297.9705810546875\n",
      "Epoch: 1448, D Loss: -27.481979370117188, G Loss: 632.487548828125\n",
      "Epoch: 1449, D Loss: -84.82037353515625, G Loss: 1067.94287109375\n",
      "Epoch: 1450, D Loss: -9.746650695800781, G Loss: 464.74273681640625\n",
      "Epoch: 1451, D Loss: -41.553951263427734, G Loss: 212.9851531982422\n",
      "Epoch: 1452, D Loss: -33.19734191894531, G Loss: 528.87255859375\n",
      "Epoch: 1453, D Loss: -29.358642578125, G Loss: 627.546142578125\n",
      "Epoch: 1454, D Loss: -27.21583366394043, G Loss: 582.9268188476562\n",
      "Epoch: 1455, D Loss: -63.44235610961914, G Loss: 822.343017578125\n",
      "Epoch: 1456, D Loss: -95.75252532958984, G Loss: 525.4569091796875\n",
      "Epoch: 1457, D Loss: -91.07830810546875, G Loss: 1188.396484375\n",
      "Epoch: 1458, D Loss: -100.55276489257812, G Loss: 604.0581665039062\n",
      "Epoch: 1459, D Loss: -118.06834411621094, G Loss: 793.7666625976562\n",
      "Epoch: 1460, D Loss: -76.17687225341797, G Loss: 1354.520751953125\n",
      "Epoch: 1461, D Loss: -59.143524169921875, G Loss: 778.0069580078125\n",
      "Epoch: 1462, D Loss: -68.93487548828125, G Loss: 572.294677734375\n",
      "Epoch: 1463, D Loss: -30.692867279052734, G Loss: 164.57298278808594\n",
      "Epoch: 1464, D Loss: -38.313392639160156, G Loss: 502.990966796875\n",
      "Epoch: 1465, D Loss: -65.87408447265625, G Loss: 369.9350891113281\n",
      "Epoch: 1466, D Loss: -74.71041870117188, G Loss: 724.082275390625\n",
      "Epoch: 1467, D Loss: -103.42709350585938, G Loss: 644.0437622070312\n",
      "Epoch: 1468, D Loss: -47.14409637451172, G Loss: 517.1134643554688\n",
      "Epoch: 1469, D Loss: -35.77840042114258, G Loss: 466.0571594238281\n",
      "Epoch: 1470, D Loss: -67.48703002929688, G Loss: 798.824951171875\n",
      "Epoch: 1471, D Loss: -84.05268096923828, G Loss: 516.344482421875\n",
      "Epoch: 1472, D Loss: -72.87726593017578, G Loss: 231.0600128173828\n",
      "Epoch: 1473, D Loss: -10015.5703125, G Loss: 372.755615234375\n",
      "Epoch: 1474, D Loss: -18143.337890625, G Loss: 316.27178955078125\n",
      "Epoch: 1475, D Loss: -27158.080078125, G Loss: 2197.9814453125\n",
      "Epoch: 1476, D Loss: 17565.791015625, G Loss: 772.3701171875\n",
      "Epoch: 1477, D Loss: 3700.257568359375, G Loss: 1286.5902099609375\n",
      "Epoch: 1478, D Loss: 3012.6669921875, G Loss: 27.147397994995117\n",
      "Epoch: 1479, D Loss: 217.45697021484375, G Loss: 1462.7728271484375\n",
      "Epoch: 1480, D Loss: 455.0494689941406, G Loss: 309.9246520996094\n",
      "Epoch: 1481, D Loss: 383.9960632324219, G Loss: 941.0633544921875\n",
      "Epoch: 1482, D Loss: 621.2762451171875, G Loss: 437.3991394042969\n",
      "Epoch: 1483, D Loss: 356.287841796875, G Loss: 1433.93359375\n",
      "Epoch: 1484, D Loss: 804.63720703125, G Loss: 1241.562255859375\n",
      "Epoch: 1485, D Loss: 618.691650390625, G Loss: 477.5902099609375\n",
      "Epoch: 1486, D Loss: 630.402099609375, G Loss: 217.86163330078125\n",
      "Epoch: 1487, D Loss: 478.2489013671875, G Loss: 252.7211151123047\n",
      "Epoch: 1488, D Loss: 682.4544677734375, G Loss: 1292.0892333984375\n",
      "Epoch: 1489, D Loss: 539.394287109375, G Loss: 3397.99462890625\n",
      "Epoch: 1490, D Loss: 431.91058349609375, G Loss: 962.8185424804688\n",
      "Epoch: 1491, D Loss: 188.74417114257812, G Loss: 532.3382568359375\n",
      "Epoch: 1492, D Loss: 89.10342407226562, G Loss: 313.7171630859375\n",
      "Epoch: 1493, D Loss: 12.42932415008545, G Loss: 1054.0426025390625\n",
      "Epoch: 1494, D Loss: -36.17875671386719, G Loss: 535.84228515625\n",
      "Epoch: 1495, D Loss: -30.09447479248047, G Loss: 269.5786437988281\n",
      "Epoch: 1496, D Loss: -2.4628334045410156, G Loss: 291.585205078125\n",
      "Epoch: 1497, D Loss: 35.183815002441406, G Loss: 281.552978515625\n",
      "Epoch: 1498, D Loss: 54.078189849853516, G Loss: 148.35816955566406\n",
      "Epoch: 1499, D Loss: -19.292428970336914, G Loss: 235.0625\n",
      "Epoch: 1500, D Loss: 182.63682556152344, G Loss: 426.2379150390625\n",
      "Epoch: 1501, D Loss: 218.93792724609375, G Loss: 190.79434204101562\n",
      "Epoch: 1502, D Loss: 262.1541748046875, G Loss: 129.28817749023438\n",
      "Epoch: 1503, D Loss: 287.3656005859375, G Loss: 240.33120727539062\n",
      "Epoch: 1504, D Loss: 175.78128051757812, G Loss: 102.48686981201172\n",
      "Epoch: 1505, D Loss: 128.61080932617188, G Loss: 322.8935546875\n",
      "Epoch: 1506, D Loss: 107.31953430175781, G Loss: 418.67852783203125\n",
      "Epoch: 1507, D Loss: 52.50163269042969, G Loss: 387.60076904296875\n",
      "Epoch: 1508, D Loss: 20.565994262695312, G Loss: 402.61029052734375\n",
      "Epoch: 1509, D Loss: 47.567359924316406, G Loss: 959.526611328125\n",
      "Epoch: 1510, D Loss: 69.97840118408203, G Loss: 1473.75341796875\n",
      "Epoch: 1511, D Loss: 89.38268280029297, G Loss: 1113.266357421875\n",
      "Epoch: 1512, D Loss: 47.68220901489258, G Loss: 213.10215759277344\n",
      "Epoch: 1513, D Loss: 82.62830352783203, G Loss: 469.86083984375\n",
      "Epoch: 1514, D Loss: 188.1563262939453, G Loss: 1262.9896240234375\n",
      "Epoch: 1515, D Loss: 11.996940612792969, G Loss: 230.84027099609375\n",
      "Epoch: 1516, D Loss: -25.69601058959961, G Loss: 416.5789794921875\n",
      "Epoch: 1517, D Loss: 225.65020751953125, G Loss: 327.0040283203125\n",
      "Epoch: 1518, D Loss: -41.4104118347168, G Loss: 337.7301330566406\n",
      "Epoch: 1519, D Loss: 16.91493797302246, G Loss: 229.15623474121094\n",
      "Epoch: 1520, D Loss: -12.79323673248291, G Loss: 1304.92431640625\n",
      "Epoch: 1521, D Loss: 78.44512176513672, G Loss: 371.11297607421875\n",
      "Epoch: 1522, D Loss: -7.608523845672607, G Loss: 320.5279846191406\n",
      "Epoch: 1523, D Loss: 17.862674713134766, G Loss: 1642.511962890625\n",
      "Epoch: 1524, D Loss: 163.6132049560547, G Loss: 1585.772705078125\n",
      "Epoch: 1525, D Loss: 15.547607421875, G Loss: 897.0614013671875\n",
      "Epoch: 1526, D Loss: 61.65103530883789, G Loss: 895.9097290039062\n",
      "Epoch: 1527, D Loss: 56.08010482788086, G Loss: 1016.459716796875\n",
      "Epoch: 1528, D Loss: -9.515411376953125, G Loss: 427.7276306152344\n",
      "Epoch: 1529, D Loss: -3.377197265625, G Loss: 84.16881561279297\n",
      "Epoch: 1530, D Loss: -26.931108474731445, G Loss: 361.7651672363281\n",
      "Epoch: 1531, D Loss: -9.25044059753418, G Loss: 615.3245849609375\n",
      "Epoch: 1532, D Loss: 28.561351776123047, G Loss: 381.50262451171875\n",
      "Epoch: 1533, D Loss: -1.129434585571289, G Loss: 395.5342102050781\n",
      "Epoch: 1534, D Loss: 67.04936218261719, G Loss: 770.963134765625\n",
      "Epoch: 1535, D Loss: 18.24669647216797, G Loss: 1290.7467041015625\n",
      "Epoch: 1536, D Loss: 149.89077758789062, G Loss: 795.1192016601562\n",
      "Epoch: 1537, D Loss: 454.41510009765625, G Loss: 80.31665802001953\n",
      "Epoch: 1538, D Loss: 603.6600341796875, G Loss: 54.32521057128906\n",
      "Epoch: 1539, D Loss: 266.8909606933594, G Loss: 26.82342529296875\n",
      "Epoch: 1540, D Loss: 145.82078552246094, G Loss: 102.34683227539062\n",
      "Epoch: 1541, D Loss: 107.7653579711914, G Loss: 46.24858856201172\n",
      "Epoch: 1542, D Loss: 68.18124389648438, G Loss: 256.00543212890625\n",
      "Epoch: 1543, D Loss: 8.259191513061523, G Loss: 273.87786865234375\n",
      "Epoch: 1544, D Loss: 17.87281036376953, G Loss: 251.48480224609375\n",
      "Epoch: 1545, D Loss: -17.3492431640625, G Loss: 170.6768035888672\n",
      "Epoch: 1546, D Loss: 15.816234588623047, G Loss: 285.70782470703125\n",
      "Epoch: 1547, D Loss: -1.411813735961914, G Loss: 248.94741821289062\n",
      "Epoch: 1548, D Loss: -19.06307601928711, G Loss: 162.21456909179688\n",
      "Epoch: 1549, D Loss: -33.757747650146484, G Loss: 152.40234375\n",
      "Epoch: 1550, D Loss: -43.516563415527344, G Loss: 332.69921875\n",
      "Epoch: 1551, D Loss: -36.155059814453125, G Loss: 460.3987731933594\n",
      "Epoch: 1552, D Loss: -45.095760345458984, G Loss: 773.56591796875\n",
      "Epoch: 1553, D Loss: -64.14883422851562, G Loss: 213.498291015625\n",
      "Epoch: 1554, D Loss: -34.0283088684082, G Loss: 598.3780517578125\n",
      "Epoch: 1555, D Loss: -18.63878059387207, G Loss: 508.61041259765625\n",
      "Epoch: 1556, D Loss: 81.92558288574219, G Loss: 540.7124633789062\n",
      "Epoch: 1557, D Loss: 122.01744842529297, G Loss: 1394.7205810546875\n",
      "Epoch: 1558, D Loss: 22.229816436767578, G Loss: 1453.94140625\n",
      "Epoch: 1559, D Loss: 166.20602416992188, G Loss: 1081.3525390625\n",
      "Epoch: 1560, D Loss: 78.96720123291016, G Loss: 1506.43798828125\n",
      "Epoch: 1561, D Loss: 61.75954055786133, G Loss: 1644.89306640625\n",
      "Epoch: 1562, D Loss: 14.583047866821289, G Loss: 102.13217163085938\n",
      "Epoch: 1563, D Loss: 68.51753997802734, G Loss: 928.9603271484375\n",
      "Epoch: 1564, D Loss: 308.75714111328125, G Loss: 235.93421936035156\n",
      "Epoch: 1565, D Loss: 75.02127838134766, G Loss: 846.4657592773438\n",
      "Epoch: 1566, D Loss: -4.054995536804199, G Loss: 1046.008056640625\n",
      "Epoch: 1567, D Loss: 67.216796875, G Loss: 625.055908203125\n",
      "Epoch: 1568, D Loss: 33.098876953125, G Loss: 458.7752685546875\n",
      "Epoch: 1569, D Loss: 286.26593017578125, G Loss: 1333.9576416015625\n",
      "Epoch: 1570, D Loss: 81.40464782714844, G Loss: 586.2430419921875\n",
      "Epoch: 1571, D Loss: 24.406034469604492, G Loss: 412.8279113769531\n",
      "Epoch: 1572, D Loss: -9.636049270629883, G Loss: 174.4425048828125\n",
      "Epoch: 1573, D Loss: -25.336660385131836, G Loss: 532.7857666015625\n",
      "Epoch: 1574, D Loss: 19.19204330444336, G Loss: 224.17251586914062\n",
      "Epoch: 1575, D Loss: 26.391443252563477, G Loss: 509.092529296875\n",
      "Epoch: 1576, D Loss: -8.033917427062988, G Loss: 423.59429931640625\n",
      "Epoch: 1577, D Loss: 74.10118865966797, G Loss: 167.93385314941406\n",
      "Epoch: 1578, D Loss: -1.7524585723876953, G Loss: 445.6832580566406\n",
      "Epoch: 1579, D Loss: -21.641433715820312, G Loss: 431.3037414550781\n",
      "Epoch: 1580, D Loss: -2.908842086791992, G Loss: 696.2266235351562\n",
      "Epoch: 1581, D Loss: -25.662593841552734, G Loss: 669.74560546875\n",
      "Epoch: 1582, D Loss: -0.051654815673828125, G Loss: 267.63616943359375\n",
      "Epoch: 1583, D Loss: -22.332666397094727, G Loss: 295.746826171875\n",
      "Epoch: 1584, D Loss: -4.66953182220459, G Loss: 248.1396484375\n",
      "Epoch: 1585, D Loss: -39.95469284057617, G Loss: 385.0135498046875\n",
      "Epoch: 1586, D Loss: -16.687885284423828, G Loss: 535.6231689453125\n",
      "Epoch: 1587, D Loss: -31.534439086914062, G Loss: 528.2495727539062\n",
      "Epoch: 1588, D Loss: 23.150299072265625, G Loss: 384.1881408691406\n",
      "Epoch: 1589, D Loss: 46.7999382019043, G Loss: 455.71734619140625\n",
      "Epoch: 1590, D Loss: 75.24114990234375, G Loss: 925.966064453125\n",
      "Epoch: 1591, D Loss: 75.39049530029297, G Loss: 321.0567626953125\n",
      "Epoch: 1592, D Loss: 437.86358642578125, G Loss: 59.16527557373047\n",
      "Epoch: 1593, D Loss: 43.62579345703125, G Loss: 969.27880859375\n",
      "Epoch: 1594, D Loss: 28.368896484375, G Loss: 272.2032470703125\n",
      "Epoch: 1595, D Loss: 110.04627227783203, G Loss: 446.9962463378906\n",
      "Epoch: 1596, D Loss: 135.3862762451172, G Loss: 91.77819061279297\n",
      "Epoch: 1597, D Loss: 84.54371643066406, G Loss: 64.90575408935547\n",
      "Epoch: 1598, D Loss: 10.0780611038208, G Loss: 272.4739990234375\n",
      "Epoch: 1599, D Loss: 14.45176887512207, G Loss: 404.942626953125\n",
      "Epoch: 1600, D Loss: 6.7746901512146, G Loss: 722.0669555664062\n",
      "Epoch: 1601, D Loss: 10.131698608398438, G Loss: 125.92916870117188\n",
      "Epoch: 1602, D Loss: 12.065088272094727, G Loss: 349.06500244140625\n",
      "Epoch: 1603, D Loss: 0.4356365203857422, G Loss: 573.5870361328125\n",
      "Epoch: 1604, D Loss: -33.54071807861328, G Loss: 354.80657958984375\n",
      "Epoch: 1605, D Loss: -32.250572204589844, G Loss: 319.4794921875\n",
      "Epoch: 1606, D Loss: -14.002126693725586, G Loss: 422.3350830078125\n",
      "Epoch: 1607, D Loss: -6.327099323272705, G Loss: 362.012451171875\n",
      "Epoch: 1608, D Loss: -42.854400634765625, G Loss: 283.21539306640625\n",
      "Epoch: 1609, D Loss: -23.975566864013672, G Loss: 204.5206298828125\n",
      "Epoch: 1610, D Loss: -53.909454345703125, G Loss: 396.294189453125\n",
      "Epoch: 1611, D Loss: -7.585624694824219, G Loss: 448.1591796875\n",
      "Epoch: 1612, D Loss: 25.192625045776367, G Loss: 294.19537353515625\n",
      "Epoch: 1613, D Loss: 6.704151153564453, G Loss: 491.911376953125\n",
      "Epoch: 1614, D Loss: 96.07145690917969, G Loss: 332.0381164550781\n",
      "Epoch: 1615, D Loss: 39.85247802734375, G Loss: 416.84552001953125\n",
      "Epoch: 1616, D Loss: 22.065940856933594, G Loss: 196.27972412109375\n",
      "Epoch: 1617, D Loss: -20.598052978515625, G Loss: 547.282958984375\n",
      "Epoch: 1618, D Loss: -40.85686111450195, G Loss: 484.5273742675781\n",
      "Epoch: 1619, D Loss: -28.31230926513672, G Loss: 836.0907592773438\n",
      "Epoch: 1620, D Loss: -4.097311019897461, G Loss: 880.1722412109375\n",
      "Epoch: 1621, D Loss: 21.81585693359375, G Loss: 501.4953308105469\n",
      "Epoch: 1622, D Loss: -19.837366104125977, G Loss: 769.5031127929688\n",
      "Epoch: 1623, D Loss: -23.260631561279297, G Loss: 1150.196044921875\n",
      "Epoch: 1624, D Loss: 3.3071398735046387, G Loss: 401.73114013671875\n",
      "Epoch: 1625, D Loss: 3.961578369140625, G Loss: 315.9777526855469\n",
      "Epoch: 1626, D Loss: -24.637222290039062, G Loss: 248.76971435546875\n",
      "Epoch: 1627, D Loss: -24.669841766357422, G Loss: 347.2889404296875\n",
      "Epoch: 1628, D Loss: -14.791345596313477, G Loss: 471.457275390625\n",
      "Epoch: 1629, D Loss: 2.1184191703796387, G Loss: 274.372802734375\n",
      "Epoch: 1630, D Loss: -66.13485717773438, G Loss: 186.69744873046875\n",
      "Epoch: 1631, D Loss: -61.41686248779297, G Loss: 458.65643310546875\n",
      "Epoch: 1632, D Loss: -35.667884826660156, G Loss: 603.63916015625\n",
      "Epoch: 1633, D Loss: 4.417437553405762, G Loss: 471.7662353515625\n",
      "Epoch: 1634, D Loss: 0.8169755935668945, G Loss: 386.11309814453125\n",
      "Epoch: 1635, D Loss: 33.53871154785156, G Loss: 332.0486755371094\n",
      "Epoch: 1636, D Loss: 31.52211570739746, G Loss: 719.0904541015625\n",
      "Epoch: 1637, D Loss: -2.9221134185791016, G Loss: 278.2569274902344\n",
      "Epoch: 1638, D Loss: -40.61455535888672, G Loss: 828.867431640625\n",
      "Epoch: 1639, D Loss: -39.05514144897461, G Loss: 445.94842529296875\n",
      "Epoch: 1640, D Loss: -67.77384948730469, G Loss: 584.455322265625\n",
      "Epoch: 1641, D Loss: -32.370365142822266, G Loss: 428.4278259277344\n",
      "Epoch: 1642, D Loss: 76.73670959472656, G Loss: 110.22919464111328\n",
      "Epoch: 1643, D Loss: 197.30191040039062, G Loss: 187.22671508789062\n",
      "Epoch: 1644, D Loss: 82.32945251464844, G Loss: 307.63238525390625\n",
      "Epoch: 1645, D Loss: -13.44980525970459, G Loss: 416.03997802734375\n",
      "Epoch: 1646, D Loss: -3.996551036834717, G Loss: 404.3232116699219\n",
      "Epoch: 1647, D Loss: 21.949535369873047, G Loss: 353.7150573730469\n",
      "Epoch: 1648, D Loss: 40.88528823852539, G Loss: 331.12274169921875\n",
      "Epoch: 1649, D Loss: -41.402931213378906, G Loss: 283.62744140625\n",
      "Epoch: 1650, D Loss: -17.012287139892578, G Loss: 289.48138427734375\n",
      "Epoch: 1651, D Loss: -50.542999267578125, G Loss: 183.661865234375\n",
      "Epoch: 1652, D Loss: -70.24424743652344, G Loss: 167.63479614257812\n",
      "Epoch: 1653, D Loss: -39.961463928222656, G Loss: 325.55426025390625\n",
      "Epoch: 1654, D Loss: -108.85575866699219, G Loss: 491.78192138671875\n",
      "Epoch: 1655, D Loss: -76.34422302246094, G Loss: 178.85223388671875\n",
      "Epoch: 1656, D Loss: -54.556129455566406, G Loss: 254.16400146484375\n",
      "Epoch: 1657, D Loss: -46.969661712646484, G Loss: 486.3114013671875\n",
      "Epoch: 1658, D Loss: -140.78521728515625, G Loss: 191.82334899902344\n",
      "Epoch: 1659, D Loss: -70.06535339355469, G Loss: 63.50531768798828\n",
      "Epoch: 1660, D Loss: -144.52345275878906, G Loss: 172.888427734375\n",
      "Epoch: 1661, D Loss: -166.72543334960938, G Loss: 609.1296997070312\n",
      "Epoch: 1662, D Loss: -152.49868774414062, G Loss: 448.63458251953125\n",
      "Epoch: 1663, D Loss: -815.7913208007812, G Loss: 435.14471435546875\n",
      "Epoch: 1664, D Loss: -1552.3260498046875, G Loss: 2139.775634765625\n",
      "Epoch: 1665, D Loss: 226.84246826171875, G Loss: 803.4075927734375\n",
      "Epoch: 1666, D Loss: 2518.193115234375, G Loss: 4873.9951171875\n",
      "Epoch: 1667, D Loss: 291.6650695800781, G Loss: 1977.80224609375\n",
      "Epoch: 1668, D Loss: 761.86181640625, G Loss: 5.9630032026356634e-34\n",
      "Epoch: 1669, D Loss: 299.6221923828125, G Loss: 67.42498779296875\n",
      "Epoch: 1670, D Loss: 395.1726379394531, G Loss: 1171.782470703125\n",
      "Epoch: 1671, D Loss: 0.2191143035888672, G Loss: 1726.437255859375\n",
      "Epoch: 1672, D Loss: 66.85466003417969, G Loss: 1594.001220703125\n",
      "Epoch: 1673, D Loss: 276.66387939453125, G Loss: 273.39849853515625\n",
      "Epoch: 1674, D Loss: 286.0223388671875, G Loss: 423.81097412109375\n",
      "Epoch: 1675, D Loss: 66.41248321533203, G Loss: 790.817626953125\n",
      "Epoch: 1676, D Loss: 132.53973388671875, G Loss: 1331.5374755859375\n",
      "Epoch: 1677, D Loss: 202.06570434570312, G Loss: 751.9429321289062\n",
      "Epoch: 1678, D Loss: 201.08706665039062, G Loss: 589.37841796875\n",
      "Epoch: 1679, D Loss: -14.865132331848145, G Loss: 378.339599609375\n",
      "Epoch: 1680, D Loss: 26.507291793823242, G Loss: 747.8006591796875\n",
      "Epoch: 1681, D Loss: -5.240860462188721, G Loss: 849.57421875\n",
      "Epoch: 1682, D Loss: -7.743585586547852, G Loss: 349.40679931640625\n",
      "Epoch: 1683, D Loss: -31.577735900878906, G Loss: 328.3297119140625\n",
      "Epoch: 1684, D Loss: -60.571266174316406, G Loss: 305.369140625\n",
      "Epoch: 1685, D Loss: -52.51518630981445, G Loss: 268.7811279296875\n",
      "Epoch: 1686, D Loss: -31.505504608154297, G Loss: 634.5411987304688\n",
      "Epoch: 1687, D Loss: -18.344749450683594, G Loss: 202.5416259765625\n",
      "Epoch: 1688, D Loss: -15.248422622680664, G Loss: 251.77560424804688\n",
      "Epoch: 1689, D Loss: -36.35154342651367, G Loss: 679.2747802734375\n",
      "Epoch: 1690, D Loss: -17.422657012939453, G Loss: 765.5169677734375\n",
      "Epoch: 1691, D Loss: -25.673240661621094, G Loss: 587.7952270507812\n",
      "Epoch: 1692, D Loss: -33.35154724121094, G Loss: 226.57730102539062\n",
      "Epoch: 1693, D Loss: -1.6400971412658691, G Loss: 661.330078125\n",
      "Epoch: 1694, D Loss: -23.360605239868164, G Loss: 312.7876892089844\n",
      "Epoch: 1695, D Loss: -29.704383850097656, G Loss: 788.906494140625\n",
      "Epoch: 1696, D Loss: -29.178966522216797, G Loss: 708.1734619140625\n",
      "Epoch: 1697, D Loss: -74.47361755371094, G Loss: 1001.6500854492188\n",
      "Epoch: 1698, D Loss: -93.05865478515625, G Loss: 598.1085815429688\n",
      "Epoch: 1699, D Loss: -86.45011138916016, G Loss: 468.983154296875\n",
      "Epoch: 1700, D Loss: -10.893768310546875, G Loss: 619.2603759765625\n",
      "Epoch: 1701, D Loss: 54.31109619140625, G Loss: 1219.1138916015625\n",
      "Epoch: 1702, D Loss: 49.46985626220703, G Loss: 195.53636169433594\n",
      "Epoch: 1703, D Loss: 76.42916870117188, G Loss: 599.636474609375\n",
      "Epoch: 1704, D Loss: 54.470436096191406, G Loss: 931.0365600585938\n",
      "Epoch: 1705, D Loss: 39.42680358886719, G Loss: 832.4073486328125\n",
      "Epoch: 1706, D Loss: 190.78762817382812, G Loss: 314.42388916015625\n",
      "Epoch: 1707, D Loss: 97.30860900878906, G Loss: 219.13287353515625\n",
      "Epoch: 1708, D Loss: -33.3592529296875, G Loss: 348.51068115234375\n",
      "Epoch: 1709, D Loss: 20.730022430419922, G Loss: 192.40255737304688\n",
      "Epoch: 1710, D Loss: 38.43867492675781, G Loss: 283.4521484375\n",
      "Epoch: 1711, D Loss: -30.793899536132812, G Loss: 546.1841430664062\n",
      "Epoch: 1712, D Loss: -29.836326599121094, G Loss: 1050.4783935546875\n",
      "Epoch: 1713, D Loss: -47.155601501464844, G Loss: 386.7042541503906\n",
      "Epoch: 1714, D Loss: -31.796449661254883, G Loss: 357.5083923339844\n",
      "Epoch: 1715, D Loss: -29.207443237304688, G Loss: 409.619384765625\n",
      "Epoch: 1716, D Loss: -28.251407623291016, G Loss: 259.83660888671875\n",
      "Epoch: 1717, D Loss: -63.014259338378906, G Loss: 482.6674499511719\n",
      "Epoch: 1718, D Loss: -60.53534698486328, G Loss: 278.71136474609375\n",
      "Epoch: 1719, D Loss: -73.076416015625, G Loss: 412.8302307128906\n",
      "Epoch: 1720, D Loss: -47.86933898925781, G Loss: 246.1048126220703\n",
      "Epoch: 1721, D Loss: -14.31198501586914, G Loss: 400.0712890625\n",
      "Epoch: 1722, D Loss: -27.06052017211914, G Loss: 413.3620300292969\n",
      "Epoch: 1723, D Loss: -146.57325744628906, G Loss: 559.0654296875\n",
      "Epoch: 1724, D Loss: -127.09902954101562, G Loss: 428.5006103515625\n",
      "Epoch: 1725, D Loss: -76.88078308105469, G Loss: 825.059326171875\n",
      "Epoch: 1726, D Loss: -7.190509796142578, G Loss: 543.343994140625\n",
      "Epoch: 1727, D Loss: -13.210926055908203, G Loss: 515.724853515625\n",
      "Epoch: 1728, D Loss: 38.7389030456543, G Loss: 675.21435546875\n",
      "Epoch: 1729, D Loss: -6.456925392150879, G Loss: 502.2121276855469\n",
      "Epoch: 1730, D Loss: 129.93521118164062, G Loss: 1451.2017822265625\n",
      "Epoch: 1731, D Loss: -0.17619800567626953, G Loss: 570.4036865234375\n",
      "Epoch: 1732, D Loss: 74.98954010009766, G Loss: 553.3018188476562\n",
      "Epoch: 1733, D Loss: -79.31178283691406, G Loss: 554.38525390625\n",
      "Epoch: 1734, D Loss: -84.25331115722656, G Loss: 762.0738525390625\n",
      "Epoch: 1735, D Loss: -54.269596099853516, G Loss: 872.8905029296875\n",
      "Epoch: 1736, D Loss: -32.132442474365234, G Loss: 233.52841186523438\n",
      "Epoch: 1737, D Loss: 390.8948974609375, G Loss: 583.488525390625\n",
      "Epoch: 1738, D Loss: 79.04289245605469, G Loss: 744.028564453125\n",
      "Epoch: 1739, D Loss: 105.0444564819336, G Loss: 287.35992431640625\n",
      "Epoch: 1740, D Loss: 18.734031677246094, G Loss: 329.599853515625\n",
      "Epoch: 1741, D Loss: 56.73938751220703, G Loss: 334.4518737792969\n",
      "Epoch: 1742, D Loss: -10.438034057617188, G Loss: 534.8595581054688\n",
      "Epoch: 1743, D Loss: -22.758804321289062, G Loss: 467.8236083984375\n",
      "Epoch: 1744, D Loss: -19.199630737304688, G Loss: 378.2767639160156\n",
      "Epoch: 1745, D Loss: -48.397300720214844, G Loss: 330.766357421875\n",
      "Epoch: 1746, D Loss: -40.64535140991211, G Loss: 838.6787109375\n",
      "Epoch: 1747, D Loss: 11.396411895751953, G Loss: 146.90296936035156\n",
      "Epoch: 1748, D Loss: 48.478233337402344, G Loss: 760.9576416015625\n",
      "Epoch: 1749, D Loss: 140.07461547851562, G Loss: 1451.01416015625\n",
      "Epoch: 1750, D Loss: 166.858154296875, G Loss: 682.3063354492188\n",
      "Epoch: 1751, D Loss: -14.350789070129395, G Loss: 240.57171630859375\n",
      "Epoch: 1752, D Loss: -4.882050514221191, G Loss: 270.415283203125\n",
      "Epoch: 1753, D Loss: -20.974628448486328, G Loss: 713.34423828125\n",
      "Epoch: 1754, D Loss: 6.006885528564453, G Loss: 597.5440673828125\n",
      "Epoch: 1755, D Loss: 25.4582462310791, G Loss: 328.3876953125\n",
      "Epoch: 1756, D Loss: 43.76461410522461, G Loss: 183.40438842773438\n",
      "Epoch: 1757, D Loss: 22.63125228881836, G Loss: 404.4215393066406\n",
      "Epoch: 1758, D Loss: 17.61651039123535, G Loss: 284.26605224609375\n",
      "Epoch: 1759, D Loss: -34.78785705566406, G Loss: 213.04833984375\n",
      "Epoch: 1760, D Loss: -50.01976013183594, G Loss: 433.9102478027344\n",
      "Epoch: 1761, D Loss: -57.6321907043457, G Loss: 371.3094787597656\n",
      "Epoch: 1762, D Loss: -85.81981658935547, G Loss: 692.4403076171875\n",
      "Epoch: 1763, D Loss: -14.625661849975586, G Loss: 232.08425903320312\n",
      "Epoch: 1764, D Loss: 92.23274230957031, G Loss: 371.70013427734375\n",
      "Epoch: 1765, D Loss: 39.382232666015625, G Loss: 211.11767578125\n",
      "Epoch: 1766, D Loss: -150.1202392578125, G Loss: 544.65283203125\n",
      "Epoch: 1767, D Loss: -22.20857810974121, G Loss: 261.978759765625\n",
      "Epoch: 1768, D Loss: -4024.56689453125, G Loss: 413.05902099609375\n",
      "Epoch: 1769, D Loss: -3142.335693359375, G Loss: 1611.7755126953125\n",
      "Epoch: 1770, D Loss: 163.63462829589844, G Loss: 1037.4549560546875\n",
      "Epoch: 1771, D Loss: -781.3510131835938, G Loss: 838.2044067382812\n",
      "Epoch: 1772, D Loss: 74.98007202148438, G Loss: 2841.12451171875\n",
      "Epoch: 1773, D Loss: 164.62112426757812, G Loss: 2171.97314453125\n",
      "Epoch: 1774, D Loss: 77.81886291503906, G Loss: 1480.9090576171875\n",
      "Epoch: 1775, D Loss: -45.36848449707031, G Loss: 889.7283325195312\n",
      "Epoch: 1776, D Loss: -55.40892791748047, G Loss: 1207.28369140625\n",
      "Epoch: 1777, D Loss: 75.90728759765625, G Loss: 1863.8955078125\n",
      "Epoch: 1778, D Loss: 232.450927734375, G Loss: 814.616943359375\n",
      "Epoch: 1779, D Loss: -47.14712905883789, G Loss: 1095.7880859375\n",
      "Epoch: 1780, D Loss: -12.16604995727539, G Loss: 821.1181030273438\n",
      "Epoch: 1781, D Loss: 185.34030151367188, G Loss: 950.6483154296875\n",
      "Epoch: 1782, D Loss: -33.30111312866211, G Loss: 712.7989501953125\n",
      "Epoch: 1783, D Loss: 47.87920379638672, G Loss: 286.9241943359375\n",
      "Epoch: 1784, D Loss: 13.601213455200195, G Loss: 141.5071563720703\n",
      "Epoch: 1785, D Loss: -56.10313415527344, G Loss: 369.08892822265625\n",
      "Epoch: 1786, D Loss: -25.483959197998047, G Loss: 723.535888671875\n",
      "Epoch: 1787, D Loss: 81.95718383789062, G Loss: 510.9856872558594\n",
      "Epoch: 1788, D Loss: 98.11996459960938, G Loss: 232.91758728027344\n",
      "Epoch: 1789, D Loss: 94.81980895996094, G Loss: 1480.964599609375\n",
      "Epoch: 1790, D Loss: 92.49525451660156, G Loss: 845.672119140625\n",
      "Epoch: 1791, D Loss: -9.678495407104492, G Loss: 364.85888671875\n",
      "Epoch: 1792, D Loss: -38.50677490234375, G Loss: 721.173583984375\n",
      "Epoch: 1793, D Loss: -58.347259521484375, G Loss: 337.688232421875\n",
      "Epoch: 1794, D Loss: -78.69419860839844, G Loss: 235.31173706054688\n",
      "Epoch: 1795, D Loss: -51.128597259521484, G Loss: 309.64617919921875\n",
      "Epoch: 1796, D Loss: -61.63386917114258, G Loss: 609.3422241210938\n",
      "Epoch: 1797, D Loss: -69.9248046875, G Loss: 379.532958984375\n",
      "Epoch: 1798, D Loss: -49.101654052734375, G Loss: 288.65777587890625\n",
      "Epoch: 1799, D Loss: -37.41078567504883, G Loss: 828.9647827148438\n",
      "Epoch: 1800, D Loss: -32.24256134033203, G Loss: 1028.64306640625\n",
      "Epoch: 1801, D Loss: -96.65850830078125, G Loss: 1154.7835693359375\n",
      "Epoch: 1802, D Loss: 137.3564453125, G Loss: 515.123779296875\n",
      "Epoch: 1803, D Loss: -66.23593139648438, G Loss: 590.2022094726562\n",
      "Epoch: 1804, D Loss: -106.70989227294922, G Loss: 789.035400390625\n",
      "Epoch: 1805, D Loss: -73.50508117675781, G Loss: 1477.8187255859375\n",
      "Epoch: 1806, D Loss: -53.5043830871582, G Loss: 855.2079467773438\n",
      "Epoch: 1807, D Loss: -79.52053833007812, G Loss: 417.62835693359375\n",
      "Epoch: 1808, D Loss: -55.756690979003906, G Loss: 928.3189697265625\n",
      "Epoch: 1809, D Loss: -45.168087005615234, G Loss: 782.58154296875\n",
      "Epoch: 1810, D Loss: -64.89599609375, G Loss: 2296.58837890625\n",
      "Epoch: 1811, D Loss: 81.42919921875, G Loss: 1788.040771484375\n",
      "Epoch: 1812, D Loss: 124.16191864013672, G Loss: 586.6236572265625\n",
      "Epoch: 1813, D Loss: 244.6404571533203, G Loss: 487.16436767578125\n",
      "Epoch: 1814, D Loss: 30.672792434692383, G Loss: 824.4230346679688\n",
      "Epoch: 1815, D Loss: -19.23973274230957, G Loss: 255.45120239257812\n",
      "Epoch: 1816, D Loss: -30.065723419189453, G Loss: 143.91055297851562\n",
      "Epoch: 1817, D Loss: -47.369056701660156, G Loss: 291.59600830078125\n",
      "Epoch: 1818, D Loss: -81.40658569335938, G Loss: 281.40966796875\n",
      "Epoch: 1819, D Loss: -116.86773681640625, G Loss: 297.2458801269531\n",
      "Epoch: 1820, D Loss: -439.5102233886719, G Loss: 404.8621826171875\n",
      "Epoch: 1821, D Loss: -2578.82666015625, G Loss: 711.7327880859375\n",
      "Epoch: 1822, D Loss: -10129.4892578125, G Loss: 1981.468017578125\n",
      "Epoch: 1823, D Loss: -3011.05419921875, G Loss: 3537.8291015625\n",
      "Epoch: 1824, D Loss: -1244.4696044921875, G Loss: 1135.900146484375\n",
      "Epoch: 1825, D Loss: -640.458984375, G Loss: 652.8990478515625\n",
      "Epoch: 1826, D Loss: -14.318422317504883, G Loss: 356.9570617675781\n",
      "Epoch: 1827, D Loss: -33.435550689697266, G Loss: 644.526123046875\n",
      "Epoch: 1828, D Loss: 61.2003173828125, G Loss: 455.7777099609375\n",
      "Epoch: 1829, D Loss: -11.934404373168945, G Loss: 456.46978759765625\n",
      "Epoch: 1830, D Loss: -42.959407806396484, G Loss: 855.609375\n",
      "Epoch: 1831, D Loss: -93.3595199584961, G Loss: 1107.6658935546875\n",
      "Epoch: 1832, D Loss: -108.66732788085938, G Loss: 779.7401123046875\n",
      "Epoch: 1833, D Loss: -78.52392578125, G Loss: 771.8721313476562\n",
      "Epoch: 1834, D Loss: -98.85725402832031, G Loss: 774.7335815429688\n",
      "Epoch: 1835, D Loss: -147.69061279296875, G Loss: 1421.02783203125\n",
      "Epoch: 1836, D Loss: -128.59994506835938, G Loss: 1459.311767578125\n",
      "Epoch: 1837, D Loss: -59.281349182128906, G Loss: 716.5083618164062\n",
      "Epoch: 1838, D Loss: -83.38849639892578, G Loss: 426.79864501953125\n",
      "Epoch: 1839, D Loss: -91.33087158203125, G Loss: 294.417724609375\n",
      "Epoch: 1840, D Loss: -101.13719177246094, G Loss: 943.8111572265625\n",
      "Epoch: 1841, D Loss: -92.74259948730469, G Loss: 603.4058837890625\n",
      "Epoch: 1842, D Loss: -109.46328735351562, G Loss: 607.2410888671875\n",
      "Epoch: 1843, D Loss: -61.078880310058594, G Loss: 400.0250549316406\n",
      "Epoch: 1844, D Loss: -54.706756591796875, G Loss: 949.97265625\n",
      "Epoch: 1845, D Loss: -74.85848236083984, G Loss: 660.95654296875\n",
      "Epoch: 1846, D Loss: -162.8978271484375, G Loss: 963.1578369140625\n",
      "Epoch: 1847, D Loss: -197.31080627441406, G Loss: 412.85711669921875\n",
      "Epoch: 1848, D Loss: -562.2373046875, G Loss: 280.07861328125\n",
      "Epoch: 1849, D Loss: -1911.5927734375, G Loss: 540.6156616210938\n",
      "Epoch: 1850, D Loss: -4726.00146484375, G Loss: 462.60369873046875\n",
      "Epoch: 1851, D Loss: -4917.0185546875, G Loss: 1007.7635498046875\n",
      "Epoch: 1852, D Loss: -5956.74658203125, G Loss: 358.1058654785156\n",
      "Epoch: 1853, D Loss: -5478.98486328125, G Loss: 899.26708984375\n",
      "Epoch: 1854, D Loss: -2982.6298828125, G Loss: 597.4222412109375\n",
      "Epoch: 1855, D Loss: -6594.47802734375, G Loss: 251.19178771972656\n",
      "Epoch: 1856, D Loss: -7567.52490234375, G Loss: 767.9169311523438\n",
      "Epoch: 1857, D Loss: -10847.091796875, G Loss: 300.56451416015625\n",
      "Epoch: 1858, D Loss: -10234.986328125, G Loss: 1207.001953125\n",
      "Epoch: 1859, D Loss: -9295.01171875, G Loss: 918.710205078125\n",
      "Epoch: 1860, D Loss: 170.6579132080078, G Loss: 1039.107421875\n",
      "Epoch: 1861, D Loss: 61.89963912963867, G Loss: 541.791259765625\n",
      "Epoch: 1862, D Loss: 45.29909896850586, G Loss: 635.4896850585938\n",
      "Epoch: 1863, D Loss: -14.432551383972168, G Loss: 1058.2811279296875\n",
      "Epoch: 1864, D Loss: -8.39036750793457, G Loss: 588.7891845703125\n",
      "Epoch: 1865, D Loss: 69.16424560546875, G Loss: 299.04766845703125\n",
      "Epoch: 1866, D Loss: 146.72183227539062, G Loss: 716.075439453125\n",
      "Epoch: 1867, D Loss: 109.93447875976562, G Loss: 595.931884765625\n",
      "Epoch: 1868, D Loss: 77.14482879638672, G Loss: 579.7261962890625\n",
      "Epoch: 1869, D Loss: 63.39403533935547, G Loss: 3462.09912109375\n",
      "Epoch: 1870, D Loss: 1285.341796875, G Loss: 190.46063232421875\n",
      "Epoch: 1871, D Loss: 623.4287109375, G Loss: 618.08056640625\n",
      "Epoch: 1872, D Loss: 199.88699340820312, G Loss: 788.247802734375\n",
      "Epoch: 1873, D Loss: 132.43128967285156, G Loss: 2309.93896484375\n",
      "Epoch: 1874, D Loss: 45.4849853515625, G Loss: 606.155517578125\n",
      "Epoch: 1875, D Loss: -13.56797981262207, G Loss: 470.42401123046875\n",
      "Epoch: 1876, D Loss: -12.5073881149292, G Loss: 203.0841064453125\n",
      "Epoch: 1877, D Loss: -50.725067138671875, G Loss: 712.47607421875\n",
      "Epoch: 1878, D Loss: -59.89731979370117, G Loss: 486.0316467285156\n",
      "Epoch: 1879, D Loss: -37.29521942138672, G Loss: 815.2442016601562\n",
      "Epoch: 1880, D Loss: -23.1627254486084, G Loss: 1012.3056030273438\n",
      "Epoch: 1881, D Loss: -75.13902282714844, G Loss: 641.430908203125\n",
      "Epoch: 1882, D Loss: -36.79410934448242, G Loss: 1406.56103515625\n",
      "Epoch: 1883, D Loss: -62.07041549682617, G Loss: 686.0365600585938\n",
      "Epoch: 1884, D Loss: -44.00651931762695, G Loss: 854.4946899414062\n",
      "Epoch: 1885, D Loss: -78.72035217285156, G Loss: 732.8871459960938\n",
      "Epoch: 1886, D Loss: -79.3910140991211, G Loss: 646.2943115234375\n",
      "Epoch: 1887, D Loss: -49.15325164794922, G Loss: 823.814208984375\n",
      "Epoch: 1888, D Loss: -6.341742038726807, G Loss: 430.7140197753906\n",
      "Epoch: 1889, D Loss: 31.46824073791504, G Loss: 486.03375244140625\n",
      "Epoch: 1890, D Loss: 136.01358032226562, G Loss: 270.7095642089844\n",
      "Epoch: 1891, D Loss: 119.15583801269531, G Loss: 339.9177551269531\n",
      "Epoch: 1892, D Loss: 238.63970947265625, G Loss: 487.39501953125\n",
      "Epoch: 1893, D Loss: 119.48450469970703, G Loss: 227.03701782226562\n",
      "Epoch: 1894, D Loss: 229.5721893310547, G Loss: 414.67620849609375\n",
      "Epoch: 1895, D Loss: 97.49851989746094, G Loss: 937.4073486328125\n",
      "Epoch: 1896, D Loss: 60.74373245239258, G Loss: 1087.7567138671875\n",
      "Epoch: 1897, D Loss: -103.58023071289062, G Loss: 271.24884033203125\n",
      "Epoch: 1898, D Loss: -82.07572937011719, G Loss: 503.5582580566406\n",
      "Epoch: 1899, D Loss: -126.8630142211914, G Loss: 331.6231384277344\n",
      "Epoch: 1900, D Loss: -58.22871780395508, G Loss: 669.49853515625\n",
      "Epoch: 1901, D Loss: 9.825581550598145, G Loss: 685.531494140625\n",
      "Epoch: 1902, D Loss: 36.097877502441406, G Loss: 1312.1263427734375\n",
      "Epoch: 1903, D Loss: -21.50600814819336, G Loss: 579.4039916992188\n",
      "Epoch: 1904, D Loss: -8.335918426513672, G Loss: 476.24090576171875\n",
      "Epoch: 1905, D Loss: -11.240656852722168, G Loss: 1423.5673828125\n",
      "Epoch: 1906, D Loss: -8.884885787963867, G Loss: 1568.235107421875\n",
      "Epoch: 1907, D Loss: -92.13265228271484, G Loss: 1306.7735595703125\n",
      "Epoch: 1908, D Loss: -139.76405334472656, G Loss: 692.1719970703125\n",
      "Epoch: 1909, D Loss: -79.05635070800781, G Loss: 1002.03515625\n",
      "Epoch: 1910, D Loss: -54.282989501953125, G Loss: 1050.3046875\n",
      "Epoch: 1911, D Loss: -37.576820373535156, G Loss: 364.72174072265625\n",
      "Epoch: 1912, D Loss: -159.55194091796875, G Loss: 1144.6123046875\n",
      "Epoch: 1913, D Loss: -125.58100891113281, G Loss: 836.1550903320312\n",
      "Epoch: 1914, D Loss: -24.809062957763672, G Loss: 746.9007568359375\n",
      "Epoch: 1915, D Loss: -61.84423828125, G Loss: 713.09912109375\n",
      "Epoch: 1916, D Loss: -79.15747833251953, G Loss: 1015.4785766601562\n",
      "Epoch: 1917, D Loss: -39.24827194213867, G Loss: 1070.193115234375\n",
      "Epoch: 1918, D Loss: -17.85022735595703, G Loss: 912.3034057617188\n",
      "Epoch: 1919, D Loss: 103.86278533935547, G Loss: 955.0147705078125\n",
      "Epoch: 1920, D Loss: 41.25626754760742, G Loss: 1467.9617919921875\n",
      "Epoch: 1921, D Loss: 106.10802459716797, G Loss: 691.8718872070312\n",
      "Epoch: 1922, D Loss: 237.69044494628906, G Loss: 1745.3203125\n",
      "Epoch: 1923, D Loss: 166.519287109375, G Loss: 1922.953369140625\n",
      "Epoch: 1924, D Loss: 0.636932373046875, G Loss: 947.51806640625\n",
      "Epoch: 1925, D Loss: 9.28622817993164, G Loss: 878.483154296875\n",
      "Epoch: 1926, D Loss: 155.56053161621094, G Loss: 762.0272827148438\n",
      "Epoch: 1927, D Loss: 267.2257080078125, G Loss: 810.122802734375\n",
      "Epoch: 1928, D Loss: 48.08033752441406, G Loss: 811.0477294921875\n",
      "Epoch: 1929, D Loss: -44.957855224609375, G Loss: 460.1459045410156\n",
      "Epoch: 1930, D Loss: -109.8486557006836, G Loss: 1179.685302734375\n",
      "Epoch: 1931, D Loss: -115.94622802734375, G Loss: 430.7086181640625\n",
      "Epoch: 1932, D Loss: -207.4171142578125, G Loss: 408.4912414550781\n",
      "Epoch: 1933, D Loss: -865.8223266601562, G Loss: 436.80584716796875\n",
      "Epoch: 1934, D Loss: -4640.3046875, G Loss: 589.2418823242188\n",
      "Epoch: 1935, D Loss: -14968.3330078125, G Loss: 309.4271240234375\n",
      "Epoch: 1936, D Loss: -13137.171875, G Loss: 2790.054931640625\n",
      "Epoch: 1937, D Loss: -875.63671875, G Loss: 3068.3134765625\n",
      "Epoch: 1938, D Loss: -3511.69873046875, G Loss: 1464.162353515625\n",
      "Epoch: 1939, D Loss: -3403.243896484375, G Loss: 1813.718505859375\n",
      "Epoch: 1940, D Loss: -1462.5274658203125, G Loss: 1172.942138671875\n",
      "Epoch: 1941, D Loss: -3549.26220703125, G Loss: 625.1524658203125\n",
      "Epoch: 1942, D Loss: -2841.47998046875, G Loss: 1336.9404296875\n",
      "Epoch: 1943, D Loss: -4458.6552734375, G Loss: 1024.446044921875\n",
      "Epoch: 1944, D Loss: -5341.78857421875, G Loss: 1232.32470703125\n",
      "Epoch: 1945, D Loss: -5569.2353515625, G Loss: 1002.86328125\n",
      "Epoch: 1946, D Loss: -7846.5947265625, G Loss: 1384.12548828125\n",
      "Epoch: 1947, D Loss: -3910.07763671875, G Loss: 928.7553100585938\n",
      "Epoch: 1948, D Loss: -4276.9765625, G Loss: 815.9320068359375\n",
      "Epoch: 1949, D Loss: -7113.21435546875, G Loss: 1968.1873779296875\n",
      "Epoch: 1950, D Loss: -7268.49755859375, G Loss: 2241.73681640625\n",
      "Epoch: 1951, D Loss: -7227.181640625, G Loss: 3399.865966796875\n",
      "Epoch: 1952, D Loss: -7503.09130859375, G Loss: 818.8350830078125\n",
      "Epoch: 1953, D Loss: -11126.935546875, G Loss: 1423.1195068359375\n",
      "Epoch: 1954, D Loss: -8079.54052734375, G Loss: 3287.660888671875\n",
      "Epoch: 1955, D Loss: -8255.951171875, G Loss: 2492.031982421875\n",
      "Epoch: 1956, D Loss: -8322.248046875, G Loss: 857.375\n",
      "Epoch: 1957, D Loss: -11279.9296875, G Loss: 742.8953247070312\n",
      "Epoch: 1958, D Loss: -19890.015625, G Loss: 1250.65478515625\n",
      "Epoch: 1959, D Loss: -6390.46044921875, G Loss: 1146.3331298828125\n",
      "Epoch: 1960, D Loss: -6033.21435546875, G Loss: 1005.4249267578125\n",
      "Epoch: 1961, D Loss: -4219.0458984375, G Loss: 2176.780029296875\n",
      "Epoch: 1962, D Loss: -2064.3408203125, G Loss: 651.332275390625\n",
      "Epoch: 1963, D Loss: -2910.787109375, G Loss: 719.3173828125\n",
      "Epoch: 1964, D Loss: -6633.78125, G Loss: 372.41619873046875\n",
      "Epoch: 1965, D Loss: -7635.251953125, G Loss: 638.2258911132812\n",
      "Epoch: 1966, D Loss: -11956.2890625, G Loss: 484.3580322265625\n",
      "Epoch: 1967, D Loss: -9725.357421875, G Loss: 673.180908203125\n",
      "Epoch: 1968, D Loss: -15918.201171875, G Loss: 1919.0782470703125\n",
      "Epoch: 1969, D Loss: -15983.666015625, G Loss: 2240.88330078125\n",
      "Epoch: 1970, D Loss: 37483.28125, G Loss: 16834.083984375\n",
      "Epoch: 1971, D Loss: 13357.326171875, G Loss: 1513.5947265625\n",
      "Epoch: 1972, D Loss: 4998.17724609375, G Loss: 1422.89892578125\n",
      "Epoch: 1973, D Loss: 605.5303955078125, G Loss: 2447.08642578125\n",
      "Epoch: 1974, D Loss: -41.58081817626953, G Loss: 1473.5009765625\n",
      "Epoch: 1975, D Loss: -192.72079467773438, G Loss: 1887.15234375\n",
      "Epoch: 1976, D Loss: 1207.1287841796875, G Loss: 2232.34912109375\n",
      "Epoch: 1977, D Loss: 1590.757568359375, G Loss: 3620.948486328125\n",
      "Epoch: 1978, D Loss: 1687.257568359375, G Loss: 1937.910400390625\n",
      "Epoch: 1979, D Loss: 159.48245239257812, G Loss: 700.799072265625\n",
      "Epoch: 1980, D Loss: 13.432348251342773, G Loss: 1183.1439208984375\n",
      "Epoch: 1981, D Loss: 86.64720153808594, G Loss: 868.5879516601562\n",
      "Epoch: 1982, D Loss: 143.5017852783203, G Loss: 3576.60107421875\n",
      "Epoch: 1983, D Loss: 70.782958984375, G Loss: 2156.774169921875\n",
      "Epoch: 1984, D Loss: 42.070640563964844, G Loss: 889.3359375\n",
      "Epoch: 1985, D Loss: 164.1951904296875, G Loss: 350.176025390625\n",
      "Epoch: 1986, D Loss: 235.21334838867188, G Loss: 1205.596923828125\n",
      "Epoch: 1987, D Loss: 165.42184448242188, G Loss: 948.401611328125\n",
      "Epoch: 1988, D Loss: 115.25373840332031, G Loss: 407.7901611328125\n",
      "Epoch: 1989, D Loss: 420.6887512207031, G Loss: 692.0860595703125\n",
      "Epoch: 1990, D Loss: 258.2395324707031, G Loss: 22.52588653564453\n",
      "Epoch: 1991, D Loss: 649.2001342773438, G Loss: 245.9130859375\n",
      "Epoch: 1992, D Loss: 653.0073852539062, G Loss: 188.14212036132812\n",
      "Epoch: 1993, D Loss: 359.5444641113281, G Loss: 92.09188842773438\n",
      "Epoch: 1994, D Loss: 101.08169555664062, G Loss: 225.23703002929688\n",
      "Epoch: 1995, D Loss: 51.82234573364258, G Loss: 710.2780151367188\n",
      "Epoch: 1996, D Loss: 74.7657241821289, G Loss: 1007.6096801757812\n",
      "Epoch: 1997, D Loss: -17.1356201171875, G Loss: 498.2877197265625\n",
      "Epoch: 1998, D Loss: -31.16497802734375, G Loss: 428.73431396484375\n",
      "Epoch: 1999, D Loss: -0.31980133056640625, G Loss: 180.33929443359375\n",
      "Epoch: 2000, D Loss: -20.907146453857422, G Loss: 322.0834655761719\n"
     ]
    }
   ],
   "source": [
    "def train_gan(gan, generator, discriminator, dataset, z_dim, epochs=100):\n",
    "    for epoch in range(epochs):\n",
    "        for real_images in dataset:\n",
    "            # Điều chỉnh kích thước của z dựa trên kích thước thực tế của real_images\n",
    "            current_batch_size = real_images.shape[0]\n",
    "            z = np.random.normal(0, 1, (current_batch_size, z_dim))\n",
    "            fake_images = generator.predict(z, verbose=0)\n",
    "\n",
    "            combined_images = np.concatenate([real_images, fake_images])\n",
    "            labels = np.concatenate([np.ones((current_batch_size, 1)), np.zeros((current_batch_size, 1))])\n",
    "            labels += 0.05 * np.random.random(labels.shape)\n",
    "\n",
    "            d_loss = discriminator.train_on_batch(combined_images, labels)\n",
    "\n",
    "            z = np.random.normal(0, 1, (current_batch_size, z_dim))\n",
    "            fake_labels = np.ones((current_batch_size, 1))\n",
    "\n",
    "            g_loss = gan.train_on_batch(z, fake_labels)\n",
    "\n",
    "        print(f\"Epoch: {epoch + 1}, D Loss: {d_loss[0]}, G Loss: {g_loss}\")\n",
    "\n",
    "\n",
    "# Chia dataset thành các batch\n",
    "dataset = [phishing_data[i:i + batch_size] for i in range(0, len(phishing_data), batch_size)]\n",
    "if len(dataset[-1]) != batch_size:\n",
    "    dataset.pop()  # Xóa batch cuối nếu kích thước không phù hợp\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# Tái tạo kiến trúc mô hình\n",
    "generator = build_generator(z_dim)\n",
    "discriminator = build_discriminator()\n",
    "\n",
    "# Tải trọng số\n",
    "generator.load_weights('generator_weights_gan.h5')\n",
    "discriminator.load_weights('discriminator_weights_gan.h5')\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Khởi tạo và huấn luyện mô hình GAN\n",
    "train_gan(gan, generator, discriminator, dataset, z_dim, 2000)\n",
    "generator.save_weights('generator_weights_gan.h5')\n",
    "discriminator.save_weights('discriminator_weights_gan.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def showFeature(features):\n",
    "  image = features.reshape(16, 32)\n",
    "  #print(image.shape)\n",
    "  #print(image)\n",
    "  \n",
    "  plt.subplot(1, 1, 1)  # Tạo subplot\n",
    "  plt.imshow(image, cmap='viridis')  # Sử dụng viridis làm bảng màu\n",
    "  plt.title(f'Feature map')\n",
    "  plt.axis('off')  # Ẩn trục tọa độ\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ 0 =================\n",
      "Fake:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.86421928e+01  4.28393269e+00 -1.15601981e+00 -4.18474770e+00\n",
      "  1.93918800e+01  1.37141981e+01  5.06095171e+00  1.34288347e+00\n",
      "  2.99841309e+00 -4.58504629e+00 -6.77374411e+00  4.45740414e+00\n",
      "  1.13855734e+01 -1.05916023e+01 -2.54278159e+00  1.50603209e+01\n",
      " -9.29679298e+00  8.43993568e+00  1.16135054e+01  7.96134806e+00\n",
      "  1.07267218e+01  1.21418390e+01 -1.32016182e-01 -4.36124611e+00\n",
      " -7.50393724e+00  2.29630527e+01  6.25579357e+00  5.02898502e+00\n",
      "  4.73771954e+00  2.11078377e+01  7.03381681e+00 -1.40054059e+00\n",
      "  2.24091053e+00 -5.01370525e+00  6.69944620e+00  5.99116659e+00\n",
      "  4.35621619e-01 -1.21964045e+01 -1.25878811e+01  3.89297009e-01\n",
      " -3.97225797e-01 -9.35318279e+00  1.41841431e+01  3.07826447e+00\n",
      "  1.45000607e-01 -4.63916183e-01  2.99140949e+01  8.45682526e+00\n",
      " -8.16820145e+00  9.47336769e+00  8.62598801e+00  2.47212582e+01\n",
      "  2.95245667e+01 -2.26492500e+00  5.46562862e+00 -2.31887507e+00\n",
      "  8.20686054e+00  4.04632111e+01  1.49277830e+01 -4.53181458e+00\n",
      " -1.00625908e+00 -9.98798311e-01  1.71343594e+01 -7.04850554e-01\n",
      "  4.85648775e+00  9.64370632e+00  6.80917883e+00  1.38544617e+01\n",
      "  6.18447661e-01 -5.44125557e+00 -7.61850357e+00  2.67552567e+01\n",
      "  1.29060781e+00 -3.70508224e-01  1.23830242e+01  5.50072289e+00\n",
      "  3.44729257e+00  2.04995799e+00  3.77574992e+00  9.52166653e+00\n",
      "  6.56370735e+00 -3.98135376e+00  1.05534191e+01  3.82882786e+00\n",
      "  9.33039856e+00  2.29060006e+00  1.31228962e+01 -3.93667698e+00\n",
      "  1.92408772e+01 -5.46815276e-01  1.60789948e+01  5.73091459e+00\n",
      "  4.49937296e+00  5.82748985e+00  7.77834129e+00  3.91982841e+00\n",
      "  1.91671467e+01  3.12648535e-01  1.13987064e+00  7.75187445e+00\n",
      "  8.24544048e+00  6.27068901e+00 -5.63061523e+00  5.90142012e+00\n",
      "  6.75796509e+00  2.72345662e-01  2.69516158e+00  1.84017074e+00\n",
      " -1.66170597e-01  1.66810918e+00 -7.38163090e+00 -2.68348670e+00\n",
      "  8.38059330e+00  3.43887019e+00  4.21939754e+00  1.21557438e+00\n",
      "  3.28116155e+00  5.89493227e+00 -4.37084258e-01  1.12646141e+01\n",
      "  1.23427563e+01 -3.93885344e-01  4.89073181e+00 -3.33750033e+00\n",
      "  2.89377022e+01  1.15696974e+01 -8.79020500e+00  5.54753447e+00\n",
      "  8.69445038e+00  7.25506449e+00  7.91460037e+00  4.42187265e-02\n",
      "  1.63120022e+01  7.96196318e+00  1.58322811e-01 -1.39694414e+01\n",
      "  9.49225044e+00  1.29427366e+01  2.96123981e+01  2.14552999e+00\n",
      "  4.77252436e+00  8.14556718e-01  2.76211095e+00 -2.77381229e+00\n",
      " -2.97816539e+00 -6.59952545e+00  2.53729725e+01 -1.18057084e+00\n",
      "  4.25961256e+00  1.66445961e+01  7.14157152e+00 -1.04168568e+01\n",
      "  2.40628300e+01  2.08503199e+00  6.00862074e+00  1.47068090e+01\n",
      " -1.24374127e+00  1.20729742e+01 -7.47500706e+00  5.65747595e+00\n",
      " -5.08664846e+00  1.33343687e+01  9.99338055e+00  3.78924131e+00\n",
      " -4.63837051e+00  2.97817516e+00 -3.34645724e+00  1.69194908e+01\n",
      "  1.04546976e+01  6.21816111e+00  2.13539386e+00 -4.14959192e+00\n",
      "  1.42711058e+01 -3.14515471e+00  8.27629757e+00 -9.46208477e+00\n",
      "  1.11201057e+01 -2.02505374e+00  1.42813435e+01  2.37374020e+00\n",
      " -2.94307327e+00  9.60816741e-01  9.61511326e+00  1.24809275e+01\n",
      "  9.10842896e-01 -7.23364401e+00  5.93592405e+00  6.65837336e+00\n",
      "  1.14988050e+01 -5.29515648e+00  2.92626333e+00  2.11865616e+00\n",
      " -3.91991472e+00 -4.57225978e-01  2.30828724e+01  5.93042040e+00\n",
      "  1.24400492e+01 -6.69284153e+00 -9.33361530e+00 -2.41000056e+00\n",
      "  1.96321833e+00  1.08245125e+01 -5.14394474e+00  1.68964424e+01\n",
      "  3.74110413e+00 -2.77357340e-01 -3.07382494e-01 -7.92668641e-01\n",
      " -4.09923172e+00  3.00168724e+01  2.05515823e+01  3.27799058e+00\n",
      "  6.31099939e+00  5.22570801e+00  3.71851635e+00 -7.32606649e+00\n",
      " -1.06989613e+01  1.63674946e+01  1.47166615e+01  1.59331827e+01\n",
      "  1.75170290e+00  1.81914120e+01 -4.04830074e+00  1.72706718e+01\n",
      "  2.67834485e-01  1.71920514e+00  5.23481941e+00  1.48547621e+01\n",
      "  1.80273759e+00  2.50546837e+01  1.92451229e+01 -5.77218771e+00\n",
      "  6.83889961e+00  5.39518070e+00 -3.12887979e+00  3.48896718e+00\n",
      " -7.69441271e+00  8.41046333e+00  8.69301128e+00  5.76868820e+00\n",
      " -6.44470644e+00 -3.07972908e+00  1.84537220e+00  1.24300337e+01\n",
      "  2.65067978e+01 -1.15849686e+01  1.74596763e+00  4.74523497e+00\n",
      "  4.92014885e+00  3.53429139e-01  2.56353354e+00  1.88329220e+01\n",
      " -4.07943296e+00 -1.98932338e+00  1.52756274e-01  9.46829033e+00\n",
      "  4.72629976e+00  1.25006618e+01  1.61184616e+01 -6.07433558e+00\n",
      "  8.80280209e+00  1.75667167e+00  1.10707617e+01 -8.97351456e+00\n",
      " -4.37520123e+00  8.20046997e+00 -5.57593775e+00  2.31250401e+01\n",
      "  3.23323941e+00  5.44585139e-02  2.02525215e+01  9.59198093e+00\n",
      "  1.43690090e+01  2.40547299e+00 -3.10649157e+00  7.98614502e+00\n",
      "  7.09121704e+01  6.28036737e+00  7.37935543e+00 -1.47732658e+01\n",
      "  1.45510459e+00  2.91207981e+00  3.90253067e+01  3.50079989e+00\n",
      " -4.72078609e+00  2.42764807e+00 -4.78792572e+00 -4.75253153e+00\n",
      "  2.19111252e+01  5.00281990e-01 -1.07863605e+00  7.23988247e+00\n",
      "  1.30456638e+01  1.84092846e+01  2.44078083e+01 -2.45076466e+00\n",
      "  9.29137516e+00  4.27770662e+00  1.63695183e+01  5.23714781e+00\n",
      "  8.19884002e-01 -2.67454123e+00  1.15431976e+01 -5.48924923e-01\n",
      "  9.58572578e+00  2.42913208e+01  1.97153454e+01  7.18421412e+00\n",
      "  2.37855816e+00 -8.58843327e+00  3.53217077e+00 -2.15521169e+00\n",
      "  6.69989491e+00  1.53232634e+00  1.96063366e+01  3.62548804e+00\n",
      " -5.64484406e+00 -1.38668716e+00  2.63723540e+00  3.26937103e+00\n",
      "  4.55801845e-01  1.19901085e+01  3.77621555e+00  3.34875607e+00\n",
      "  1.18136644e+01  2.20992231e+00  1.01232862e+01 -3.62315869e+00\n",
      "  1.02413034e+01  8.22462845e+00  1.27116337e+01  1.44164772e+01\n",
      "  1.14675331e+01 -6.81908751e+00  1.07931795e+01  4.09626198e+00\n",
      " -5.73331881e+00  1.94254417e+01 -2.04286146e+00 -1.15247977e+00\n",
      "  3.39155108e-01  2.08279591e+01  1.32977982e+01 -2.24229932e+00\n",
      " -5.02294958e-01  5.32598639e+00 -2.74005198e+00  3.78171372e+00\n",
      " -1.95010436e+00 -1.04689512e+01  1.66503181e+01 -4.99979401e+00\n",
      "  9.86773372e-01 -5.70929289e+00 -2.29132366e+00  1.37457104e+01\n",
      "  8.68583965e+00 -1.78501070e+00 -9.23819923e+00  4.64575434e+00\n",
      "  7.63989258e+00 -4.36743164e+00 -1.61365356e+01  1.28990686e+00\n",
      " -4.43052387e+00  1.09445124e+01  8.80730724e+00  9.35144997e+00\n",
      " -6.03513002e+00  1.79882507e+01  5.82126999e+00  5.30717993e+00\n",
      "  7.24929142e+00  1.34620786e-01  1.64859700e+00 -4.63707733e+00\n",
      " -4.19610858e-01 -1.20792603e+00  1.14418087e+01  1.87230301e+01\n",
      "  1.52392330e+01 -4.11876583e+00  6.13134241e+00  8.29356098e+00\n",
      " -4.34373331e+00  6.95950365e+00  1.16831331e+01  1.08461964e+00\n",
      "  3.87380672e+00  6.55013990e+00  1.48408198e+00  2.49608593e+01\n",
      " -4.28329945e+00  3.57348990e+00  1.00226316e+01 -1.16582513e+00\n",
      " -6.88980865e+00  9.44622898e+00  2.72154808e+01  6.16459417e+00\n",
      " -5.63283348e+00  7.53331661e+00  3.03572297e+00  5.61349535e+00\n",
      " -3.97096610e+00  9.04191208e+00 -3.61972141e+00  2.42645302e+01\n",
      " -1.01159163e+01 -2.53155684e+00  1.15704346e+01  3.18235278e-01\n",
      "  2.07417431e+01  2.88996482e+00 -7.55983829e+00  9.19813728e+00\n",
      "  1.40934095e+01  1.36214676e+01 -1.33824692e+01  3.33577776e+00\n",
      "  9.20601654e+00  4.84272814e+00  5.43846226e+00 -1.90686107e+00\n",
      "  4.04638195e+00  3.96051519e-02  3.97383094e+00  2.62546754e+00\n",
      "  3.40076852e+00  1.15843844e+00 -5.20976162e+00  9.07508087e+00\n",
      "  2.46268177e+00  2.02093673e+00 -5.99540615e+00  2.02006550e+01\n",
      "  6.57635975e+00  1.51842225e+00  1.19824085e+01  9.05794907e+00\n",
      "  2.74685154e+01 -4.64202309e+00 -3.95486164e+00  1.86342716e+00\n",
      "  2.42481155e+01  7.24821091e+00 -5.37854338e+00 -1.45258017e+01\n",
      "  5.94240522e+00  7.79380131e+00  8.14578629e+00  1.88300838e+01\n",
      "  5.34924364e+00  8.25790310e+00 -1.25096483e+01  1.53690977e+01\n",
      "  1.52476015e+01  5.87208605e+00 -6.39235115e+00 -2.80233884e+00\n",
      "  1.22914925e+01  1.44170866e+01  1.31236048e+01 -7.24014759e+00\n",
      "  1.20176258e+01 -4.89900112e+00  1.95189923e-01 -7.89607334e+00\n",
      " -5.98884439e+00 -6.72722578e+00  1.61236076e+01 -1.95021331e+00\n",
      "  7.33729792e+00  1.72986641e+01  1.86807632e+01  1.93063474e+00\n",
      " -2.16306858e+01  8.79645061e+00 -8.41443241e-02 -2.93819845e-01\n",
      "  4.45340204e+00  4.91208649e+00  1.33113680e+01  1.54197550e+01\n",
      "  4.15948868e+00  1.60972633e+01  5.89134884e+00  7.79738092e+00\n",
      " -4.69033051e+00  8.45039082e+00  9.59583855e+00  5.91276598e+00\n",
      "  8.66126823e+00  2.18055115e+01  6.10891294e+00 -3.82198024e+00\n",
      "  5.86158991e+00 -1.59242183e-01  1.04945290e+00 -1.44920731e+01\n",
      " -1.03995667e+01 -2.12837458e+00  1.41494970e+01  1.14049301e+01\n",
      "  6.75045919e+00  8.05583191e+00 -7.88455391e+00  3.96288085e+00\n",
      "  1.37135515e+01  2.08355451e+00 -9.21766102e-01 -6.39660501e+00]\n",
      "[4.25514145e+01 0.00000000e+00 5.10038710e+00 0.00000000e+00\n",
      " 1.13169308e+01 9.81753445e+00 4.84032021e+01 3.05434418e+01\n",
      " 7.15155716e+01 0.00000000e+00 2.65068340e+00 0.00000000e+00\n",
      " 7.12394810e+00 0.00000000e+00 0.00000000e+00 1.33557449e+02\n",
      " 0.00000000e+00 3.06032562e+01 9.46832581e+01 1.06052608e+01\n",
      " 0.00000000e+00 0.00000000e+00 4.63431549e+01 3.58048286e+01\n",
      " 2.20995116e+00 2.12126980e+01 2.72438126e+01 7.41269684e+00\n",
      " 3.09811707e+01 2.93457246e+00 2.99856701e+01 0.00000000e+00\n",
      " 5.27476120e+01 4.66558075e+01 0.00000000e+00 4.37158203e+01\n",
      " 0.00000000e+00 0.00000000e+00 2.04267063e+01 0.00000000e+00\n",
      " 1.35901966e+01 3.56528015e+01 3.91430092e+01 1.71665344e+01\n",
      " 1.67061977e+01 2.42696667e+01 3.10326805e+01 4.54485397e+01\n",
      " 6.93117828e+01 1.07428551e+00 5.50691700e+00 3.87136650e+01\n",
      " 1.37301693e+01 4.06428223e+01 5.97300339e+00 4.09254341e+01\n",
      " 6.85179596e+01 1.30244583e+02 9.57965546e+01 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 2.97680664e+00\n",
      " 2.65893478e+01 4.45191050e+00 4.90629882e-01 4.72913837e+00\n",
      " 2.45379963e+01 0.00000000e+00 0.00000000e+00 4.50879250e+01\n",
      " 0.00000000e+00 1.61953888e+01 0.00000000e+00 5.37085991e+01\n",
      " 0.00000000e+00 2.84851589e+01 1.00610619e+01 2.37999344e+01\n",
      " 0.00000000e+00 0.00000000e+00 5.95825729e+01 7.75518570e+01\n",
      " 3.59994888e+00 4.39614487e+00 1.54876404e+02 0.00000000e+00\n",
      " 2.27681236e+01 6.55843887e+01 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 3.61450081e+01 3.39567413e+01 0.00000000e+00\n",
      " 1.53488169e+01 0.00000000e+00 0.00000000e+00 9.85809517e+00\n",
      " 0.00000000e+00 9.77010441e+00 0.00000000e+00 3.65662651e+01\n",
      " 0.00000000e+00 1.49162340e+01 0.00000000e+00 1.65093880e+01\n",
      " 0.00000000e+00 0.00000000e+00 1.71021290e+01 0.00000000e+00\n",
      " 0.00000000e+00 4.95141106e+01 0.00000000e+00 1.57680273e+01\n",
      " 6.51684418e+01 5.14306297e+01 3.57413025e+01 1.29257860e+01\n",
      " 1.02534354e+00 1.14736986e+01 0.00000000e+00 0.00000000e+00\n",
      " 2.32967682e+01 0.00000000e+00 0.00000000e+00 1.64969406e+01\n",
      " 8.16180515e+00 0.00000000e+00 2.81818905e+01 1.62060738e+01\n",
      " 0.00000000e+00 3.62525291e+01 1.12430725e+01 0.00000000e+00\n",
      " 5.63639717e+01 0.00000000e+00 3.33200874e+01 1.70616293e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 3.34756231e+00\n",
      " 0.00000000e+00 0.00000000e+00 2.42209377e+01 1.37253662e+02\n",
      " 7.54749870e+00 4.95688515e+01 6.32764482e+00 8.12551498e+00\n",
      " 5.80308800e+01 6.64355850e+01 0.00000000e+00 3.65580750e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 3.45749245e+01 1.08482876e+01\n",
      " 2.23910427e+01 5.15612259e+01 6.46025181e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 1.95237713e+01 2.07290096e+01 0.00000000e+00 1.88904972e+01\n",
      " 1.59097462e+01 0.00000000e+00 0.00000000e+00 1.24276695e+01\n",
      " 3.51387482e+01 1.19411631e+01 2.34215975e+00 1.41672611e+01\n",
      " 2.35972919e+01 4.15010490e+01 0.00000000e+00 0.00000000e+00\n",
      " 2.73229623e+00 0.00000000e+00 0.00000000e+00 7.29823971e+00\n",
      " 2.52920780e+01 4.93166618e+01 3.96676025e+01 0.00000000e+00\n",
      " 1.29462738e+01 6.01634026e+01 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 6.61733551e+01 0.00000000e+00 3.51447639e+01\n",
      " 2.78252926e+01 0.00000000e+00 2.74752655e+01 1.46699057e+01\n",
      " 0.00000000e+00 1.21718502e+01 8.57078781e+01 3.01336193e+01\n",
      " 1.16689281e+01 3.63099098e+01 0.00000000e+00 1.71856461e+01\n",
      " 0.00000000e+00 1.82747722e-01 1.17399902e+01 6.99379272e+01\n",
      " 4.13019638e+01 3.39258385e+01 4.39164276e+01 3.68118134e+01\n",
      " 6.54882660e+01 2.82260532e+01 3.16941261e+01 0.00000000e+00\n",
      " 1.10440643e+02 1.00060225e+01 2.26730232e+01 0.00000000e+00\n",
      " 9.21906738e+01 3.76009250e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 1.30224218e+01 1.58335054e+00 1.55472975e+01\n",
      " 3.36073112e+01 4.45961342e+01 9.01676407e+01 2.76119471e+00\n",
      " 0.00000000e+00 4.04124489e+01 8.64200497e+00 3.10694389e+01\n",
      " 5.12781143e+00 3.71914077e+00 3.14437466e+01 0.00000000e+00\n",
      " 2.88317127e+01 0.00000000e+00 1.09810982e+01 3.77828331e+01\n",
      " 1.83324070e+01 4.25527916e+01 7.21045456e+01 0.00000000e+00\n",
      " 2.55601794e-01 1.55847206e+01 3.67569809e+01 0.00000000e+00\n",
      " 0.00000000e+00 1.04769335e+01 5.72624626e+01 4.63792267e+01\n",
      " 0.00000000e+00 4.79906425e+01 3.55366135e+01 0.00000000e+00\n",
      " 6.54300308e+00 8.81869736e+01 0.00000000e+00 0.00000000e+00\n",
      " 2.40357315e+02 7.55960083e+01 6.31607399e+01 1.25220966e+01\n",
      " 0.00000000e+00 0.00000000e+00 8.25938644e+01 6.13436031e+00\n",
      " 0.00000000e+00 2.35716991e+01 0.00000000e+00 0.00000000e+00\n",
      " 1.70663605e+01 9.71893787e+00 4.96922445e+00 1.23861980e+01\n",
      " 0.00000000e+00 2.43589234e+00 2.22695675e+01 0.00000000e+00\n",
      " 3.09231205e+01 1.54028330e+01 5.52775307e+01 1.81151829e+01\n",
      " 1.22893791e+01 0.00000000e+00 3.33341599e+00 0.00000000e+00\n",
      " 0.00000000e+00 4.01084480e+01 1.52291229e+02 5.97971964e+00\n",
      " 0.00000000e+00 2.75655632e+01 3.28519249e+01 3.52015572e+01\n",
      " 2.41252804e+01 0.00000000e+00 1.31582146e+01 4.43391266e+01\n",
      " 4.82803679e+00 0.00000000e+00 0.00000000e+00 1.06401081e+01\n",
      " 4.02240868e+01 9.27472973e+00 1.12101479e+01 2.99061508e+01\n",
      " 3.22775650e+01 0.00000000e+00 4.16931496e+01 6.42538881e+00\n",
      " 2.63145332e+01 0.00000000e+00 0.00000000e+00 4.05083923e+01\n",
      " 2.38842144e+01 0.00000000e+00 4.56723366e+01 3.73965187e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 2.28948345e+01\n",
      " 6.60791302e+00 1.02389565e+01 8.68584671e+01 7.24165583e+00\n",
      " 1.67668724e+01 2.09716034e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 2.76012731e+00 4.32433281e+01 0.00000000e+00\n",
      " 4.76756210e+01 4.41266670e+01 3.95329170e+01 6.04678392e-02\n",
      " 6.70309877e+00 5.37510633e+00 0.00000000e+00 1.68608761e+01\n",
      " 1.33166590e+01 2.92001939e+00 1.52341480e+01 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 1.73665146e+02 3.59749794e+01\n",
      " 2.26747475e+01 0.00000000e+00 0.00000000e+00 4.06105766e+01\n",
      " 2.80028648e+01 2.03515186e+01 2.06771011e+01 1.32880840e+01\n",
      " 1.26428003e+01 3.16701870e+01 5.99101028e+01 9.03505096e+01\n",
      " 7.59235535e+01 4.99874535e+01 8.01329727e+01 1.57331009e+01\n",
      " 1.22909012e+01 1.97172439e+00 3.26495624e+00 0.00000000e+00\n",
      " 6.61597214e+01 5.76749268e+01 1.73035774e+01 6.37468719e+01\n",
      " 0.00000000e+00 0.00000000e+00 4.65117544e-01 3.18253574e+01\n",
      " 0.00000000e+00 4.31534538e+01 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 3.83001938e+01 4.62528610e+00 0.00000000e+00\n",
      " 0.00000000e+00 2.79460406e+00 0.00000000e+00 0.00000000e+00\n",
      " 3.06738243e+01 0.00000000e+00 0.00000000e+00 2.02635727e+01\n",
      " 4.30746841e+01 0.00000000e+00 2.85981045e+01 4.77046890e+01\n",
      " 2.54028244e+01 3.16162071e+01 6.98195028e+00 4.94363022e+01\n",
      " 2.69973259e+01 6.73862123e+00 0.00000000e+00 0.00000000e+00\n",
      " 8.00393600e+01 0.00000000e+00 0.00000000e+00 2.51123333e+01\n",
      " 2.82525883e+01 3.52519488e+00 1.85401306e+01 1.54287672e+01\n",
      " 1.90001640e+01 2.99222779e+00 3.18448982e+01 1.06883659e+02\n",
      " 4.71807861e+01 0.00000000e+00 7.37759018e+01 7.95407677e+00\n",
      " 0.00000000e+00 2.87931709e+01 7.89145584e+01 0.00000000e+00\n",
      " 2.98119297e+01 1.07418451e+01 0.00000000e+00 2.96734219e+01\n",
      " 9.92761688e+01 5.82973900e+01 6.80800962e+00 1.42164412e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 4.10831642e+01\n",
      " 8.42478485e+01 1.72484169e+01 0.00000000e+00 2.80905132e+01\n",
      " 2.45975971e+00 3.12460003e+01 4.37762947e+01 3.00077400e+01\n",
      " 2.63734531e+01 6.28620529e+01 0.00000000e+00 4.07742691e+01\n",
      " 2.39383183e+01 1.67522926e+01 8.45459061e+01 3.47993202e+01\n",
      " 0.00000000e+00 0.00000000e+00 1.71434441e+01 6.87777405e+01\n",
      " 0.00000000e+00 1.45697427e+00 2.38551064e+01 0.00000000e+00\n",
      " 1.02114159e+02 2.55723810e+00 4.24750481e+01 4.60598373e+01\n",
      " 5.02587938e+00 3.38494444e+00 6.13735886e+01 3.19068203e+01\n",
      " 2.66105976e+01 1.88814735e+01 0.00000000e+00 2.30563730e-01\n",
      " 1.69430542e+02 5.69717598e+01 4.34869881e+01 5.87481022e+00\n",
      " 1.81125793e+01 2.02704525e+01 4.78023720e+01 1.68393898e+01\n",
      " 6.53973103e+00 1.05791349e+01 4.42379904e+00 5.08464661e+01\n",
      " 0.00000000e+00 0.00000000e+00 5.28258896e+01 6.21490622e+00\n",
      " 9.23654785e+01 2.15387039e+01 2.17008057e+01 3.24510956e+01]\n"
     ]
    }
   ],
   "source": [
    "z = np.random.normal(0, 1, (10, z_dim))\n",
    "fake_images = generator.predict(z, verbose=0)\n",
    "for i in range(1):\n",
    "  print(\"================\", i, \"=================\")\n",
    "  print(\"Fake:\")\n",
    "  showFeature(fake_images[i])\n",
    "  print(\"Real:\")\n",
    "  showFeature(dataset[0][i])\n",
    "  print(fake_images[i])\n",
    "  print(dataset[0][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "@tf.function\n",
    "def train_step(images, z_dim, discriminator, generator, discriminator_optimizer, generator_optimizer):\n",
    "    # Tạo nhiễu ngẫu nhiên\n",
    "    noise = tf.random.normal([batch_size, z_dim])\n",
    "\n",
    "    # GradientTape cho discriminator\n",
    "    with tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(noise, training=True)\n",
    "\n",
    "        real_output = discriminator(images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "    # GradientTape cho generator\n",
    "    with tf.GradientTape() as gen_tape:\n",
    "        generated_images = generator(noise, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_model():\n",
    "    DNN_model = Sequential([\n",
    "        Dense(1024, activation='relu', input_shape=(512,)),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return DNN_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client  0 : \n",
      "956 / 7490\n",
      "956 / 7490\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'z_dim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/haohao/Desktop/Hieu/DNN_with_GAN_And_FL_SameDataRatio_without_TFF.ipynb Cell 17\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/haohao/Desktop/Hieu/DNN_with_GAN_And_FL_SameDataRatio_without_TFF.ipynb#X22sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(X_client_phishing), \u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mlen\u001b[39m(X_client_benign))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/haohao/Desktop/Hieu/DNN_with_GAN_And_FL_SameDataRatio_without_TFF.ipynb#X22sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(y_client_phishing), \u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mlen\u001b[39m(y_client_benign))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/haohao/Desktop/Hieu/DNN_with_GAN_And_FL_SameDataRatio_without_TFF.ipynb#X22sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m z \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mnormal(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, (\u001b[39m1\u001b[39m, z_dim))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/haohao/Desktop/Hieu/DNN_with_GAN_And_FL_SameDataRatio_without_TFF.ipynb#X22sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m gen_img \u001b[39m=\u001b[39m generator\u001b[39m.\u001b[39mpredict(z, verbose\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/haohao/Desktop/Hieu/DNN_with_GAN_And_FL_SameDataRatio_without_TFF.ipynb#X22sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m X_client_phishing \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mvstack((X_client_phishing, gen_img[\u001b[39m0\u001b[39m]))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'z_dim' is not defined"
     ]
    }
   ],
   "source": [
    "# Chia dữ liệu thành 10 phần\n",
    "\n",
    "federated_data = []\n",
    "\n",
    "# Giả sử X, y là dữ liệu của bạn\n",
    "num_clients = 1\n",
    "\n",
    "num_samples_phishing_per_client = len(phishing_train) // num_clients\n",
    "num_samples_benign_per_client = len(benign_train) // num_clients\n",
    "\n",
    "for i in range(num_clients):\n",
    "    start_idx = i * num_samples_phishing_per_client\n",
    "    end_idx = (i + 1) * num_samples_phishing_per_client if i != num_clients - 1 else len(phishing_train)\n",
    "    \n",
    "    start_idx_benign = i * num_samples_benign_per_client\n",
    "    end_idx_benign = (i + 1) * num_samples_benign_per_client if i != num_clients - 1 else len(benign_train)\n",
    "\n",
    "    X_client_phishing = phishing_train[start_idx:end_idx]\n",
    "    y_client_phishing = phishing_labels_train[start_idx:end_idx]\n",
    "\n",
    "    X_client_benign = benign_train[start_idx_benign:end_idx_benign]\n",
    "    y_client_benign = benign_labels_train[start_idx_benign:end_idx_benign]\n",
    "\n",
    "    print(\"Client \", i, \": \")\n",
    "    while (len(X_client_phishing) < len(X_client_benign)):\n",
    "        print(len(X_client_phishing), \"/\", len(X_client_benign))\n",
    "\n",
    "        print(len(y_client_phishing), \"/\", len(y_client_benign))\n",
    "        z = np.random.normal(0, 1, (1, z_dim))\n",
    "        gen_img = generator.predict(z, verbose=0)\n",
    "\n",
    "        X_client_phishing = np.vstack((X_client_phishing, gen_img[0]))\n",
    "        y_client_phishing = np.append(y_client_phishing, 1)\n",
    "\n",
    "    X_client = np.concatenate((X_client_phishing, X_client_benign), axis=0)\n",
    "    y_client = np.concatenate((y_client_phishing, y_client_benign), axis=0)\n",
    "    print(X_client.shape)\n",
    "    print(y_client.shape)\n",
    "    federated_data.append((X_client, y_client))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('federated_data.pkl', 'wb') as f:\n",
    "    pickle.dump(federated_data, f)\n",
    "np.savez('test_data.npz', testX=test_data, testy=test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loaded_arrays = np.load('test_data.npz')\n",
    "\n",
    "# Accessing the arrays\n",
    "test_data = loaded_arrays['testX']\n",
    "test_labels = loaded_arrays['testy']\n",
    "\n",
    "with open('federated_data.pkl', 'rb') as f:\n",
    "    federated_data = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(X_data, y_data):\n",
    "    # Chuyển đổi sang TensorFlow Dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X_data, y_data))\n",
    "    dataset = dataset.batch(64) # BatchSize\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "federated_data_train = [preprocessing(X_client, y_client) for X_client, y_client in federated_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_weights_changed(old_weights, new_weights, tolerance=1e-5):\n",
    "    \"\"\"\n",
    "    Kiểm tra xem trọng số của mô hình có thay đổi giữa các vòng đào tạo hay không.\n",
    "    :param old_weights: Trọng số của mô hình từ vòng trước.\n",
    "    :param new_weights: Trọng số mới của mô hình.\n",
    "    :param tolerance: Ngưỡng dung sai cho sự thay đổi.\n",
    "    :return: True nếu có sự thay đổi, ngược lại False.\n",
    "    \"\"\"\n",
    "    for old_layer, new_layer in zip(old_weights, new_weights):\n",
    "        if np.any(np.abs(new_layer - old_layer) > tolerance):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2112\n"
     ]
    }
   ],
   "source": [
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo tập dữ liệu từ test_data và test_labels\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_data, test_labels))\n",
    "\n",
    "# Chia thành các batch (ví dụ: batch_size=32)\n",
    "test_dataset = test_dataset.batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "235/235 [==============================] - 6s 18ms/step - loss: 2.7708 - accuracy: 0.9874 - val_loss: 112.2649 - val_accuracy: 0.8868\n",
      "Epoch 2/1000\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 7.1829 - accuracy: 0.9745 - val_loss: 7.3056 - val_accuracy: 0.8868\n",
      "Epoch 3/1000\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 7.2921 - accuracy: 0.9489 - val_loss: 19.4493 - val_accuracy: 0.8868\n",
      "Epoch 4/1000\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 2.1670 - accuracy: 0.9422 - val_loss: 14.5480 - val_accuracy: 0.8868\n",
      "Epoch 5/1000\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 3.0029 - accuracy: 0.9192 - val_loss: 6.1184 - val_accuracy: 0.8868\n",
      "Epoch 6/1000\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 0.7787 - accuracy: 0.9315 - val_loss: 2.1150 - val_accuracy: 0.8868\n",
      "Epoch 7/1000\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 0.6721 - accuracy: 0.9215 - val_loss: 3.1534 - val_accuracy: 0.8868\n",
      "Epoch 8/1000\n",
      "235/235 [==============================] - 4s 18ms/step - loss: 0.4521 - accuracy: 0.9368 - val_loss: 3.3369 - val_accuracy: 0.8868\n",
      "Epoch 9/1000\n",
      "235/235 [==============================] - 4s 18ms/step - loss: 0.4484 - accuracy: 0.9336 - val_loss: 2.1415 - val_accuracy: 0.8868\n",
      "Epoch 10/1000\n",
      "235/235 [==============================] - 4s 18ms/step - loss: 0.5110 - accuracy: 0.9605 - val_loss: 0.5363 - val_accuracy: 0.8868\n",
      "Epoch 11/1000\n",
      "235/235 [==============================] - 5s 19ms/step - loss: 0.2842 - accuracy: 0.9262 - val_loss: 5.2498 - val_accuracy: 0.8868\n",
      "Epoch 12/1000\n",
      "235/235 [==============================] - 5s 19ms/step - loss: 0.6187 - accuracy: 0.9750 - val_loss: 0.5203 - val_accuracy: 0.8868\n",
      "Epoch 13/1000\n",
      "235/235 [==============================] - 4s 19ms/step - loss: 0.2130 - accuracy: 0.9356 - val_loss: 4.1317 - val_accuracy: 0.8868\n",
      "Epoch 14/1000\n",
      "235/235 [==============================] - 4s 18ms/step - loss: 1.1878 - accuracy: 0.9262 - val_loss: 2.4440 - val_accuracy: 0.8868\n",
      "Epoch 15/1000\n",
      "235/235 [==============================] - 4s 18ms/step - loss: 0.3101 - accuracy: 0.9629 - val_loss: 2.2164 - val_accuracy: 0.8868\n",
      "Epoch 16/1000\n",
      "235/235 [==============================] - 4s 19ms/step - loss: 0.4372 - accuracy: 0.9359 - val_loss: 0.4499 - val_accuracy: 0.8868\n",
      "Epoch 17/1000\n",
      "235/235 [==============================] - 4s 18ms/step - loss: 0.1932 - accuracy: 0.9360 - val_loss: 5.0078 - val_accuracy: 0.8868\n",
      "Epoch 18/1000\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 0.5894 - accuracy: 0.9363 - val_loss: 2.4400 - val_accuracy: 0.8868\n",
      "Epoch 19/1000\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 0.4987 - accuracy: 0.9320 - val_loss: 0.4517 - val_accuracy: 0.8873\n",
      "Epoch 20/1000\n",
      "235/235 [==============================] - 4s 18ms/step - loss: 0.2502 - accuracy: 0.9360 - val_loss: 0.4325 - val_accuracy: 0.8873\n",
      "Epoch 21/1000\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 0.2419 - accuracy: 0.9361 - val_loss: 0.4188 - val_accuracy: 0.8873\n",
      "Epoch 22/1000\n",
      "235/235 [==============================] - 4s 18ms/step - loss: 0.2890 - accuracy: 0.9283 - val_loss: 0.4069 - val_accuracy: 0.8873\n",
      "Epoch 23/1000\n",
      "235/235 [==============================] - 4s 18ms/step - loss: 0.1465 - accuracy: 0.9333 - val_loss: 2.6670 - val_accuracy: 0.8868\n",
      "Epoch 24/1000\n",
      "235/235 [==============================] - 4s 18ms/step - loss: 0.3569 - accuracy: 0.9762 - val_loss: 0.5667 - val_accuracy: 0.8868\n",
      "Epoch 25/1000\n",
      "235/235 [==============================] - 4s 18ms/step - loss: 0.4868 - accuracy: 0.9571 - val_loss: 0.3847 - val_accuracy: 0.8873\n",
      "Epoch 26/1000\n",
      "235/235 [==============================] - 4s 18ms/step - loss: 0.2166 - accuracy: 0.9362 - val_loss: 0.3797 - val_accuracy: 0.8873\n",
      "Epoch 27/1000\n",
      "235/235 [==============================] - 4s 18ms/step - loss: 0.2139 - accuracy: 0.9362 - val_loss: 0.3755 - val_accuracy: 0.8873\n",
      "Epoch 28/1000\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 0.2117 - accuracy: 0.9363 - val_loss: 0.3720 - val_accuracy: 0.8873\n",
      "Epoch 29/1000\n",
      "235/235 [==============================] - 4s 18ms/step - loss: 0.2144 - accuracy: 0.9324 - val_loss: 0.3689 - val_accuracy: 0.8873\n",
      "Epoch 30/1000\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 0.2083 - accuracy: 0.9363 - val_loss: 0.3664 - val_accuracy: 0.8873\n",
      "Epoch 31/1000\n",
      "235/235 [==============================] - 4s 18ms/step - loss: 1.6923 - accuracy: 0.9220 - val_loss: 5.9929 - val_accuracy: 0.8868\n",
      "Epoch 32/1000\n",
      "235/235 [==============================] - 5s 19ms/step - loss: 1.9723 - accuracy: 0.9342 - val_loss: 2.5597 - val_accuracy: 0.8868\n",
      "Epoch 33/1000\n",
      "235/235 [==============================] - 5s 19ms/step - loss: 1.3735 - accuracy: 0.9057 - val_loss: 7.5272 - val_accuracy: 0.8868\n",
      "Epoch 34/1000\n",
      "235/235 [==============================] - 4s 19ms/step - loss: 0.7131 - accuracy: 0.9362 - val_loss: 4.1539 - val_accuracy: 0.8868\n",
      "Epoch 35/1000\n",
      "235/235 [==============================] - 5s 19ms/step - loss: 0.5744 - accuracy: 0.9362 - val_loss: 0.3646 - val_accuracy: 0.8873\n",
      "Epoch 36/1000\n",
      "235/235 [==============================] - 4s 19ms/step - loss: 0.2061 - accuracy: 0.9363 - val_loss: 0.3626 - val_accuracy: 0.8873\n",
      "Epoch 37/1000\n",
      "235/235 [==============================] - 5s 19ms/step - loss: 0.2054 - accuracy: 0.9360 - val_loss: 0.3610 - val_accuracy: 0.8873\n",
      "Epoch 38/1000\n",
      "235/235 [==============================] - 4s 18ms/step - loss: 0.2042 - accuracy: 0.9363 - val_loss: 0.3595 - val_accuracy: 0.8873\n",
      "Epoch 39/1000\n",
      "235/235 [==============================] - 4s 18ms/step - loss: 2.9830 - accuracy: 0.9148 - val_loss: 1.4960 - val_accuracy: 0.8868\n",
      "Epoch 40/1000\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 0.2408 - accuracy: 0.9361 - val_loss: 4.6440 - val_accuracy: 0.8868\n",
      "Epoch 41/1000\n",
      "235/235 [==============================] - 4s 18ms/step - loss: 0.7944 - accuracy: 0.9471 - val_loss: 0.3594 - val_accuracy: 0.8873\n",
      "Epoch 42/1000\n",
      "235/235 [==============================] - 4s 18ms/step - loss: 0.2034 - accuracy: 0.9362 - val_loss: 0.3581 - val_accuracy: 0.8873\n",
      "Epoch 43/1000\n",
      "235/235 [==============================] - 4s 18ms/step - loss: 0.2026 - accuracy: 0.9364 - val_loss: 0.3571 - val_accuracy: 0.8873\n",
      "Epoch 44/1000\n",
      "235/235 [==============================] - 4s 19ms/step - loss: 0.2021 - accuracy: 0.9364 - val_loss: 0.3563 - val_accuracy: 0.8873\n",
      "Epoch 45/1000\n",
      "235/235 [==============================] - 4s 18ms/step - loss: 0.2023 - accuracy: 0.9360 - val_loss: 0.3556 - val_accuracy: 0.8873\n",
      "Epoch 46/1000\n",
      "235/235 [==============================] - 4s 18ms/step - loss: 0.2017 - accuracy: 0.9361 - val_loss: 0.3549 - val_accuracy: 0.8873\n",
      "Epoch 47/1000\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 0.2012 - accuracy: 0.9362 - val_loss: 0.3543 - val_accuracy: 0.8873\n",
      "Epoch 48/1000\n",
      "235/235 [==============================] - 4s 19ms/step - loss: 0.2005 - accuracy: 0.9365 - val_loss: 0.3545 - val_accuracy: 0.8873\n",
      "Epoch 49/1000\n",
      "235/235 [==============================] - 4s 18ms/step - loss: 0.2005 - accuracy: 0.9364 - val_loss: 0.3535 - val_accuracy: 0.8873\n",
      "Epoch 50/1000\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 0.1997 - accuracy: 0.9367 - val_loss: 0.3528 - val_accuracy: 0.8873\n",
      "Epoch 51/1000\n",
      "235/235 [==============================] - 4s 18ms/step - loss: 0.1995 - accuracy: 0.9368 - val_loss: 0.3528 - val_accuracy: 0.8873\n",
      "Epoch 52/1000\n",
      "235/235 [==============================] - 4s 18ms/step - loss: 0.1990 - accuracy: 0.9370 - val_loss: 0.3530 - val_accuracy: 0.8873\n",
      "Epoch 53/1000\n",
      "235/235 [==============================] - 4s 18ms/step - loss: 0.1989 - accuracy: 0.9369 - val_loss: 0.3519 - val_accuracy: 0.8878\n",
      "Epoch 54/1000\n",
      "235/235 [==============================] - 5s 19ms/step - loss: 0.1992 - accuracy: 0.9369 - val_loss: 0.3516 - val_accuracy: 0.8878\n",
      "Epoch 55/1000\n",
      "235/235 [==============================] - 4s 19ms/step - loss: 0.2004 - accuracy: 0.9362 - val_loss: 0.3513 - val_accuracy: 0.8878\n",
      "Epoch 56/1000\n",
      "235/235 [==============================] - 4s 19ms/step - loss: 0.1673 - accuracy: 0.9294 - val_loss: 3.6972 - val_accuracy: 0.8868\n",
      "Epoch 57/1000\n",
      "235/235 [==============================] - 4s 19ms/step - loss: 0.6066 - accuracy: 0.9332 - val_loss: 17.2274 - val_accuracy: 0.8868\n",
      "Epoch 58/1000\n",
      "235/235 [==============================] - 4s 19ms/step - loss: 2.9212 - accuracy: 0.9142 - val_loss: 3.7742 - val_accuracy: 0.8873\n",
      "Epoch 59/1000\n",
      "235/235 [==============================] - 4s 18ms/step - loss: 0.5239 - accuracy: 0.9367 - val_loss: 0.3523 - val_accuracy: 0.8873\n",
      "Epoch 60/1000\n",
      "235/235 [==============================] - 4s 18ms/step - loss: 0.2031 - accuracy: 0.9368 - val_loss: 0.3520 - val_accuracy: 0.8873\n",
      "Epoch 61/1000\n",
      "235/235 [==============================] - 4s 18ms/step - loss: 0.2008 - accuracy: 0.9369 - val_loss: 0.3520 - val_accuracy: 0.8873\n",
      "Epoch 62/1000\n",
      "235/235 [==============================] - 4s 18ms/step - loss: 0.2003 - accuracy: 0.9370 - val_loss: 0.3520 - val_accuracy: 0.8873\n",
      "Epoch 63/1000\n",
      "235/235 [==============================] - 4s 18ms/step - loss: 0.2000 - accuracy: 0.9370 - val_loss: 0.3520 - val_accuracy: 0.8873\n",
      "Epoch 64/1000\n",
      "235/235 [==============================] - 4s 18ms/step - loss: 0.1998 - accuracy: 0.9371 - val_loss: 0.3517 - val_accuracy: 0.8873\n",
      "Epoch 65/1000\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.1997 - accuracy: 0.9370Restoring model weights from the end of the best epoch: 55.\n",
      "235/235 [==============================] - 4s 18ms/step - loss: 0.1993 - accuracy: 0.9372 - val_loss: 0.3516 - val_accuracy: 0.8873\n",
      "Epoch 65: early stopping\n",
      "Số lượng epochs đã thực hiện: 65\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Tạo một callback EarlyStopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',   # Giám sát mất mát của tập kiểm định\n",
    "    min_delta=0.00000001,      # Thay đổi tối thiểu để coi là cải thiện\n",
    "    patience=10,          # Số lượng epochs không cải thiện trước khi dừng\n",
    "    verbose=1,            # Hiển thị thông báo khi dừng\n",
    "    mode='min',           # Dừng quá trình huấn luyện khi giá trị 'val_loss' không giảm\n",
    "    restore_best_weights=True # Khôi phục trọng số tốt nhất khi kết thúc\n",
    ")\n",
    "\n",
    "# Giả sử 'model' là mô hình của bạn và bạn đã chuẩn bị dữ liệu\n",
    "# model = ...\n",
    "\n",
    "# Huấn luyện mô hình với dữ liệu và callback EarlyStopping\n",
    "history = model.fit(\n",
    "    federated_data_train[0],\n",
    "    validation_data=test_dataset,\n",
    "    epochs=1000,          # Số lượng epochs tối đa\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "# In số lượng epochs đã thực hiện\n",
    "print(\"Số lượng epochs đã thực hiện:\", len(history.history['loss']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/33 [========================>.....] - ETA: 0s - loss: 5.1750 - accuracy: 0.8666 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 0s 8ms/step - loss: 4.3909 - accuracy: 0.8868\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4.390923500061035, 0.8868371248245239]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_dataset)\n",
    "# test_data => wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, roc_auc_score\n",
    "\n",
    "def evaluate_model(model, test_dataset, loss_fn):\n",
    "    # Đánh giá mô hình trên tập dữ liệu kiểm tra\n",
    "    test_loss, test_accuracy = model.evaluate(test_dataset, verbose=0)\n",
    "\n",
    "    # Dự đoán nhãn và tính toán các chỉ số\n",
    "    y_true = np.concatenate([y_batch.numpy().flatten() for _, y_batch in test_dataset])\n",
    "    y_pred = np.concatenate([model.predict(x_batch, verbose=0).flatten() for x_batch, _ in test_dataset])\n",
    "\n",
    "    y_pred_rounded = np.array([1 if y > 0.5 else 0 for y in y_pred])\n",
    "\n",
    "    print(len(y_true))\n",
    "    print(len(y_pred))\n",
    "    cnt0 = 0\n",
    "    cnt1 = 0\n",
    "    for y in y_pred_rounded:\n",
    "        if y == 0:\n",
    "            cnt0 += 1\n",
    "        else:\n",
    "            cnt1 += 1\n",
    "    print(cnt0)\n",
    "    print(cnt1)\n",
    "\n",
    "    # Tính toán các chỉ số đánh giá\n",
    "    f1 = f1_score(y_true, y_pred_rounded)\n",
    "    recall = recall_score(y_true, y_pred_rounded)\n",
    "    precision = precision_score(y_true, y_pred_rounded)\n",
    "    auc = roc_auc_score(y_true, y_pred_rounded)\n",
    "\n",
    "    return test_loss, test_accuracy, f1, recall, precision, auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2112\n",
      "2112\n",
      "2110\n",
      "2\n",
      "Test Loss: 0.3512907326221466\n",
      "Test Accuracy: 0.8877840638160706\n",
      "Test F1: 0.016597510373443983\n",
      "Test Recall: 0.008368200836820083\n",
      "Test Precision: 1.0\n",
      "Test Auc: 0.50418410041841\n"
     ]
    }
   ],
   "source": [
    "loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "test_loss, test_accuracy, f1, recall, precision, auc = evaluate_model(model, test_dataset, loss_fn)\n",
    "\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "print(f\"Test F1: {f1}\")\n",
    "print(f\"Test Recall: {recall}\")\n",
    "print(f\"Test Precision: {precision}\")\n",
    "print(f\"Test Auc: {auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/////////////////////////////ROUND  1 ///////////////////////////\n",
      "e: 0\n",
      "Loss: 0.0, Accuracy: 1.0\n",
      "Client i weight thay doi\n",
      "2112\n",
      "2112\n",
      "Test Loss: 20.750492095947266\n",
      "Test Accuracy: 0.8868371248245239\n",
      "Test F1: 0.0\n",
      "Test Recall: 0.0\n",
      "Test Precision: 0.0\n",
      "Test Auc: 0.5\n",
      "Trọng số thay đổi sau vòng 1\n",
      "res\n",
      "24/33 [====================>.........] - ETA: 0s - loss: 28.5319 - accuracy: 0.8444   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haohao/miniconda3/envs/tf/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 0s 7ms/step - loss: 20.7505 - accuracy: 0.8868\n",
      "33/33 [==============================] - 0s 6ms/step\n",
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def train_model_on_client(model, client_dataset, epochs=1):\n",
    "    for epoch in range(epochs):\n",
    "        print(\"e:\",epoch)\n",
    "        for step, (x_batch_train, y_batch_train) in enumerate(client_dataset):\n",
    "            loss, accuracy = model.train_on_batch(x_batch_train, y_batch_train)\n",
    "        print(f'Loss: {loss}, Accuracy: {accuracy}')\n",
    "    \n",
    "    return model.get_weights(), loss, accuracy\n",
    "    \n",
    "def aggregate_weights(client_weights):\n",
    "    new_weights = [np.mean([client_weights[i][layer] for i in range(len(client_weights))], axis=0) for layer in range(len(client_weights[0]))]\n",
    "    return new_weights\n",
    "\n",
    "# Training loop\n",
    "global_model = create_model()\n",
    "\n",
    "global_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "previous_weights = global_model.get_weights()\n",
    "\n",
    "for round_num in range(1):\n",
    "    print(\"/////////////////////////////ROUND \", round_num + 1, \"///////////////////////////\")\n",
    "    client_weights = []\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "\n",
    "    for client_data in federated_data_train:\n",
    "        weights, loss, accuracy = train_model_on_client(global_model, client_data, epochs=1)\n",
    "        client_weights.append(weights)\n",
    "        losses.append(loss)\n",
    "        accuracies.append(accuracy)\n",
    "        if has_weights_changed(weights, previous_weights):\n",
    "            print(\"Client i weight thay doi\")\n",
    "        else:\n",
    "            print(\"Client i weight khong doi\")\n",
    "    \n",
    "    # Aggregate the weights and update the global model\n",
    "    averaged_weights = aggregate_weights(client_weights)\n",
    "    global_model.set_weights(averaged_weights)\n",
    "    \n",
    "\n",
    "    loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    test_loss, test_accuracy, f1, recall, precision, auc = evaluate_model(global_model, test_dataset, loss_fn)\n",
    "\n",
    "    print(f\"Test Loss: {test_loss}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy}\")\n",
    "    print(f\"Test F1: {f1}\")\n",
    "    print(f\"Test Recall: {recall}\")\n",
    "    print(f\"Test Precision: {precision}\")\n",
    "    print(f\"Test Auc: {auc}\")\n",
    "\n",
    "    if has_weights_changed(previous_weights, averaged_weights):\n",
    "        print(f\"Trọng số thay đổi sau vòng {round_num + 1}\")\n",
    "    else:\n",
    "        print(f\"Không có sự thay đổi trong trọng số sau vòng {round_num + 1}\")\n",
    "\n",
    "    # Cập nhật trọng số cho vòng tiếp theo\n",
    "    \n",
    "    previous_weights = averaged_weights\n",
    "    \n",
    "    # Evaluate the model on the test data\n",
    "    print(\"res\")\n",
    "    \n",
    "    global_model.evaluate(test_dataset)\n",
    "    res = global_model.predict(test_dataset).flatten()\n",
    "    print(res)\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
